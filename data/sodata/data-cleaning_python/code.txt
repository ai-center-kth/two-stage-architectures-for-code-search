"In [11]: df = pd.DataFrame([[1, 'a', 2.]])\n\nIn [12]: df\nOut[12]: \n   0  1  2\n0  1  a  2\n\nIn [13]: df.dtypes\nOut[13]: \n0      int64\n1     object\n2    float64\ndtype: object\n\nIn [14]: df.dtypes == object\nOut[14]: \n0    False\n1     True\n2    False\ndtype: bool\n\nIn [15]: df.loc[:, df.dtypes == object]\nOut[15]: \n   1\n0  a\n\nIn [16]: df.loc[:, df.dtypes == object] = df.loc[:, df.dtypes == object].fillna('')\n"
"rnm_cols = dict(size='Size', sum='Sum', mean='Mean', std='Std')\ndf.set_index(['Category', 'Item']).stack().groupby('Category') \\\n  .agg(rnm_cols.keys()).rename(columns=rnm_cols)\n\n            Size   Sum        Mean        Std\nCategory                                     \nBooks          3    58   19.333333   2.081666\nClothes        3   148   49.333333   4.041452\nTechnology     6  1800  300.000000  70.710678\n\nagg_funcs = dict(Size='size', Sum='sum', Mean='mean', Std='std')\ndf.set_index(['Category', 'Item']).stack().groupby(level=0).agg(agg_funcs)\n\n                  Std   Sum        Mean  Size\nCategory                                     \nBooks        2.081666    58   19.333333     3\nClothes      4.041452   148   49.333333     3\nTechnology  70.710678  1800  300.000000     6\n\ndf.set_index(['Category', 'Item']).stack().groupby(level=0).describe().unstack()\n\n            count        mean        std    min    25%    50%    75%    max\nCategory                                                                   \nBooks         3.0   19.333333   2.081666   17.0   18.5   20.0   20.5   21.0\nClothes       3.0   49.333333   4.041452   45.0   47.5   50.0   51.5   53.0\nTechnology    6.0  300.000000  70.710678  200.0  262.5  300.0  337.5  400.0\n"
'cols = ["Weight","Height","BootSize","SuitSize","Type"]\ndf2[cols] = df2[cols].replace({\'0\':np.nan, 0:np.nan})\n'
"import numpy as np\nimport pandas as pd\n\ndf = pd.DataFrame({'id': [1,1,2,2,1,2,1,1], 'x':[10,20,100,200,np.nan,np.nan,300,np.nan]})\ndf['x'] = df.groupby(['id'])['x'].ffill()\nprint(df)\n\n   id      x\n0   1   10.0\n1   1   20.0\n2   2  100.0\n3   2  200.0\n4   1   20.0\n5   2  200.0\n6   1  300.0\n7   1  300.0\n"
'import nltk\nwords = set(nltk.corpus.words.words())\n\nsent = "Io andiamo to the beach with my amico."\n" ".join(w for w in nltk.wordpunct_tokenize(sent) \\\n         if w.lower() in words or not w.isalpha())\n# \'Io to the beach with my\'\n'
"import pandas as pd\ndf = pd.DataFrame({'x': [1, 1, 2, 2, 1, 1], 'y':[1, 2, 2, 2, 2, 1]})\nprint pd.factorize(pd.lib.fast_zip([df.x, df.y]))[0]\n\n[0 1 2 2 1 0]\n"
"print (df[pd.to_numeric(df.col, errors='coerce').isnull()])\n\ndf = pd.DataFrame({'B':['a','7','8'],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[pd.to_numeric(df.B, errors='coerce').isnull()])\n   B  C\n0  a  7\n\ndf = pd.DataFrame({'B':['a',7, 8],\n                   'C':[7,8,9]})\n\nprint (df)\n   B  C\n0  a  7\n1  7  8\n2  8  9\n\nprint (df[df.B.apply(lambda x: isinstance(x, str))])\n   B  C\n0  a  7\n"
"In [35]: df.groupby('company').product.agg([('count', 'count'), ('product', ', '.join)])\nOut[35]: \n           count          product\ncompany                          \nAmazon         1           E-comm\nFacebook       1     Social Media\nGoogle         2  Search, Android\nMicrosoft      2        OS, X-box\n"
"df['29'] = df['29'].str.strip(r'\\\\r')\nprint df\n             id             29\n0      location  Uttar Pradesh\n1  country_name          India\n2  total_deaths             20\n\nprint df.replace({r'\\\\r': ''}, regex=True)\n             id             29\n0      location  Uttar Pradesh\n1  country_name          India\n2  total_deaths             20\n\nprint df\n               id               29\n0        location  Uttar Pradesh\\r\n1    country_name            India\n2  total_deaths\\r               20\n\nprint df.replace({'29': {r'\\\\r': ''}}, regex=True)\n               id             29\n0        location  Uttar Pradesh\n1    country_name          India\n2  total_deaths\\r             20\n\nprint df.replace({r'\\\\r': ''}, regex=True)\n             id             29\n0      location  Uttar Pradesh\n1  country_name          India\n2  total_deaths             20\n\nimport pandas as pd\n\ndf = pd.read_csv('data_source_test.csv')\nprint df\n   id country_name           location  total_deaths\n0   1        India          New Delhi           354\n1   2        India         Tamil Nadu            48\n2   3        India          Karnataka             0\n3   4        India      Andra Pradesh            32\n4   5        India              Assam           679\n5   6        India             Kerala           128\n6   7        India             Punjab             0\n7   8        India      Mumbai, Thane             1\n8   9        India  Uttar Pradesh\\r\\n            20\n9  10        India             Orissa            69\n\nprint df.replace({r'\\r\\n': ''}, regex=True)\n   id country_name       location  total_deaths\n0   1        India      New Delhi           354\n1   2        India     Tamil Nadu            48\n2   3        India      Karnataka             0\n3   4        India  Andra Pradesh            32\n4   5        India          Assam           679\n5   6        India         Kerala           128\n6   7        India         Punjab             0\n7   8        India  Mumbai, Thane             1\n8   9        India  Uttar Pradesh            20\n9  10        India         Orissa            69\n\ndf['location'] = df.location.str.replace(r'\\r\\n', '')\nprint df\n   id country_name       location  total_deaths\n0   1        India      New Delhi           354\n1   2        India     Tamil Nadu            48\n2   3        India      Karnataka             0\n3   4        India  Andra Pradesh            32\n4   5        India          Assam           679\n5   6        India         Kerala           128\n6   7        India         Punjab             0\n7   8        India  Mumbai, Thane             1\n8   9        India  Uttar Pradesh            20\n9  10        India         Orissa            69\n"
'&gt;&gt;&gt; "this tweet is example #key1_key2_key3".replace("#", "").replace("_", " ")\n'
"df = data[[0]].join(data[1].str.get_dummies(', ').replace(0, ''))\nprint (df)\n       0 event2 event3 event4\n0  userA      1      1       \n1  userB             1      1\n2  userC      1              \n\nprint (data[1].str.get_dummies(', '))\n   event2  event3  event4\n0       1       1       0\n1       0       1       1\n2       1       0       0\n"
"df['datestart'] = pd.to_datetime(df['datestart'], coerce=True)\n\ndf['datestart'].values.astype('datetime64[Y]')\n\ndf['year'] =  pd.DatetimeIndex(df['datestart']).year\n"
"In [71]: import pandas_datareader.data as web\n\nIn [110]: df = web.DataReader('SBIN.NS', 'yahoo', '2014-10-21', '2014-11-25')\n\nIn [111]: df\nOut[111]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2014-10-21  2580.0000  2607.0001  2569.5999  2584.1501  15022300   251.8850\n2014-10-22  2608.9999  2613.5999  2565.1001  2575.2499  14511100   251.0175\n2014-10-23  2591.4001  2593.7000  2573.9999  2578.5501   2376200   251.3392\n2014-10-24  2578.5501  2578.5501  2578.5501  2578.5501         0   251.3392\n2014-10-27  2592.0001  2619.8999  2581.0001  2597.8000  13429500   253.2155\n2014-10-28  2607.9999  2664.2999  2606.0001  2656.7499  22963400   258.9616\n2014-10-29  2677.0001  2678.9999  2631.0001  2643.7500  17372900   257.6944\n2014-10-30  2649.8999  2653.0499  2622.0001  2637.9999  15544200   257.1339\n2014-10-31   265.2000   270.9800   264.6000   270.2800  20770200    26.3450   # &lt;bad_data&gt;\n2014-11-03   270.6000   274.3500   269.4250   272.3450  17780600    26.5463\n2014-11-04   272.3450   272.3450   272.3450   272.3450         0    26.5463\n2014-11-05   273.3000   279.9800   272.4050   278.1900  26605100    27.1160\n2014-11-06   278.1900   278.1900   278.1900   278.1900         0    27.1160\n2014-11-07   277.5000   278.1000   273.0000   274.2500  18163000    26.7320\n2014-11-10   275.9000   276.9000   273.3000   273.9500  12068800    26.7027\n2014-11-11   274.7900   276.2500   270.5000   274.0350  17405900    26.7110\n2014-11-12   275.3000   277.1500   273.5550   274.6050  16233200    26.7666\n2014-11-13   275.6100   276.2250   269.5000   271.9300  16859000    26.5059\n2014-11-14   273.0000   280.6900   272.0000   278.7850  50846600    27.1740\n2014-11-17   279.4000   295.1300   279.2200   294.0600  49164100    28.6629\n2014-11-18   295.6950   297.9000   292.4100   294.5750  32898300    28.7131\n2014-11-19   294.9000   296.8000   290.3550   291.0500  20735900    28.3695   # &lt;/bad_data&gt;\n2014-11-20   294.7500   298.7500   291.2500   297.1000  18099500   289.5925\n2014-11-21   299.9000   307.0000   297.2500   305.5000  21009200   297.7802\n2014-11-24   307.8000   309.8500   306.0500   308.8500  18631400   301.0456\n2014-11-25   309.9000   309.9500   301.0000   304.4500  26776600   296.7568\n\nIn [112]: bad_idx = df.index[df['Adj Close'].pct_change().abs().ge(0.5)]\n\nIn [113]: bad_idx\nOut[113]: DatetimeIndex(['2014-10-31', '2014-11-20'], dtype='datetime64[ns]', name='Date', freq=None)\n\nIn [114]: df.loc[(df.index &gt;= bad_idx.min()) &amp; (df.index &lt; bad_idx.max()), 'Adj Close'] *= 10\n\nIn [115]: df\nOut[115]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2014-10-21  2580.0000  2607.0001  2569.5999  2584.1501  15022300   251.8850\n2014-10-22  2608.9999  2613.5999  2565.1001  2575.2499  14511100   251.0175\n2014-10-23  2591.4001  2593.7000  2573.9999  2578.5501   2376200   251.3392\n2014-10-24  2578.5501  2578.5501  2578.5501  2578.5501         0   251.3392\n2014-10-27  2592.0001  2619.8999  2581.0001  2597.8000  13429500   253.2155\n2014-10-28  2607.9999  2664.2999  2606.0001  2656.7499  22963400   258.9616\n2014-10-29  2677.0001  2678.9999  2631.0001  2643.7500  17372900   257.6944\n2014-10-30  2649.8999  2653.0499  2622.0001  2637.9999  15544200   257.1339\n2014-10-31   265.2000   270.9800   264.6000   270.2800  20770200   263.4500\n2014-11-03   270.6000   274.3500   269.4250   272.3450  17780600   265.4630\n2014-11-04   272.3450   272.3450   272.3450   272.3450         0   265.4630\n2014-11-05   273.3000   279.9800   272.4050   278.1900  26605100   271.1600\n2014-11-06   278.1900   278.1900   278.1900   278.1900         0   271.1600\n2014-11-07   277.5000   278.1000   273.0000   274.2500  18163000   267.3200\n2014-11-10   275.9000   276.9000   273.3000   273.9500  12068800   267.0270\n2014-11-11   274.7900   276.2500   270.5000   274.0350  17405900   267.1100\n2014-11-12   275.3000   277.1500   273.5550   274.6050  16233200   267.6660\n2014-11-13   275.6100   276.2250   269.5000   271.9300  16859000   265.0590\n2014-11-14   273.0000   280.6900   272.0000   278.7850  50846600   271.7400\n2014-11-17   279.4000   295.1300   279.2200   294.0600  49164100   286.6290\n2014-11-18   295.6950   297.9000   292.4100   294.5750  32898300   287.1310\n2014-11-19   294.9000   296.8000   290.3550   291.0500  20735900   283.6950\n2014-11-20   294.7500   298.7500   291.2500   297.1000  18099500   289.5925\n2014-11-21   299.9000   307.0000   297.2500   305.5000  21009200   297.7802\n2014-11-24   307.8000   309.8500   306.0500   308.8500  18631400   301.0456\n2014-11-25   309.9000   309.9500   301.0000   304.4500  26776600   296.7568\n\nIn [119]: df.loc[(df.index &gt;= bad_idx.min()) &amp; (df.index &lt; bad_idx.max()), 'Adj Close'] = np.nan\n\nIn [120]: df\nOut[120]:\n                 Open       High        Low      Close    Volume  Adj Close\nDate\n2014-10-21  2580.0000  2607.0001  2569.5999  2584.1501  15022300   251.8850\n2014-10-22  2608.9999  2613.5999  2565.1001  2575.2499  14511100   251.0175\n2014-10-23  2591.4001  2593.7000  2573.9999  2578.5501   2376200   251.3392\n2014-10-24  2578.5501  2578.5501  2578.5501  2578.5501         0   251.3392\n2014-10-27  2592.0001  2619.8999  2581.0001  2597.8000  13429500   253.2155\n2014-10-28  2607.9999  2664.2999  2606.0001  2656.7499  22963400   258.9616\n2014-10-29  2677.0001  2678.9999  2631.0001  2643.7500  17372900   257.6944\n2014-10-30  2649.8999  2653.0499  2622.0001  2637.9999  15544200   257.1339\n2014-10-31   265.2000   270.9800   264.6000   270.2800  20770200        NaN\n2014-11-03   270.6000   274.3500   269.4250   272.3450  17780600        NaN\n2014-11-04   272.3450   272.3450   272.3450   272.3450         0        NaN\n2014-11-05   273.3000   279.9800   272.4050   278.1900  26605100        NaN\n2014-11-06   278.1900   278.1900   278.1900   278.1900         0        NaN\n2014-11-07   277.5000   278.1000   273.0000   274.2500  18163000        NaN\n2014-11-10   275.9000   276.9000   273.3000   273.9500  12068800        NaN\n2014-11-11   274.7900   276.2500   270.5000   274.0350  17405900        NaN\n2014-11-12   275.3000   277.1500   273.5550   274.6050  16233200        NaN\n2014-11-13   275.6100   276.2250   269.5000   271.9300  16859000        NaN\n2014-11-14   273.0000   280.6900   272.0000   278.7850  50846600        NaN\n2014-11-17   279.4000   295.1300   279.2200   294.0600  49164100        NaN\n2014-11-18   295.6950   297.9000   292.4100   294.5750  32898300        NaN\n2014-11-19   294.9000   296.8000   290.3550   291.0500  20735900        NaN\n2014-11-20   294.7500   298.7500   291.2500   297.1000  18099500   289.5925\n2014-11-21   299.9000   307.0000   297.2500   305.5000  21009200   297.7802\n2014-11-24   307.8000   309.8500   306.0500   308.8500  18631400   301.0456\n2014-11-25   309.9000   309.9500   301.0000   304.4500  26776600   296.7568\n\nIn [122]: df['Adj Close'] = df['Adj Close'].interpolate()\n\nIn [123]: df\nOut[123]:\n                 Open       High        Low      Close    Volume   Adj Close\nDate\n2014-10-21  2580.0000  2607.0001  2569.5999  2584.1501  15022300  251.885000\n2014-10-22  2608.9999  2613.5999  2565.1001  2575.2499  14511100  251.017500\n2014-10-23  2591.4001  2593.7000  2573.9999  2578.5501   2376200  251.339200\n2014-10-24  2578.5501  2578.5501  2578.5501  2578.5501         0  251.339200\n2014-10-27  2592.0001  2619.8999  2581.0001  2597.8000  13429500  253.215500\n2014-10-28  2607.9999  2664.2999  2606.0001  2656.7499  22963400  258.961600\n2014-10-29  2677.0001  2678.9999  2631.0001  2643.7500  17372900  257.694400\n2014-10-30  2649.8999  2653.0499  2622.0001  2637.9999  15544200  257.133900\n2014-10-31   265.2000   270.9800   264.6000   270.2800  20770200  259.297807\n2014-11-03   270.6000   274.3500   269.4250   272.3450  17780600  261.461713\n2014-11-04   272.3450   272.3450   272.3450   272.3450         0  263.625620\n2014-11-05   273.3000   279.9800   272.4050   278.1900  26605100  265.789527\n2014-11-06   278.1900   278.1900   278.1900   278.1900         0  267.953433\n2014-11-07   277.5000   278.1000   273.0000   274.2500  18163000  270.117340\n2014-11-10   275.9000   276.9000   273.3000   273.9500  12068800  272.281247\n2014-11-11   274.7900   276.2500   270.5000   274.0350  17405900  274.445153\n2014-11-12   275.3000   277.1500   273.5550   274.6050  16233200  276.609060\n2014-11-13   275.6100   276.2250   269.5000   271.9300  16859000  278.772967\n2014-11-14   273.0000   280.6900   272.0000   278.7850  50846600  280.936873\n2014-11-17   279.4000   295.1300   279.2200   294.0600  49164100  283.100780\n2014-11-18   295.6950   297.9000   292.4100   294.5750  32898300  285.264687\n2014-11-19   294.9000   296.8000   290.3550   291.0500  20735900  287.428593\n2014-11-20   294.7500   298.7500   291.2500   297.1000  18099500  289.592500\n2014-11-21   299.9000   307.0000   297.2500   305.5000  21009200  297.780200\n2014-11-24   307.8000   309.8500   306.0500   308.8500  18631400  301.045600\n2014-11-25   309.9000   309.9500   301.0000   304.4500  26776600  296.756800\n"
"prng = np.random.RandomState(0)\ndf = pd.DataFrame({'company_id': prng.randint(10**6, size=10**7), \n                   'company_name': prng.rand(10**7).astype('str')})\n# It has 10m unique identifiers each having 10 entries on average\n# ranges from 1 to 28.\n\ndf.head()\n   company_id         company_name\n0      985772   0.4097176168442743\n1      305711    0.506659503051052\n2      435829  0.45049621797963846\n3      117952  0.21756825314220174\n4      963395  0.07977409062048224\n\n%%timeit\nmapper = df.drop_duplicates(subset='company_id').set_index('company_id')['company_name']\ndf['company_id'].map(mapper)\n1 loop, best of 3: 1.86 s per loop\n\n%timeit df.groupby('company_id')['company_name'].transform('first')\n1 loop, best of 3: 2.33 s per loop\n\n%timeit df.groupby('company_id').first().loc[df.company_id]\n1 loop, best of 3: 26.4 s per loop\n"
"df = ohlc.assign(Close=ohlc['Close'].ffill()).bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n\nohlc['Close'] = ohlc['Close'].ffill()\ndf = ohlc.bfill(axis=1)\nprint (df)\n                     Open  High  Low  Close\n2017-07-23 03:13:00   1.0   5.0  1.0    5.0\n2017-07-23 03:14:00   5.0   5.0  5.0    5.0\n2017-07-23 03:15:00   5.0   5.0  2.0    2.0\n2017-07-23 03:16:00   2.0   2.0  2.0    2.0\n"
"df_10v['emp_length'] = df_10v['emp_length'].astype(str).str.replace('\\D+', '')\n\ndf_10v['emp_length'] = df_10v['emp_length'].astype(str).str.lstrip('&lt;').str.rstrip('+')\n"
"df['group']=(df.col1==0)&amp;(df.col2==1)\ndf['group']=df['group'].cumsum()\n\nmask=df.groupby('group').apply(lambda x : sum((x.col1==0)&amp;(x.col2==2)))\ndf.loc[df.group.isin(mask[mask.eq(1)].index)]\n\n\nOut[363]: \n    col1  col2   id  group\n3      0     1  id1      2\n4      1     2  id1      2\n5      0     2  id1      2\n6      0     1  id1      3\n7      1     2  id1      3\n8      1     2  id1      3\n9      0     2  id1      3\n10     0     1  id1      4\n11     1     2  id2      4\n12     0     2  id2      4\n\nmask=df.groupby('group').last().loc[lambda x : (x.col1==0)&amp;(x.col2==2),].index\ndf.loc[df.group.isin(mask)]\n\n\nOut[379]: \n    col1  col2   id  group\n3      0     1  id1      2\n4      1     2  id1      2\n5      0     2  id1      2\n6      0     1  id1      3\n7      1     2  id1      3\n8      1     2  id1      3\n9      0     2  id1      3\n10     0     1  id1      4\n11     1     2  id2      4\n12     0     2  id2      4\n"
"df['type']=df[df.columns[1:]].ffill(axis=1).iloc[:,-1]\n#alternatively-&gt; df['type']=df.loc[:,['animal','fruit','veg']].ffill(axis=1).iloc[:,-1]\ndf_new=df[['name','type']]\nprint(df_new)\n\n           name    type\nindex                  \n0           cow  animal\n1         apple   fruit\n2        carrot     veg\n3           dog  animal\n4         horse  animal\n5           car     NaN\n6          pear   fruit\n7        pepper     veg\n8      cucumber     veg\n9         house     NaN\n"
"PYTHONIOENCODING=UTF-8 python3  europarl_extractor.py\n\nimport gzip\n\nif __name__ == '__main__':\n    with gzip.open('europarl-v7.fr.gz', 'rb') as f_in:\n        bs = f_in.read()\n        txt = bs.decode('utf-8')\n        print(txt[:100])\n\n'LC_ALL', 'LC_CTYPE', 'LANG', 'LANGUAGE'\n"
"for col in df.columns[df.dtypes=='object']:\n    df.loc[df[col].str.startswith('$$ER',na=False),col]=''\n\nfor col in df.columns[df.dtypes=='object']:\n    df.loc[df[col].str.contains('$$ER',na=False,regex=False),col]=''\n\n"
"df_new = example.apply(pd.Series.explode)    \ndf_new.set_index('col1', append=True).unstack()\n\ncol1    key1    key2    key3    key4    key5\n0     value1  value2  value3     NaN     NaN\n1     value1     NaN     NaN  value4     NaN\n2     value1     NaN  value3  value4  value5\n"
"c = ['ID','Income']\nd = [\n[1, 26000],  \n[2, '45K'],\n[3, '-'],\n[4, 0],  \n[5, 'N/A'],     \n[6, 2000],         \n[7, '30000 - 45000'],\n[8, '21000 per Annum'],\n[9, '50000 per annum'],\n[10, '21000 to 30000'],\n[11, ''],\n[12, '21000 To 50000'],\n[13, '43000/year'],\n[14, ''],\n[15, '80000/Year'],\n[16, '12.40 p/h'],\n[17, '12.40 per hour'],\n[18, 45000.00]]\n\nimport pandas as pd\nimport numpy as np\ndf = pd.DataFrame(d,columns=c)\n\ndf['Income1'] = df['Income'].astype(str).str.lower()\n\ndf['Income1'].replace({'n/a' : '0', '':'0', '-':'0', 0:'0'}, regex=False, inplace=True)\n\ndf['Income1'].replace({'k$': '000','to': '+', '-': '+', ' per annum': '', 'p/h' : 'per hour', '/year': ''}, regex=True, inplace=True)\n\ndf['Income1'].replace(' per hour', ' * 12 * 52', regex=True, inplace=True)\n\ndf.loc[df.astype(str).Income1.str.contains('\\+'),'Income1'] = '(' + df['Income1'].astype(str) + ') / 2'\n\ndf['Income1'] = df['Income1'].apply(lambda x: eval(x) if (pd.notnull(x)) else x)\n\ndf['Income1'] = (df['Income1'].fillna(0)\n                 .astype(int)\n                 .astype(object)\n                 .where(df['Income1'].notnull()))\n\nprint (df)\n\n    ID           Income Income1\n0    1            26000   26000\n1    2              45K   45000\n2    3                -     NaN\n3    4                0     NaN\n4    5              N/A     NaN\n5    6             2000    2000\n6    7    30000 - 45000   37500\n7    8  21000 per Annum   21000\n8    9  50000 per annum   50000\n9   10   21000 to 30000   25500\n10  11                      NaN\n11  12   21000 To 50000   35500\n12  13       43000/year   43000\n13  14                      NaN\n14  15       80000/Year   80000\n15  16        12.40 p/h    7737\n16  17   12.40 per hour    7737\n17  18            45000   45000\n"
"In [1]: from random import randint \n\nIn [2]: months = [randint(1, 12) for x in range(10)]\n\nIn [3]: years = [randint(2000, 2020) for x in range(10)]\n\nIn [4]: months\nOut[4]: [12, 3, 7, 6, 10, 10, 11, 9, 9, 10]\n\nIn [5]: years\nOut[5]: [2017, 2016, 2001, 2004, 2015, 2013, 2001, 2020, 2013, 2016]\n\nIn [6]: import pandas as pd\n\nIn [7]: df = pd.DataFrame()\n\nIn [8]: df['Month'] = months\n\nIn [9]: df['Year'] = years\n\nIn [10]: filterDict = [{1: 2003}, {2: 2008}, {3: 2011}, {4: 2012}, {5: 2008}, {6: 2008}, {7: 2002}, {8: 2006}, {9: 2005}, {3: 2016}, {6: 2004}, {12: 2001}]\n\nIn [11]: filterList = [d.items()[0] for d in filterDict]\n\nIn [12]: df[df.apply(lambda x: (x['Month'],x['Year']) in filterList, axis=1)]\nOut[12]: \n   Month  Year\n1      3  2016\n3      6  2004\n\nIn [13]: df\nOut[13]: \n   Month  Year\n0     12  2017\n1      3  2016\n2      7  2001\n3      6  2004\n4     10  2015\n5     10  2013\n6     11  2001\n7      9  2020\n8      9  2013\n9     10  2016\n"
"df['dif'] = df.groupby('id')['day'].diff(-1) * (-1)\nprint (df)\n    id        day  total_amount      dif\n0    1 2015-07-09          1000 105 days\n1    1 2015-10-22           100  21 days\n2    1 2015-11-12           200  15 days\n3    1 2015-11-27          2392  19 days\n4    1 2015-12-16           123      NaT\n5    7 2015-07-09           200   0 days\n6    7 2015-07-09          1000  49 days\n7    7 2015-08-27        100018  90 days\n8    7 2015-11-25          1000      NaT\n9    8 2015-08-27          1000 102 days\n10   8 2015-12-07         10000  42 days\n11   8 2016-01-18           796  73 days\n12   8 2016-03-31         10000      NaT\n13  15 2015-09-10          1500  20 days\n14  15 2015-09-30          1000      NaT\n\ndf['diff'] = df.groupby('id')['day'].apply(lambda x: x.shift(-1) - x)\nprint (df)\n    id        day  total_amount     diff\n0    1 2015-07-09          1000 105 days\n1    1 2015-10-22           100  21 days\n2    1 2015-11-12           200  15 days\n3    1 2015-11-27          2392  19 days\n4    1 2015-12-16           123      NaT\n5    7 2015-07-09           200   0 days\n6    7 2015-07-09          1000  49 days\n7    7 2015-08-27        100018  90 days\n8    7 2015-11-25          1000      NaT\n9    8 2015-08-27          1000 102 days\n10   8 2015-12-07         10000  42 days\n11   8 2016-01-18           796  73 days\n12   8 2016-03-31         10000      NaT\n13  15 2015-09-10          1500  20 days\n14  15 2015-09-30          1000      NaT\n\ndf['diff'] = df.groupby('id')['day'].diff(-1) * (-1) / np.timedelta64(1, 'h')\nprint (df)\n    id        day  total_amount    diff\n0    1 2015-07-09          1000  2520.0\n1    1 2015-10-22           100   504.0\n2    1 2015-11-12           200   360.0\n3    1 2015-11-27          2392   456.0\n4    1 2015-12-16           123     NaN\n5    7 2015-07-09           200     0.0\n6    7 2015-07-09          1000  1176.0\n7    7 2015-08-27        100018  2160.0\n8    7 2015-11-25          1000     NaN\n9    8 2015-08-27          1000  2448.0\n10   8 2015-12-07         10000  1008.0\n11   8 2016-01-18           796  1752.0\n12   8 2016-03-31         10000     NaN\n13  15 2015-09-10          1500   480.0\n14  15 2015-09-30          1000     NaN\n\ndf['diff'] = df.groupby('id')['day'].apply(lambda x: x.shift(-1) - x) / \n                                     np.timedelta64(1, 'h')\nprint (df)\n    id        day  total_amount    diff\n0    1 2015-07-09          1000  2520.0\n1    1 2015-10-22           100   504.0\n2    1 2015-11-12           200   360.0\n3    1 2015-11-27          2392   456.0\n4    1 2015-12-16           123     NaN\n5    7 2015-07-09           200     0.0\n6    7 2015-07-09          1000  1176.0\n7    7 2015-08-27        100018  2160.0\n8    7 2015-11-25          1000     NaN\n9    8 2015-08-27          1000  2448.0\n10   8 2015-12-07         10000  1008.0\n11   8 2016-01-18           796  1752.0\n12   8 2016-03-31         10000     NaN\n13  15 2015-09-10          1500   480.0\n14  15 2015-09-30          1000     NaN\n"
'df.Name = df.Name.replace(incorrect_to_correct)\n\ndf = pd.DataFrame({\'Name\' : ["john","mary","jon", "mar"]})\nprint (df)\n   Name\n0  john\n1  mary\n2   jon\n3   mar\n\nincorrect_to_correct = {\'jon\':\'john\', \'mar\':\'mary\'}\n\ndf.Name = df.Name.replace(incorrect_to_correct)\nprint (df)\n   Name\n0  john\n1  mary\n2  john\n3  mary\n'
"def check_type(homicide_df):\n    for i, age in homicide_df['Perpetrator Age'].iteritems():\n        if type(age) is str:\n            print(i, age, type(age))\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':[10, '15', 'aa']})\nprint (homicide_df)\n  Perpetrator Age\n0              10\n1              15\n2              aa\n\n\ndef check_type(homicide_df):\n    for i, age in homicide_df['Perpetrator Age'].iteritems():\n        if type(age) is str:\n            print(i, age, type(age))\n\ncheck_type(homicide_df)\n1 15 &lt;class 'str'&gt;\n2 aa &lt;class 'str'&gt;\n\ndef check_type(homicide_df):\n    return homicide_df.loc[homicide_df['Perpetrator Age'].apply(type)==str,'Perpetrator Age']\n\nprint  (check_type(homicide_df))\n1    15\n2    aa\nName: Perpetrator Age, dtype: object\n\nprint ((homicide_df['Perpetrator Age'].apply(type)==str).all())\nTrue\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':['10', '15']})\n\nhomicide_df['Perpetrator Age'] = homicide_df['Perpetrator Age'].astype(int)\nprint (homicide_df)\n\n   Perpetrator Age\n0               10\n1               15\n\nprint (homicide_df['Perpetrator Age'].dtypes)\nint32\n\nhomicide_df = pd.DataFrame({'Perpetrator Age':[10, '15', 'aa']})\n\nhomicide_df['Perpetrator Age']=pd.to_numeric(homicide_df['Perpetrator Age'], errors='coerce')\nprint (homicide_df)\n   Perpetrator Age\n0             10.0\n1             15.0\n2              NaN\n\nhomicide_df['Perpetrator Age'] = homicide_df['Perpetrator Age'].fillna(0).astype(int)\nprint (homicide_df)\n   Perpetrator Age\n0               10\n1               15\n2                0\n"
"import os\nfrom random import shuffle\nfrom difflib import SequenceMatcher\n\nemails = [... ...] # for example the 16 email addresses you gave in your question\nshuffle(emails) # everyday i'm shuffling\nemails = sorted(emails) # sort that shit!\nnames = [email.split('@')[0] for email in emails]\n\nT = 0.7 # &lt;- set your string similarity threshold here!!\n\nsplit_indices=[]\nfor i in range(1,len(emails)):\n    if SequenceMatcher(None, emails[i], emails[i-1]).ratio() &lt; T:\n        split_indices.append(i) # we want to remember where dissimilar email address occurs\n\ngrouped=[]\nfor i in split_indices:\n    grouped.append(emails[:i])\ngrouped.append(emails[i:])\n# now we have similar email addresses grouped, we want to find the common prefix for each group\nprefix_strings=[]\nfor group in grouped:\n    prefix_strings.append(os.path.commonprefix(group))\n\n# finally\nham=[]\nspam=[]\ntrue_ids = [names.index(p) for p in prefix_strings]\nfor i in range(len(emails)):\n    if i in true_ids:\n        ham.append(emails[i])\n    else:\n        spam.append(emails[i])\n\nIn [30]: ham\nOut[30]: ['abc7020@gmail.com', 'attn1@gmail.com']\n\nIn [31]: spam\nOut[31]: \n['abc7020.10@gmail.com',\n 'abc7020.11@gmail.com',\n 'abc7020.12@gmail.com',\n 'abc7020.13@gmail.com',\n 'abc7020.14@gmail.com',\n 'abc7020.15@gmail.com',\n 'abc7020.1@gmail.com',\n 'attn12345678@gmail.com',\n 'attn1234567@gmail.com',\n 'attn123456@gmail.com',\n 'attn12345@gmail.com',\n 'attn1234@gmail.com',\n 'attn123@gmail.com',\n 'attn12@gmail.com']  \n\n# THE TRUTH YALL!\n"
"colnames = ['col1','col2','col3'] # or whatever you want\nwith open('data.txt') as f:\n    df = pd.DataFrame([parse(l) for l in f], columns=colnames)\n\nIn [6]: df.append([1,2,3])\nOut[6]:\n   0\n0  1\n1  2\n2  3\n\nIn [7]: df = pd.DataFrame()\n\nIn [8]: df.append([[1, 2, 3]])\nOut[8]:\n   0  1  2\n0  1  2  3\n"
"df1.Time=pd.to_datetime(df1.Time)\ndf1.Time=df1.Time.dt.month+df1.Time.dt.year*100\ndf1['Var_3']=df1['Var_3'].astype(int)\n\noutput=df1.groupby(['ID','Product_ID','Time']).agg({'Var_1':'mean','Var_2':'sum','Var_3':'sum'})\noutput=output.unstack(2).stack(dropna=False).fillna(0)# missing one .\n\n\noutput['Count']=output.max(1)\noutput.reset_index().sort_values(['Product_ID','ID'])\n\n\nOut[1032]: \n  ID Product_ID    Time  Var_3  Var_2  Var_1  Count\n0  A          1  201601    0.0    0.0  0.000    0.0\n1  A          1  201602    2.0    1.0  0.155    2.0\n4  B          1  201601    2.0    1.0  0.215    2.0\n5  B          1  201602    0.0    1.0  0.400    1.0\n2  A          2  201601    1.0    1.0  0.110    1.0\n3  A          2  201602    0.0    0.0  0.000    0.0\n6  B          2  201601    0.0    0.0  0.000    0.0\n7  B          2  201602    1.0    0.0  0.525    1.0\n8  C          2  201601    0.0    1.0  0.190    1.0\n9  C          2  201602    0.0    0.0  0.000    0.0\n"
"df = df.apply(pd.to_numeric, errors='coerce')\n\nfor c in df.columns:\n    df[c] = pd.to_numeric(df[c], errors='coerce')\n"
"# sample data\ndf = pd.DataFrame({'c1':['a','b','c'], 'c2':['a b c d','e f g h','i j k l']})\n\n# transform into multiple columns\ndf = pd.concat([df['c1'],df['c2'].str.split(' ', expand=True)], axis=1)\n\nprint(df)\n\n  c1  0  1  2  3\n0  a  a  b  c  d\n1  b  e  f  g  h\n2  c  i  j  k  l\n"
"arr_first = (df.T.reset_index().groupby([col for col in df.T.columns])['index']\n                .transform(lambda x: x.iloc[0]).values)\n\nduplicate_index = pd.Series(pd.np.where(arr_first != df.columns, arr_first, None),df.columns)\n\nA    None\nB    None\nC       A\nD       A\ndtype: object\n"
"df['Section_Number'] = df['Section'].str.replace('([A-Z]+)', '')\ndf['Section_Letter'] = df['Section'].str.extract('([A-Z]+)')\nprint (df)\n    Name Section Section_Number Section_Letter\n1  James      P3              3              P\n2    Sam    2.5C            2.5              C\n3  Billy     T35             35              T\n4  Sarah     A85             85              A\n5  Felix      5I              5              I\n\ndf['Section_Number'] = df['Section'].str.replace('([A-Za-z]+)', '')\ndf['Section_Letter'] = df['Section'].str.extract('([A-Za-z]+)')\nprint (df)\n    Name Section Section_Number Section_Letter\n1  James      P3              3              P\n2    Sam    2.5C            2.5              C\n3  Billy     T35             35              T\n4  Sarah     A85             85              A\n5  Felix      5I              5              I\n"
"top_cats = df.studio.value_counts().head(8).index.tolist() + ['other']\ndf['studio'] = pd.Categorical(df['studio'], categories=top_cats).fillna('other')\n"
"df.loc[lambda df: df.groupby('part')['price'].transform(np.any)]\\\n  .sort_values('date')\\\n  .assign(price=lambda df: df.groupby('part')['price'].ffill())\\\n  .dropna()\\\n  .reset_index(drop=True)\n\n    part    price   date\n0   1   88.37   2018-07-01\n1   3   264.02  2018-07-01\n2   1   88.37   2018-08-01\n3   3   264.02  2018-08-01\n4   1   88.37   2018-09-01\n5   3   212.70  2018-09-01\n6   1   67.32   2018-10-01\n7   3   167.34  2018-10-01\n8   1   67.32   2018-11-01\n9   3   167.34  2018-11-01\n10  1   67.32   2018-12-01\n11  3   99.16   2018-12-01\n\ndf = df.iloc[::-1].reset_index(drop=True)\n"
'import pandas as pd\ndf = pd.DataFrame([["Apple", 10], ["Pear", 20], ["Apple", 5], ["Banana", 7], ["Banana", 12], ["Pear", 8], ["Apple", 10]], columns=["Fruit", "Count"])\ndf = df.groupby([\'Fruit\'])[\'Count\'].sum()\nprint(df)\n\nFruit\nApple     25\nBanana    19\nPear      28\n'
' accom_cond=[((house.bedrooms==1) &amp; (house.accommodates.isna()))]\n accom_val= [2]\n'
"import re\n\ntestlist = ['Just caught up with #FlirtyDancing. Just so cute! Loved it. ', 'After work drinks with this one @MrLukeBenjamin no dancing tonight though @flirtydancing @AshleyBanjo #FlirtyDancing pic.twitter.com/GJpRUZxUe8', 'Only just catching up and @AshleyBanjo you are gorgeous #FlirtyDancing', 'Loved working on this. Always a pleasure getting to assist the wonderful @kendrahorsburgh on @ashleybanjogram wonderful new show !! #flirtydancing pic.twitter.com/URMjUcgmyi', 'Just watching #FlirtyDancing &amp; \\n@AshleyBanjo what an amazing way to meet someone.. It made my heart all warm &amp; fuzzy for these people! both couples meet back up.. pic.twitter.com/iwCLRmAi5n',]\nregexp = r'pic\\.twitter\\.com\\S+|@\\S+|#\\S+'\n\nres = [re.sub(regexp, '', sent) for sent in testlist]\nprint(res)\n\nJust caught up with  Just so cute! Loved it. \nAfter work drinks with this one  no dancing tonight though    \nOnly just catching up and  you are gorgeous \nLoved working on this. Always a pleasure getting to assist the wonderful  on  wonderful new show !!  \nJust watching  &amp; \n what an amazing way to meet someone.. It made my heart all warm &amp; fuzzy for these people! both couples meet back up.. \n"
'df[df.notna().sum(axis=1) &lt;= 1]\n    a    b    c   d\n0 NaN  1.0  NaN NaN\n3 NaN  NaN  1.0 NaN\n'
"df['VendorID']= pd.Series(range(1,df.shape[0]+1)) #starts with 1\n\ndf['VendorID']= pd.Series(range(0,df.shape[0])) #starts with 0\n"
"cols = df.columns.to_numpy()\ndf['Tags_col'] = [list(cols[x]) for x in df.eq(1).to_numpy()]\nprint (df)\n\n   Dog  Cat  Rabbit       Tags_col\n0    0    1       1  [Cat, Rabbit]\n1    1    0       0          [Dog]\n\ndf['Tags_col'] = df.apply(lambda x: list(x.index[x==1]), axis=1)\nprint (df)\n   Dog  Cat  Rabbit       Tags_col\n0    0    1       1  [Cat, Rabbit]\n1    1    0       0          [Dog]\n"
"df.iloc[:, :5].dropna(how='all')\n\ndf.iloc[:, :5].dropna(how='any')\n\nIn [2107]: ix = df.iloc[:, :5].dropna(how='all').index.tolist()\n\nIn [2110]: df = df.loc[ix]\n\nIn [2111]: df\nOut[2111]: \n       LotName    C15    C16    C17    C18  C19  Spots15  Spots16\nCherry      St  439.0  464.0  555.0  239.0  420      101    101.0\nBarton     Lot   34.0   24.0   43.0   45.0   39       10      9.0\n"
"import pandas as pd\nimport os\n\ndirectory = r'../html/species'\n\nfor filename in os.listdir(directory):\n    if filename.endswith('.html'):\n        csv_filename = filename.replace('.html','.csv')\n        fname = os.path.join(directory,filename)\n        with open(fname, 'r') as f:\n            table = pd.read_html(f.read())[0]\n            table.to_csv(csv_filename, index=False)\n\nprint(data)\n"
'import re\n\n# This pattern should match correct data lines and should not\n# match "continuation" lines (lines added by the unquoted newline).\n# This pattern means: start of line, then a number, then white space,\n# then another number, then more white space, then another number.\n\n# This program won\'t work right if this pattern isn\'t correct.\npat = re.compile("^\\d+\\s+\\d+\\s+\\d+")\n\ndef collect_lines(iterable):\n    itr = iter(iterable)  # get an iterator\n\n    # First, loop until we find a valid line.\n    # This will skip the first line with the "header" info.\n    line = next(itr)\n    while True:\n        line = next(itr)\n        if pat.match(line):\n            # found a valid line; hold it as cur\n            cur = line\n            break\n    for line in itr:\n        # Look at the line after cur.  Is it a valid line?\n        if pat.match(line):\n            # Line after cur is valid!\n            yield cur  # output cur\n            cur = line  # hold new line as new cur\n        else:\n            # Line after cur is not valid; append to cur but do not output yet.\n            cur = cur.rstrip(\'\\r\\n\') + line\n    yield cur\n\ndata = """\\\n   JOB  REF Comment V2  Other\n@@@1   3   45  This was a small job    NULL    sdnsdf\n@@@2   4   456 This was a large job and I have to go onto a new line, \n@@@    but I didn\'t properly escape so it\'s on the next row whoops!    NULL    NULL        \n@@@3   7   354 NULL    NULL    NULL\n"""\n\nlines = data.split(\'@@@\')\nfor line in collect_lines(lines):\n    print("&gt;&gt;&gt;{}&lt;&lt;&lt;".format(line))\n\nwith open("filename", "rt") as f:\n    for line in collect_lines(f):\n        # do something with each line\n'
'def rolling_group(val):\n    if pd.notnull(val): rolling_group.group +=1 #pd.notnull is signal to switch group\n    return rolling_group.group\nrolling_group.group = 0 #static variable\n\n#groups = df.groupby(df[\'Control\'].apply(rolling_group),as_index=False)\n\ndef rolling_group(val):\n    if pd.notnull(val): rolling_group.group +=1 #pd.notnull is signal to switch group\n    return rolling_group.group\nrolling_group.group = 0 #static variable\n\ndef joinFunc(g,column):\n    col =g[column]\n    joiner = "/" if column == "Action" else ","\n    s = joiner.join([str(each) for each in col if pd.notnull(each)])\n    s = re.sub("(?&lt;=&amp;)"+joiner," ",s) #joiner = " "\n    s = re.sub("(?&lt;=-)"+joiner,"",s) #joiner = ""\n    s = re.sub(joiner*2,joiner,s)    #fixes double joiner condition\n    return s\n\nif __name__ == "__main__":\n    df = """      Control      Recd_Date/Due_Date                Action        Signature/Requester\n0     2000-1703   2000-01-31 00:00:00           OC/OER/OPA/PMS/                 M WEBB\n1           NaN   2000-02-29 00:00:00                       NaN              DATA CORP\n2     2000-1776   2000-01-02 00:00:00            OC/ORA/OE/DCP/                  G KAN\n3           NaN   2000-01-03 00:00:00           OC/ORA/ORO/PNC/              PALM POST\n4           NaN                   NaN  FDA/OGROP/ORA/SE-FO/FLA-                    NaN\n5           NaN                   NaN                DO/FLA-CB/                    NaN\n6     2000-1983   2000-02-02 00:00:00  FDA/OGROP/ORA/CE-FO/CHI-                 M EGAN\n7           NaN   2000-02-03 00:00:00                DO/CHI-CB/   BERNSTEIN LIEBHARD &amp;\n8           NaN                   NaN                       NaN             LONDON LLP"""\n    df =  pd.read_csv(StringIO.StringIO(df),sep = "\\s\\s+",engine=\'python\')\n\n    groups = df.groupby(df[\'Control\'].apply(rolling_group),as_index=False)\n    groupFunct = lambda g: pd.Series([joinFunc(g,col) for col in g.columns],index=g.columns)\n    print groups.apply(groupFunct)\n\n     Control                       Recd_Date/Due_Date  \\\n0  2000-1703  2000-01-31 00:00:00,2000-02-29 00:00:00   \n1  2000-1776  2000-01-02 00:00:00,2000-01-03 00:00:00   \n2  2000-1983  2000-02-02 00:00:00,2000-02-03 00:00:00   \n\n                                              Action  \\\n0                                    OC/OER/OPA/PMS/   \n1  OC/ORA/OE/DCP/OC/ORA/ORO/PNC/FDA/OGROP/ORA/SE-...   \n2                 FDA/OGROP/ORA/CE-FO/CHI-DO/CHI-CB/   \n\n                      Signature/Requester  \n0                        M WEBB,DATA CORP  \n1                         G KAN,PALM POST  \n2  M EGAN,BERNSTEIN LIEBHARD &amp; LONDON LLP  \n'
'import re\n\nlines = [\n    "Yes, I\'d say so. Nov 08, 2014 UTC",\n    "Hell yes! Oct 01, 2014 UTC"\n]\n\nfor ln in lines:\n    print re.sub(r\'(\\w+\\s\\d{2}, \\d{4} UTC)$\', r\'\\t\\1\', ln)\n\nYes, I\'d say so.    Nov 08, 2014 UTC\nHell yes!   Oct 01, 2014 UTC\n'
"daily_price_df = daily_price_df.fillna(method='pad', limit=8)\nOR\ndaily_price_df.fillna(method='pad', limit=8, inplace=True)\n"
'library(dplyr)\ndf %&gt;% group_by(ID) %&gt;% filter(Year %in% c(Year - 1, Year + 1))\n# Source: local data frame [4 x 4]\n# Groups: ID [2]\n# \n#      ID  Year     X     Y\n#   (int) (int) (int) (int)\n# 1     2  2011     4     3\n# 2     2  2012     2     2\n# 3     3  2012     2     1\n# 4     3  2013     0     3\n\ndf %&gt;% group_by(ID) %&gt;% filter(!Year %in% c(Year - 1, Year + 1))\n# Source: local data frame [3 x 4]\n# Groups: ID [2]\n# \n#      ID  Year     X     Y\n#   (int) (int) (int) (int)\n# 1     1  2010     2     3\n# 2     1  2012     4     0\n# 3     3  2010     3     1\n'
"In [17]:\npercentile_list.iloc[0].apply(pd.Series)\n\nOut[17]:\n          0  1  2  3\nlst1Tite  1  3  4  5\nlst2Tite  1  2  3  1\nlst3Tite  1  2  3  3\n\nIn [20]:\npd.DataFrame(percentile_list.iloc[0].apply(pd.Series).values, columns = ['col1','col2','col3','col4'])\n\nOut[20]:\n   col1  col2  col3  col4\n0     1     3     4     5\n1     1     2     3     1\n2     1     2     3     3\n\nIn [41]:\nd={}\nfor l in percentile_list.index:\n    d['percentile_' + str(l)] = pd.DataFrame(percentile_list.loc[l].apply(pd.Series).values, columns = ['col1','col2','col3','col4'])\nd\n\nOut[41]:\n{'percentile_0':    col1  col2  col3  col4\n 0     1     3     4     5\n 1     1     2     3     1\n 2     1     2     3     3, 'percentile_1':    col1  col2  col3  col4\n 0     1     2     3     3\n 1     1     4     1     2\n 2     3     2     1     2, 'percentile_2':    col1  col2  col3  col4\n 0     2     3     4     5\n 1     3     3     1     5\n 2     1     3     1     4, 'percentile_3':    col1  col2  col3  col4\n 0     3     4     5     5\n 1     2     4     1     5\n 2     2     4     3     5}\n\nIn [42]:\nd['percentile_0']\n\nOut[42]:\n   col1  col2  col3  col4\n0     1     3     4     5\n1     1     2     3     1\n2     1     2     3     3\n"
'df.loc[:, df.apply(lambda col: max(col.value_counts()) &lt;= df.shape[0]/2)]\n\n#   0   1   2   4\n#0  1   2   9   1\n#1  2   3   3   0\n#2  1   4   2   2\n#3  2   3   1   2\n#4  1   2   3   8\n#5  2   2   5   1\n#6  1   3   8   4\n#7  2   4   7   3\n'
's = pd.Series([\n      "[u\'Basketball\', \'Swimming\', \'Gym\']",\n      "[u\'Gym\', u\'Soccer\', u\'Football\']",\n      "[u\'Ballet\', u\'Basketball\', u\'Volleyball\']"\n    ])\ns\n\n0           [u\'Basketball\', \'Swimming\', \'Gym\']\n1             [u\'Gym\', u\'Soccer\', u\'Football\']\n2    [u\'Ballet\', u\'Basketball\', u\'Volleyball\']\ndtype: object\n\ns.apply(eval)\n\n0         [Basketball, Swimming, Gym]\n1             [Gym, Soccer, Football]\n2    [Ballet, Basketball, Volleyball]\ndtype: object\n\ns.apply(eval).apply(pd.Series)\n'
"column_headers = ['Date', 'Time', 'Duration', 'IP', 'Request']\ndf = pd.DataFrame([], columns = column_headers)\ndf.to_csv('out.log', index=None, sep=';')\n\n# if you don't want to include a header line, skip the previous lines and start here\nfor df in pd.read_csv('data.log', sep='\\s',  header=None, chunksize=6):\n    df.reset_index(drop=True, inplace=True)\n    df.fillna('', inplace=True)\n    d = pd.DataFrame([df.loc[3,0], df.loc[3,1], ' '.join(df.loc[3,4:8]), ' '.join(df.loc[4,4:6]), ' '.join(df.loc[5,4:])])\n    d.T.to_csv('out.log', index=False, header=False, mode='a', sep=';')\n\ncolumn_headers = ['Date', 'Time', 'Duration', 'IP', 'Request']\n\nwith open('data.log') as log, open('out.log', 'w') as out:\n    out.write(';'.join(column_headers)+'\\n') # skip this line if you don't want to include column headers                                                                \n    while True:\n        try:\n            lines = [next(log).strip('\\n').split(' ',4) for i in range(6)][3:]\n            out.write(';'.join(lines[0][:2]+[l[4] for l in lines])+'\\n')\n        except StopIteration:\n            break\n"
'df[df.Amount.groupby(df.Key).apply(lambda x: x == x.min())]\n\n# Key   Amount   Term   Other   Other_2\n#1  A      261  Short     ABC       100\n#2  B      281   Long     CDE       200\n#3  C      140   Long     EFG       300\n\ndf.groupby("Key", group_keys=False).apply(lambda g: g.nsmallest(1, "Amount"))\n\n# Key   Amount   Term   Other   Other_2\n#1  A      261  Short     ABC       100\n#2  B      281   Long     CDE       200\n#3  C      140   Long     EFG       300\n'
'(df1.groupby("Buyer ID")["Product ID"].agg(["first", "last", "count"])\n .set_index(\'count\', append=True).stack()\n .reset_index(level=2, drop=True)\n .rename("Product ID").reset_index().drop_duplicates())\n\n(df1.groupby("Buyer ID", group_keys=False)\n .apply(lambda g: g.iloc[[0,-1], :].assign(count = len(g)))\n .drop_duplicates())\n'
"ourdates = pd.to_datetime(df['timestamp'], format='%d/%b/%Y:%H:%M:%S')\n"
'df.columns = df.columns.str.strip()     \ndf.columns = df.columns.str.replace(\' \', \'_\')         \ndf.columns = df.columns.str.replace(r"[^a-zA-Z\\d\\_]+", "")    \ndf.columns = df.columns.str.replace(r"[^a-zA-Z\\d\\_]+", "")\n'
"for piece in key.split('\\n'):\n    print(piece.strip())\n"
"sc_sub = re.compile('\\W+')\ndf['amenities'] = [sc_sub.sub('', amenity) for amenity in df['amenities']]\n"
"df[['A','B','C','D']] = df[['A','B','C','D']].ffill()\n"
"In [441]: df['code'].str.extract('(\\d*\\.?\\d*)', expand=False).astype(float)\nOut[441]:\n0    10.00\n1    25.00\n2    10.00\n3     6.05\n4     6.01\nName: code, dtype: float64\n"
'from bs4 import BeautifulSoup\nimport urllib.request\n\nURL = "https://www.tripadvisor.com.au/Restaurants-g255068-c8-Brisbane_Brisbane_Region_Queensland.html"\n\ndef get_info(link):\n    response = urllib.request.urlopen(link)\n    soup = BeautifulSoup(response.read(),"lxml")\n    for items in soup.find_all(class_="shortSellDetails"):\n        name = items.find(class_="property_title").get_text(strip=True)\n        bubble = items.find(class_="ui_bubble_rating").get("alt")\n        review = items.find(class_="reviewCount").get_text(strip=True)\n        print(name,bubble,review)\n\nif __name__ == \'__main__\':\n    get_info(URL)\n\nDouble Shot New Farm 4.5 of 5 bubbles 218 reviews\nGoodness Gracious Cafe 4.5 of 5 bubbles 150 reviews\nNew Farm Deli &amp; Cafe 4.5 of 5 bubbles 273 reviews\nCoffee Anthology 4.5 of 5 bubbles 116 reviews\n'
'&gt;&gt;&gt; a = np.array(mylist1)\n&gt;&gt;&gt; a[np.isnan(a)] = mylist2\n&gt;&gt;&gt; a.astype(int).tolist()\n[1, 2, 3, -10, -11, 4, -12, 5]\n'
"df = df[(df.astype(str) != '?').all(axis=1)]\n#alternative solution\n#df = df[~(df.astype(str) == '?').any(axis=1)]\nprint (df)\n   X  Y  Z\n1  1  2  3\n3  4  4  4\n\ndf = df[(df.values != '?').all(axis=1)]\n\nprint (df.astype(str) != '?')\n       X      Y      Z\n0   True   True  False\n1   True   True   True\n2  False  False   True\n3   True   True   True\n4  False   True   True\n\nprint ((df.astype(str) != '?').all(axis=1))\n0    False\n1     True\n2    False\n3     True\n4    False\ndtype: bool\n"
'df = pd.read_csv(FilePath, parse_dates=[\'TIMESTAMP\'], index_col=[\'TIMESTAMP\'])\n\ndf = df.drop([\'RECORD\'],axis=1)\n\ndf = df.apply(lambda x: pd.to_numeric(x, errors=\'coerce\'))\n\nimport pandas as pd\n\npd.options.display.max_columns = 20\n\ntemp=u""""TIMESTAMP","RECORD","WM1_u_ms","WM1_v_ms","WM1_w_ms","WM2_u_ms","WM2_v_ms","WM2_w_ms","WS1_u_ms","WS1_v_ms"\n"2018-04-06;14:31:11.5",29699805,2.628a,4.629a,0.599s,3.908,7.971,0.47,2;;51,7.18\n"2018-04-06;14:31:11.75",29699806,3.264,4.755,-0.095,2.961,6.094,-0.504,2.47,7.18\n"2018-04-06;14:31:12",29699807,1.542,5.793,0.698,4.95,4.91,0.845,2.18,7.5\n"2018-04-06;14:31:12.25",29699808,2.527,5.207,0.012,4.843,6.285,0.924,2.15,7.4\n"2018-04-06;14:31:12.5",29699809,3.511,4.528,1.059,2.986,5.636,0.949,3.29,5.54\n"2018-04-06;14:31:12.75",29699810,3.445,3.957,-0.075,3.127,6.561,0.259,3.85,5.45\n"2018-04-06;14:31:13",29699811,2.624,5.238,-0.166,3.451,7.199,0.242,3.94,a"""\n#after testing replace \'pd.compat.StringIO(temp)\' to \'filename.csv\'\ndf = pd.read_csv(pd.compat.StringIO(temp), parse_dates=[\'TIMESTAMP\'], index_col=[\'TIMESTAMP\'])\n\nprint (df)\n                           RECORD WM1_u_ms WM1_v_ms WM1_w_ms  WM2_u_ms  \\\nTIMESTAMP                                                                \n2018-04-06 14:31:11.500  29699805   2.628a   4.629a   0.599s     3.908   \n2018-04-06 14:31:11.750  29699806    3.264    4.755   -0.095     2.961   \n2018-04-06 14:31:12.000  29699807    1.542    5.793    0.698     4.950   \n2018-04-06 14:31:12.250  29699808    2.527    5.207    0.012     4.843   \n2018-04-06 14:31:12.500  29699809    3.511    4.528    1.059     2.986   \n2018-04-06 14:31:12.750  29699810    3.445    3.957   -0.075     3.127   \n2018-04-06 14:31:13.000  29699811    2.624    5.238   -0.166     3.451   \n\n                         WM2_v_ms  WM2_w_ms WS1_u_ms WS1_v_ms  \nTIMESTAMP                                                      \n2018-04-06 14:31:11.500     7.971     0.470    2;;51     7.18  \n2018-04-06 14:31:11.750     6.094    -0.504     2.47     7.18  \n2018-04-06 14:31:12.000     4.910     0.845     2.18      7.5  \n2018-04-06 14:31:12.250     6.285     0.924     2.15      7.4  \n2018-04-06 14:31:12.500     5.636     0.949     3.29     5.54  \n2018-04-06 14:31:12.750     6.561     0.259     3.85     5.45  \n2018-04-06 14:31:13.000     7.199     0.242     3.94        a  \n\nprint (df.dtypes)\nRECORD        int64\nWM1_u_ms     object\nWM1_v_ms     object\nWM1_w_ms     object\nWM2_u_ms    float64\nWM2_v_ms    float64\nWM2_w_ms    float64\nWS1_u_ms     object\nWS1_v_ms     object\ndtype: object\n\nprint (df.index)\nDatetimeIndex([\'2018-04-06 14:31:11.500000\', \'2018-04-06 14:31:11.750000\',\n                      \'2018-04-06 14:31:12\', \'2018-04-06 14:31:12.250000\',\n               \'2018-04-06 14:31:12.500000\', \'2018-04-06 14:31:12.750000\',\n                      \'2018-04-06 14:31:13\'],\n              dtype=\'datetime64[ns]\', name=\'TIMESTAMP\', freq=None)\n\n\ndf = df.drop([\'RECORD\'],axis=1)\ndf = df.apply(lambda x: pd.to_numeric(x, errors=\'coerce\'))\n\nprint (df)\n                         WM1_u_ms  WM1_v_ms  WM1_w_ms  WM2_u_ms  WM2_v_ms  \\\nTIMESTAMP                                                                   \n2018-04-06 14:31:11.500       NaN       NaN       NaN     3.908     7.971   \n2018-04-06 14:31:11.750     3.264     4.755    -0.095     2.961     6.094   \n2018-04-06 14:31:12.000     1.542     5.793     0.698     4.950     4.910   \n2018-04-06 14:31:12.250     2.527     5.207     0.012     4.843     6.285   \n2018-04-06 14:31:12.500     3.511     4.528     1.059     2.986     5.636   \n2018-04-06 14:31:12.750     3.445     3.957    -0.075     3.127     6.561   \n2018-04-06 14:31:13.000     2.624     5.238    -0.166     3.451     7.199   \n\n                         WM2_w_ms  WS1_u_ms  WS1_v_ms  \nTIMESTAMP                                              \n2018-04-06 14:31:11.500     0.470       NaN      7.18  \n2018-04-06 14:31:11.750    -0.504      2.47      7.18  \n2018-04-06 14:31:12.000     0.845      2.18      7.50  \n2018-04-06 14:31:12.250     0.924      2.15      7.40  \n2018-04-06 14:31:12.500     0.949      3.29      5.54  \n2018-04-06 14:31:12.750     0.259      3.85      5.45  \n2018-04-06 14:31:13.000     0.242      3.94       NaN  \n\nprint (df.dtypes)\nWM1_u_ms    float64\nWM1_v_ms    float64\nWM1_w_ms    float64\nWM2_u_ms    float64\nWM2_v_ms    float64\nWM2_w_ms    float64\nWS1_u_ms    float64\nWS1_v_ms    float64\ndtype: object\n'
"x= ['GA','TA','SA','TA','GA','TA','SA']\n\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing.label import _encode\nfrom sklearn.utils import column_or_1d\nx = column_or_1d(x, warn=True)\nclasses_,encoded_values = _encode(x,uniques=np.array(['GA','TA','SA']),encode=True)\nencoded_values, classes_\n\n#(array([0, 1, 2, 1, 0, 1, 2]), ['GA', 'TA', 'SA'])\n\n#comparing with labelencoder, which will sort the labels before encoding\nle = LabelEncoder()\n\nle.fit_transform(x),le.classes_\n\n#\n(array([0, 2, 1, 2, 0, 2, 1], dtype=int64),\n array(['GA', 'SA', 'TA'], dtype='&lt;U2'))\n"
"df['Col'].str.extract(r'-CC\\s+(.*?)\\s+-')\n"
"from difflib import SequenceMatcher\n'\\n'.join(s for s in string.splitlines() if SequenceMatcher(' '.__eq__, s, substring).ratio() &lt; 0.6)\n\nAdam is a boy who lives in Michigan.\nHe also enjoys playing with his dog and cat.\nAdam is a happy boy.\n"
'import pandas as pd\nimport re\n\ndf[\'algo_new\'] = df.algo.str.findall(f"({ \'|\'.join(ml) })")\n\n&gt;&gt; out\n\n    col1    gender  algo                                                algo_new\n0   usa     male    machine learning and fraud detection are a mus...   [machine learning, fraud detection, clustering]\n1   fr      female  monte carlo method is great and so is hmm,pca,...   [monte carlo method]\n2   arg     male    logistical regression and data management and ...   [logistical regression, data management, fraud..\n'
"trends = ['Bill','Visits', 'Avg. Visits','abc',\n          'mcd', 'mckfd', 'mfd', 'aps', 'mvmv', 'dep']\n\ndf_new = df.loc[df.index.repeat(len(trends))].assign(trend_type=trends * len(df))\nprint(df_new)\n\n   id          email   trend_type\n0   1    abc@xyz.com         Bill\n0   1    abc@xyz.com       Visits\n0   1    abc@xyz.com  Avg. Visits\n0   1    abc@xyz.com          abc\n0   1    abc@xyz.com          mcd\n0   1    abc@xyz.com        mckfd\n0   1    abc@xyz.com          mfd\n0   1    abc@xyz.com          aps\n0   1    abc@xyz.com         mvmv\n0   1    abc@xyz.com          dep\n1   2  cdsm@kcmd.com         Bill\n1   2  cdsm@kcmd.com       Visits\n1   2  cdsm@kcmd.com  Avg. Visits\n1   2  cdsm@kcmd.com          abc\n1   2  cdsm@kcmd.com          mcd\n1   2  cdsm@kcmd.com        mckfd\n1   2  cdsm@kcmd.com          mfd\n1   2  cdsm@kcmd.com          aps\n1   2  cdsm@kcmd.com         mvmv\n1   2  cdsm@kcmd.com          dep\n"
"df.to_csv('clean_soccer.csv', encoding='utf-8-sig')\n\ndf.to_csv('clean_soccer.csv', encoding='utf-8')\n"
'df = pd.read_excel(\'InputFile.xls\', index_col=None)\n\n# Find the first occurrence of "Col1"\ncolumn_row = df.index[df.iloc[:, 0] == "Col1"][0]\n\n# Use this row as header\ndf.columns = df.iloc[column_row]\n\n# Remove the column name (currently an useless index number)\ndel df.columns.name\n\n# Keep only the data after the (old) column row\ndf = df.iloc[column_row + 1:]\n\n# And tidy it up by resetting the index\ndf.reset_index(drop=True, inplace=True)\n'
"import regex\n\ntext = regex.sub(r'\\p{So}+', '', text)\n\nperl -i -CSD -Mutf8 -pe 's/\\p{So}+//g' file\n"
"df.columns.name=''\n\ndf=pd.DataFrame()\ndf['a']=[1,2,3]\ndf.columns.name='name column'\ndf.index.name='name index'\ndf\n\nname column  a\nname index  \n0            1\n1            2\n2            3\n\ndf.columns.name=''\n\n           a\nname index  \n0          1\n1          2\n2          3\n"
"&gt;&gt;&gt; [x for sublist in master_list for x in sublist or [] if x]\n['the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.3.',\n 'the supply fan is running, the VFD speed output mean value is 94.3.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.2.',\n 'the supply fan is running, the VFD speed output mean value is 94.2.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.1.',\n 'the supply fan is running, the VFD speed output mean value is 94.1.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 94.0.',\n 'the supply fan is running, the VFD speed output mean value is 94.0.',\n 'the supply fan speed mean is over 90% like the fan isnt building static, mean value recorded is 93.9.',\n 'the supply fan is running, the VFD speed output mean value is 93.9.']\n"
"str_num = ['309.00 ', ' 38.00 ', ' 12,486.00 ', '6,108.00', ' 2,537.00']\n\nlist(map(lambda s: float (s.replace (',', '')), str_num))\n\n[309.0, 38.0, 12486.0, 6108.0, 2537.0]\n\nmember_clean['TotalDepositBalances'] = member_clean['TotalDepositBalances'].str.replace(',', '')\n\nmember_clean['TotalDepositBalances'] = member_clean['TotalDepositBalances'].replace('$', '').replace(',', '').astype(float)\n"
'year,gender,age,country,population\n2002,F,11-15,BR,0\n2002,F,11-15,CO,1\n2002,F,9-10,BR,1\n2002,F,9-10,CO,2\n2002,M,11-15,BR,1\n2002,M,11-15,CO,0\n2002,M,9-10,BR,0\n2002,M,9-10,CO,1\n2003,F,11-15,BR,0\n2003,F,11-15,CO,0\n2003,F,9-10,BR,1\n2003,F,9-10,CO,1\n2003,M,11-15,BR,0\n2003,M,11-15,CO,0\n2003,M,9-10,BR,0\n2003,M,9-10,CO,2\n2004,F,11-15,BR,1\n2004,F,11-15,CO,1\n2004,F,9-10,BR,1\n2004,F,9-10,CO,1\n2004,M,11-15,BR,0\n2004,M,11-15,CO,0\n2004,M,9-10,BR,0\n2004,M,9-10,CO,0\n'
'id,city,country\n1,Berlin,Germany\n2,Paris,France\n3,New York,USA\n4,Frankfurt,Germany\n\nid,city,country\n1,Berlin,Germany\n4,Frankfurt,Germany\n\nid,city,country\n2,Paris,France\n\nid,city,country\n3,New York,USA\n'
"(df1.assign(index2 = df1.groupby(['Winner','Loser']).cumcount())\n    .merge(df2.assign(index2 = df2.groupby(['Winner','Loser']).cumcount()),\n           on = ['Winner','Loser', 'index2'])\n    .drop(columns = 'index2'))\n\ndf1.merge(df2 ,on =['Winner','Loser']).drop_duplicates()\n"
'df[\'city\'] = df[\'city\'].str.extract(\'{"(.+?)"\')\nprint(df)\n\n    city\n0  Hefei\n'
"counts = my_df['Neighborhood'].value_counts()\nnew_df = my_df.loc[my_df['Neighborhood'].isin(counts.index[counts &gt; 50])]\n"
"df_new[df_new['Job'].str.split(r'/').str[1].str.len().eq(2.)]\n\n# corrected with @jon's remarks\ndf_new[df_new['Job'].str.contains(r'^.{3}/.{2}/.{3}$',na=False)] \n"
"data['fatalities'] = pd.eval(data['fatalities'])\nprint (data)\n  fatalities\n0          1\n1          4\n2         10\n3          9\n4          5\n5         11\n6         16\n7          9\n\ndata['fatalities'] = data['fatalities'].apply(pd.eval)\n"
"s = (\n    df['RespondentID'].ne(df['RespondentID'].shift())                # Condition 1\n    | df.Purpose.eq(1).shift().fillna(False)                         # Condition 2\n    | (df.Purpose.eq(7) &amp; df.TripNumber.eq(1)).shift().fillna(False) # Condition 3\n    )\n\ndf['JourneyID'] = s.cumsum()\n"
'a =  df.groupby("Date").apply(lambda x: x[x["Type"]=="Actual"] if (x["Type"]=="Actual").any() else x[x["Type"]=="Forecast"]).reset_index(drop=True)\n'
"import pandas as pd\ndf = pd.read_csv('./datasets/apps.csv', index_col=0)\nmode_dict = dict(df.groupby('Category')['Android Ver'].agg(lambda x: x.mode()))\ndf['Android Ver'].loc[df['Android Ver'].isna()] = df.loc[df['Android Ver'].isna()].apply(lambda x: mode_dict[x.Category],axis=1)\n"
'csv = &quot;&quot;&quot;\\\none;two;three\n1;2;3;\n4;5;6;\n&quot;&quot;&quot;\ndf1 = pd.read_csv(StringIO(csv), sep=&quot;;&quot;, index_col=False)\nprint (df1)\n   one  two  three\n0    1    2      3\n1    4    5      6\n'
"def fwd_fill_gaps(df, col, gap_max):\n    &quot;&quot;&quot; Fill conseuctive NaN when size is &lt;= gap_max &quot;&quot;&quot;\n\n    s = df[col].notnull().cumsum().where(df[col].isnull())\n    # Only True for NaN gaps of size &lt;= gap_max\n    s = s.groupby(s).transform('size').le(gap_max)\n\n    return df[col].fillna(df[col].ffill().where(s), downcast='infer')\n\n\nfor col in ['A', 'B', 'C']:\n    df[col] = fwd_fill_gaps(df, col, gap_max=3)\n\n                        A    B        C\n2019-06-17 00:00:00  8001  201  11991.0\n2019-06-17 00:01:01  7999  209  15631.0\n2019-06-17 00:02:00  7998  298  47998.0\n2019-06-17 00:03:04  7998  300  38030.0\n2020-06-17 00:04:00  9900  300  19900.0\n2020-06-17 00:05:00  9342  342  29342.0\n2020-06-17 00:06:00  9324  324      NaN\n2020-06-17 00:07:00  8534  854      NaN\n2020-06-17 00:08:00  8358  858      NaN\n2020-06-17 00:09:00  9457  457      NaN\n2020-06-17 00:10:00  9457  145  27245.0\n2020-06-17 00:11:00  8999  189  28999.0\n2020-06-17 00:12:00  8492  192  28492.0\n2020-06-17 00:13:00  8492  134  29334.0\n2020-06-17 00:14:00  8492  135  28234.0\n"
"df = df[df.columns[~df.columns.isin(['Rows','Label','v9'])]]\ndf.v1.dot(df.v1)\n\n     v1  v2  v3  v4  v5  v6  v7  v8  v10\nv1    2   0   0   0   0   1   0   0    2\nv2    0   2   0   1   0   0   0   1    0\nv3    0   0   1   0   0   0   0   0    0\nv4    0   1   0   2   0   0   0   1    1\nv5    0   0   0   0   1   0   0   0    0\nv6    1   0   0   0   0   1   0   0    1\nv7    0   0   0   0   0   0   2   0    0\nv8    0   1   0   1   0   0   0   1    0\nv10   2   0   0   1   0   1   0   0    3\n"
"def replace_all(text, dictReplace): #Made dictReplace as second parameter\n    rep = dict((re.escape(k), v) for k, v in dictReplace.items())\n    pattern = re.compile(&quot;|&quot;.join(rep.keys()))\n    text = pattern.sub(lambda m: rep[re.escape(m.group(0))], text) \n    return text\n\ndictRepStrings = {&quot;1st lien&quot;: &quot;1l&quot;, &quot;first lien&quot;: &quot;1l&quot;, &quot;2nd lien&quot;: &quot;2l&quot;, &quot;second lien&quot;: &quot;2l&quot;, &quot;term loan&quot;: &quot;tl&quot;}\ndf['NewCol'] = df['InvestmentDesc'].apply(replace_all, args=[dictRepStrings]) #modified apply function with args\ndf\n"
"df['DOB'] = pd.to_datetime(df['DOB'], errors='coerce')\n\nout = df['DOB'].isnull().sum()\n"
"import pandas as pd\nimport numpy as np\n\n# example dataframe\ndf = pd.DataFrame({\n    'A': [1, 2, 3],\n    'B': [4, 5, np.nan],\n    'C': [np.nan, np.nan, 6],\n    'D': [np.nan, np.nan, np.nan]\n})\n\n\n   A    B    C   D\n0  1  4.0  NaN NaN\n1  2  5.0  NaN NaN\n2  3  NaN  6.0 NaN\n\ndf.dropna(thresh=2, axis=1)\n\n   A    B\n0  1  4.0\n1  2  5.0\n2  3  NaN\n"
"df = pd.DataFrame({'Facilities':[\n[{'name': 'Work from home', 'icon': 'WFH.svg'}],\n[{'name': 'Gymnasium', 'icon': 'Gym.svg'}, {'name': 'Cafeteria', 'icon': 'Cafeteria.svg'}, {'name': 'Work from home', 'icon': 'WFH.svg'}],\n[{'name': 'Free food', 'icon': 'FreeFood.svg'}, {'name': 'Team outings', 'icon': 'TeamOuting.svg'}, {'name': 'Education assistance', 'icon': 'Education.svg'}],\n[{'name': 'Soft skill training', 'icon': 'SoftSkillsTraining.svg'}, {'name': 'Job training', 'icon': 'JobTraining.svg'}],\n[{'name': 'Free transport', 'icon': 'Transportation.svg'}, {'name': 'Work from home', 'icon': 'WFH.svg'}, {'name': 'Team outings', 'icon': 'TeamOuting.svg'}, {'name': 'Soft skill training', 'icon': 'SoftSkillsTraining.svg'}],\n    ]})\n\nprint(df)\n\n                                          Facilities\n0    [{'name': 'Work from home', 'icon': 'WFH.svg'}]\n1  [{'name': 'Gymnasium', 'icon': 'Gym.svg'}, {'n...\n2  [{'name': 'Free food', 'icon': 'FreeFood.svg'}...\n3  [{'name': 'Soft skill training', 'icon': 'Soft...\n4  [{'name': 'Free transport', 'icon': 'Transport...\n\ndf['Facilities'] = df['Facilities'].apply(lambda x: [d['name'] for d in x])\nprint(df)\n\n                                          Facilities\n0                                   [Work from home]\n1             [Gymnasium, Cafeteria, Work from home]\n2    [Free food, Team outings, Education assistance]\n3                [Soft skill training, Job training]\n4  [Free transport, Work from home, Team outings,...\n"
"cols = ['Val','Dist']\ndf[cols] =  df[cols].fillna(df.groupby(df.Date.dt.floor('H'))\n                              [cols].transform('median')\n                           )\n\n                  Date       Val      Dist\n0  2020-09-24 00:00:00  1.764052  0.864436\n1  2020-09-24 00:12:00  0.400157  0.653619\n2  2020-09-24 00:24:00  0.978738  0.864436\n3  2020-09-24 00:36:00  2.240893  0.864436\n4  2020-09-24 00:48:00  1.867558  2.269755\n5  2020-09-24 01:00:00  0.153690  0.757559\n6  2020-09-24 01:12:00  0.950088  0.045759\n7  2020-09-24 01:24:00 -0.151357 -0.187184\n8  2020-09-24 01:36:00 -0.103219  1.532779\n9  2020-09-24 01:48:00  0.410599  1.469359\n10 2020-09-24 02:00:00  0.144044  0.154947\n11 2020-09-24 02:12:00  1.454274  0.378163\n12 2020-09-24 02:24:00  0.761038  0.154947\n13 2020-09-24 02:36:00  0.121675  0.154947\n14 2020-09-24 02:48:00  0.443863 -0.347912\n15 2020-09-24 03:00:00  0.333674  0.156349\n16 2020-09-24 03:12:00  1.494079  1.230291\n17 2020-09-24 03:24:00 -0.205158  1.202380\n18 2020-09-24 03:36:00  0.313068 -0.387327\n19 2020-09-24 03:48:00  0.323371 -0.302303\n"
'df[&quot;GDP&quot;].fillna(\n    df.groupby([&quot;Country&quot;, &quot;Region&quot;])[&quot;GDP&quot;].transform(\n        lambda x: x.mode()[0]\n    )\n)\n'
"def fix(s):\n    if s['B']=='cherry':\n        s['A']=s['A'].replace('//,',',')\n    return s\n\ndf.apply(fix, axis=1)\n\n        A       B\n0    foo,  cherry\n1  bar//,  orange\n2    bar,  cherry\n3     bar   apple\n"
"laptops = pd.DataFrame({&quot;processor_speed_ghz&quot;:[2.0,3.0, 2.0, 5.0, 3.0, 3.0]})\nprint (laptops)\n   processor_speed_ghz\n0                  2.0\n1                  3.0\n2                  2.0\n3                  5.0\n4                  3.0\n5                  3.0\n\nprint(laptops[&quot;processor_speed_ghz&quot;].value_counts())\n3.0    3\n2.0    2\n5.0    1\nName: processor_speed_ghz, dtype: int64\n\nprint(laptops[&quot;processor_speed_ghz&quot;].value_counts().index)\nFloat64Index([3.0, 2.0, 5.0], dtype='float64')\n"
"' | '.join(new_line.split(','))\n"
'(class|def)(.+)\\s+("""[\\w\\s\\(\\)\\-\\,\\;\\:]+""")\n\n(class|def)(.+)\\s+("""[\\w\\s(),;:-]+""")\n\n(class|def)(.+)\\s+("{3}[\\w\\s(),;:-]+"{3})\n                    ^^^              ^^^\n\n(class|def)(.+)\\s+"{3}[\\w\\s(),;:-]+"{3}\n                 ^^                   ^^\n\n\\b(class|def)(.+)\\s+"{3}[\\w\\s(),;:-]+"{3}\n^^\n\n\\b(class.+|def.+)\\s+"{3}[\\w\\s(),;:-]+"{3}\n  ^^^^^^^^^^^^^^^\n\n\\b(def.+|class.+)\\s+"{3}[\\w\\s(),;:-]+"{3}\n  ^^^^^^^^^^^^^^^\n'
"d= {'Company_Name':'first','2019_Amt':'sum','2019_Amt':'sum',\n               '2020_Amt':'sum','Code':'first','Flag':'first'}\ngrouper = df['Company_Name'].str.split().str[0]\n\nout = df.drop_duplicates().groupby(grouper).agg(d).reset_index(drop=True)\nprint(out)\n\n               Company_Name  2019_Amt  2020_Amt Code Flag\n0           ABC Company Ltd      2000       400    A    Y\n1  DEFGHIJ Company (London)      2580      1800    B    N\n2              KLM Services      9000      7000    A    Y\n3           NOPQ Industries      7300      8400    C    Y\n"
"from collections import Counter\nmaster_wordlist = ['dog', 'cat', 'hat', 'bat', 'big']\narticle_a = ['dog', 'cat', 'dog','big']\narticle_b = ['dog', 'hat', 'big', 'big', 'big']\n\nc_a = Counter(article_a)\nc_b = Counter(article_b)\n\nprint [c_a[x] for x in master_wordlist]\nprint [c_b[x] for x in master_wordlist]\n\n[2, 1, 0, 0, 1]\n[1, 0, 1, 0, 3]\n"
"&gt;&gt;&gt; df\n  Name antiguo completado\n0  ssd       X          X\n1  adf       B        NaN\n2  dsf       C          C\n3  eee     NaN        NaN\n4  wqe     NaN          C\n&gt;&gt;&gt; df['antiguo'].fillna(df['completado'])\n0      X\n1      B\n2      C\n3    NaN\n4      C\nName: antiguo, dtype: object\n"
"id       address\n 1  IP:123.1.1.1\n 2  IP:456.1.1.1\n 3  IP:789.1.1.1\n\ndct = { 'address': lambda x: x.replace('IP:','') }\n\ndf = pd.read_csv( 'foo.txt', delimiter=' *', converters=dct )\n\n   id    address\n0   1  123.1.1.1\n1   2  456.1.1.1\n2   3  789.1.1.1\n"
"import datetime as dt\n\ndf = pd.DataFrame({'deviceId': {0: '1224EG12', 1: '1224EG13'},\n 'readingtime': {0: {u'$date': u'2014-11-04T17:27:50.000+0000'},\n  1: {u'$date': u'2014-11-04T17:27:50.000+0000'}}})\n\n&gt;&gt;&gt; df\n   deviceId                                  readingtime\n0  1224EG12  {u'$date': u'2014-11-04T17:27:50.000+0000'}\n1  1224EG13  {u'$date': u'2014-11-04T17:27:50.000+0000'}\n\n\n&gt;&gt;&gt; df.readingtime.apply(lambda x: dt.datetime.strptime(x['$date'][:-7], \n                                                        '%Y-%m-%dT%H:%M:%S.%f')) \n0   2014-11-04 17:27:50\n1   2014-11-04 17:27:50\nName: readingtime, dtype: datetime64[ns]\n"
"if value == None:\n   print 'ok'\nelif value == NotImplementedError:\n    print 'ok'\nelif '\\n' in str(value):\n   value = str(value)    \n   value.replace('\\n',' ')\nelif '\\r' in str(value):\n   value = str(value)    \n   value.replace('\\r',' ')\n"
'require(data.table) # v1.9.5+\ncols1 = names(df1)[2:4]\ncols2 = names(df2)[2:4]\n\nfoo &lt;- function(x, y) {\n    nas = is.na(x)\n    x[nas] = y[nas]\n    x\n}\nsetDT(df1)[df2, c(cols1, cols2) := c(Map(foo, mget(cols1), \n                   mget(cols2)), mget(cols2)), on = "ID"]\n\n&gt; df1\n#          ID Aa Ab Ac no.match Ba Bb Bc\n# 1: Person.A  0  0  0        0  0 NA  0\n# 2: Person.B  1  1  1        1 NA  1  1\n# 3: Person.C  2  2  2        2  2  2  2\n# 4: Person.D  1  1  2        2  1  2  2\n# 5: Person.E  1  1  1        1  1  1  1\n# 6: Person.F  1  1  1        2 NA NA NA\n'
"data.ix[(data.OUT.str.contains('nan')), 'OUT'] = np.nan\n\ndata['OUT'] = pd.to_datetime(data['OUT'], errors='coerce')\nprint data\n      CARD    IN Date                  IN   OUT Date                 OUT\n0   100001 2015-04-30 2015-04-30 14:19:18 2015-01-05 2015-01-05 00:10:56\n1   100002 2015-04-30 2015-04-30 11:27:52        NaT                 NaT\n2   100003 2015-04-30 2015-04-30 17:59:47 2015-01-05 2015-01-05 04:51:52\n3   100004 2015-04-30 2015-04-30 16:15:25        NaT                 NaT\n4   100005 2015-04-30 2015-04-30 10:25:13 2015-01-05 2015-01-05 01:25:13\n5   100006 2015-04-30 2015-04-30 16:59:10        NaT                 NaT\n6   100007 2015-04-30 2015-04-30 13:22:06        NaT                 NaT\n7   100008 2015-04-30 2015-04-30 09:15:29        NaT                 NaT\n8   100009 2015-04-30 2015-04-30 17:01:10 2015-01-05 2015-01-05 01:51:01\n9   100010 2015-04-30 2015-04-30 13:13:30 2015-01-05 2015-01-05 01:37:28\n10  100011 2015-04-30 2015-04-30 09:37:28 2015-01-05 2015-01-05 00:37:28\n11  100012 2015-04-30 2015-04-30 18:55:44 2015-01-05 2015-01-05 03:22:22\n12  100013 2015-04-30 2015-04-30 14:28:16 2015-01-05 2015-01-05 01:27:18\n13  100014 2015-04-30 2015-04-30 09:02:13 2015-01-05 2015-01-05 00:02:13\n14  100015 2015-04-30 2015-04-30 09:04:10 2015-01-05 2015-01-05 00:04:10\n15  100016 2015-04-30 2015-04-30 18:51:56 2015-01-05 2015-01-05 09:51:56\n16  100017 2015-04-30 2015-04-30 09:12:51 2015-01-05 2015-01-05 00:12:51\n17  100018 2015-04-30 2015-04-30 10:40:31 2015-01-05 2015-01-05 01:40:31\n18  100019 2015-04-30 2015-04-30 10:35:56 2015-01-05 2015-01-05 01:35:56\n19  100020 2015-04-30 2015-04-30 17:50:03 2015-01-05 2015-01-05 03:54:54\n20  100021 2015-04-30 2015-04-30 17:00:16 2015-01-05 2015-01-05 02:45:35\n21  100022 2015-04-30 2015-04-30 11:18:41 2015-01-05 2015-01-05 01:15:52\n"
"import numpy as np\nimport scipy.stats\nimport pandas as pd\n\nnp.random.seed(0)\ndf = pd.DataFrame(np.random.randn(7, 5), columns=list('ABCDE'))\ndf.iat[1, 0] = np.nan\ndf.iat[3, 1] = np.nan\ndf.iat[5, 2] = np.nan\n\n&gt;&gt;&gt; df\n          A         B         C         D         E\n0  1.764052  0.400157  0.978738  2.240893  1.867558\n1       NaN  0.950088 -0.151357 -0.103219  0.410599\n2  0.144044  1.454274  0.761038  0.121675  0.443863\n3  0.333674       NaN -0.205158  0.313068 -0.854096\n4 -2.552990  0.653619  0.864436 -0.742165  2.269755\n5 -1.454366  0.045759       NaN  1.532779  1.469359\n6  0.154947  0.378163 -0.887786 -1.980796 -0.347912\n\nmask = df.notnull().all(axis=1), ['A', 'B']\ndf.loc[mask] = scipy.stats.mstats.winsorize(df.loc[mask].values, limits=0.4)\n\n&gt;&gt;&gt; df\n          A         B         C         D         E\n0  0.400157  0.400157  0.978738  2.240893  1.867558\n1       NaN  0.950088 -0.151357 -0.103219  0.410599\n2  0.378163  0.400157  0.761038  0.121675  0.443863\n3  0.333674       NaN -0.205158  0.313068 -0.854096\n4  0.378163  0.400157  0.864436 -0.742165  2.269755\n5 -1.454366  0.045759       NaN  1.532779  1.469359\n6  0.378163  0.378163 -0.887786 -1.980796 -0.347912\n"
'import csv\nwith open(\'temperature.csv\') as csvfile:\n    reader = csv.DictReader(csvfile)\n    for row in reader:\n        print(row)\n        ...\n\n{"outlook":"overcast", "temperature":"hot", "humidity":"high","windy":"FALSE","result":"yes"},\n...\n'
"c = ('${title}', 'VIDEO_TITLE')\nstring_check = p['title'].map(lambda x: x if not isinstance(x, list) else x[0])\nstring_check = string_check.map(lambda s: any(c_str in s for c_str in c))\np.loc[string_check, 'title'] = 'NA'\n"
'leading_whitespace_pattern = re.compile(r"^( {4,}|\\t( |\\t)*).*?$", re.MULTILINE)\nbacktick_pattern = re.compile(r"```.*?```", re.DOTALL)\n\ncode_pattern = re.compile(\'&lt;pre&gt;.*?&lt;/pre&gt;\', re.DOTALL)\nat_pattern = re.compile(r"@.*?@")\n'
'df = pd.DataFrame({"FDT_DATE":[1417390467000, 1417390428000, 1417390608000, 1417390548000,\n    1417390668000, 1417390717000, 1417390758000, 1417390798000, 1417390818000,\n    1417390827000, 1417390907000], "FFLT_LATITUDE":[31.2899, 31.291, 31.2944, 31.294,\n    31.2954, 31.2965, 31.2946, 31.2932, 31.294, 31.2946, 31.2952],\n    "FFLT_LONGITUDE":[121.4845, 121.4859, 121.4857, 121.485, 121.4886, 121.4937,\n    121.494, 121.496, 121.4966, 121.4974, 121.4986],\n    "FINT_STAT":[0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0],\n    "FSTR_ID":[112609, 112609, 112609, 112609, 112609, 112609, 112609, 112609,\n    112609, 112609, 112609]})\n\ndf = df.sort([\'FDT_DATE\']).reset_index(drop=True).reset_index()\n\ndef func(x):\n    global a\n    global b\n    if (x[\'index\'] - x[\'FINT_STAT\']) != x[\'index\']:\n        return a\n    else:\n        b += 1\n        a = b\n\n# Create \'t1\' column for filter "1" groups in \'FINT_STAT\' column\na = 0\nb = 0\ndf[\'t1\'] = df[[\'index\', \'FINT_STAT\']].apply(lambda x: func(x), axis=1)\n\n# Initialize result dataframe\ndf_res = df.drop_duplicates(subset=[\'t1\'])[[\'FSTR_ID\', \'FDT_DATE\', \'t1\']].copy()\\\n    .reset_index(drop=True)\ndf_res = df_res.dropna().reset_index(drop=True)\n\n# First create \'POLYLINE\' column then convert it into \'object\'\ndf_res[\'POLYLINE\'] = np.nan\ndf_res[\'POLYLINE\'] = df_res[\'POLYLINE\'].astype(object)\n\n# Inserting list into dataframe is available with \'pd.DataFrame.set_value()\nfor i in df[\'t1\'].dropna().unique():\n    df_res.set_value(df_res.loc[df_res[\'t1\'] == i, \'t1\'].index.tolist()[0], \'POLYLINE\',\n         df.loc[df[\'t1\'] == i, [\'FFLT_LATITUDE\', \'FFLT_LONGITUDE\']].values.tolist())\n\ndf_res = df_res.drop([\'t1\'], axis=1)\n\n   FSTR_ID       FDT_DATE                                                                            POLYLINE\n0   112609  1417390548000  [[31.294, 121.485], [31.2944, 121.4857], [31.2954, 121.4886], [31.2965, 121.4937]]\n1   112609  1417390798000                       [[31.2932, 121.496], [31.294, 121.4966], [31.2946, 121.4974]]\n'
"In [1]: import pandas as pd\n\nIn [2]: df = pd.DataFrame([0, float('nan'), '.942.197'], columns=['load'])\n\nIn [3]: df\nOut[3]: \n       load\n0         0\n1       NaN\n2  .942.197\n\nIn [4]: pd.to_numeric(df['load'], errors='coerce')\nOut[4]: \n0    0.0\n1    NaN\n2    NaN\nName: load, dtype: float64\n\nIn [5]: df = pd.DataFrame([0, float('nan'), '123.456'], columns=['load'])\n\nIn [6]: df\nOut[6]: \n      load\n0        0\n1      NaN\n2  123.456\n\nIn [7]: pd.to_numeric(df['load'], errors='coerce')\nOut[7]: \n0      0.000\n1        NaN\n2    123.456\nName: load, dtype: float64\n"
"df_split_num = df['Issn'].map(lambda x: x.split('ISSN ')[1].split(', '))\ndf_dash_num = df_split_num.map(lambda x: [num[:4] + '-' + num[4:] for num in x])\n\ndf_split_issn = pd.DataFrame(data=list(df_dash_num), columns=['Issn1', 'Issn2'])\ndf[['Issn1', 'Issn2']] = df_split_issn\ndel df['Issn']\n"
'df1=df1[df1.gender=="female"]\n#reset index with parameter drop if need new monotonic index (0,1,2,...)\ndf1=df1.reset_index(drop=True)\n\ndf1.address = df1.address.str.replace(r"[^a-zA-Z.,\' ]",r\' \')\n'
"df.loc[df.groupby('ID', as_index=False).apply(lambda g: g.isnull().sum(axis=1).idxmin())]\n\n#   ID  Customer     Status  Score  Size\n#1  1     Cust-A      Valid  100.0     A\n#2  2     Cust-B    Invalid   80.0     B\n#4  3     Cust-C      Valid   95.0     C\n#5  4     Cust-D    Invalid   76.0   NaN\n"
'from itertools import product\n\n# Outer merge and drop the unwanted column\ndf = pd.merge(df1, df2, left_on=[\'Buyer ID\', \'Product\'], right_on=[\'Buyer Num\', \'Product\'], \n              how=\'outer\').drop(\'Buyer Num\', axis=1)\n\n# Generate cartesian product of \'Buyer ID\' &amp; \'Price\' after retrieving unique values \nmidx = product(df1[\'Buyer ID\'].unique(), df1[\'Price\'].unique())\n# Set the earlier columns as index and reindex based on the obtained cartesian product values\nd = df.set_index([\'Buyer ID\', \'Price\']).reindex(midx)\n# Fill Nans in \'Product\' with the finite value in each sub-group of level 1 grouped index\nd[\'Product\'].fillna(d.groupby(level=\'Price\')[\'Product\'].transform(\'first\'), inplace=True)\n# Fill the remaining Nans with "No"\nd.fillna(\'No\').sort_values(\'Product\').reset_index()\n'
"df.dir[df.dir == df.dir // 1]\n\ndf = pd.DataFrame(dict(dir=[1, 1.5, 2, 2.5]))\nprint(df)\n\n   dir\n0  1.0\n1  1.5\n2  2.0\n3  2.5\n\ndf.assign(floor_div=df.dir // 1)\n\n   dir  floor_div\n0  1.0        1.0\n1  1.5        1.0\n2  2.0        2.0\n3  2.5        2.0\n\ndf.assign(\n    floor_div=df.dir // 1,\n    is_int=df.dir // 1 == df.dir\n)\n\n   dir  floor_div is_int\n0  1.0        1.0   True\n1  1.5        1.0  False\n2  2.0        2.0   True\n3  2.5        2.0  False\n\ndf.dir[df.dir == df.dir // 1]\n\n0    1.0\n2    2.0\nName: dir, dtype: float64\n\ndf.dir = pd.to_numeric(df.dir, 'coerce')\ndf.dir[df.dir == df.dir // 1]\n"
'\'Selector 1\': [ x.replace(\'A\', \'B\') for x in response.xpath(\'...\').extract() ]\n\n"A,B,C".split(",") # returns [ "A", "B", "C" ]\n'
"df = pd.DataFrame({'owner1_name':['THOMAS MARY D', 'JOE Long', 'MARY Small']})\n\nsplitted = df['owner1_name'].str.split()\ndf['owner1_first_name'] = splitted.str[0]\ndf['owner1_last_name'] = splitted.str[-1]\ndf['owner1_middle_name'] = splitted.str[1]\ndf['owner1_middle_name'] = df['owner1_middle_name']\n                             .mask(df['owner1_middle_name'] == df['owner1_last_name'], '')\nprint (df)\n     owner1_name owner1_first_name owner1_last_name owner1_middle_name\n0  THOMAS MARY D            THOMAS                D               MARY\n1       JOE Long               JOE             Long                   \n2     MARY Small              MARY            Small  \n\nsplitted = df['owner1_name'].str.split()\ndf['owner1_first_name'] = splitted.str[0]\ndf['owner1_last_name'] = splitted.str[-1]\nmiddle = splitted.str[1] \ndf['owner1_middle_name'] = middle.mask(middle == df['owner1_last_name'], '')\nprint (df)\n     owner1_name owner1_first_name owner1_last_name owner1_middle_name\n0  THOMAS MARY D            THOMAS                D               MARY\n1       JOE Long               JOE             Long                   \n2     MARY Small              MARY            Small                   \n\ndf = pd.DataFrame({'owner1_name':['THOMAS MARY-THOMAS', 'JOE LongJOE', 'MARY Small']})\n\nsplitted = df['owner1_name'].str.split()\ndf['a'] = splitted.str[0]\ndf['b'] = splitted.str[-1]\n\ndf['c'] = df.apply(lambda x: x['b'].replace(x['a'], ''), axis=1)\nprint (df)\n          owner1_name       a            b      c\n0  THOMAS MARY-THOMAS  THOMAS  MARY-THOMAS  MARY-\n1         JOE LongJOE     JOE      LongJOE   Long\n2          MARY Small    MARY        Small  Small\n\ndf['owner1_first_name'] = df['owner1_name'].str.split().str[0]\ndf['owner1_last_name'] = df.apply(lambda x: x['owner1_name'].split()\n[-1].replace(x['owner1_first_name'], ''), axis=1)\ndf['owner1_middle_name'] = df.apply(lambda x: \nx['owner1_name'].replace(x['owner1_first_name'], \n'').replace(x['owner1_last_name'], ''), axis=1)\n"
"import collections\nimport numpy as np\nimport pandas as pd\ndf = pd.read_excel('test.xlsx', sheetname='Sheet1')\ndf_dict = collections.defaultdict(list)\nfor i in df['**Hotel Info**']:\n    i_list = i.split('    ') #split with multiple spaces (&amp;nbsp;&amp;nbsp;)\n    df_dict['**Hotel Open**'].append([e.split('open')[0].strip() for e in i_list if 'open' in e])\n    df_dict['**Hotel Retrofit**'].append([e.split('retrofit')[0].strip() for e in i_list if 'retrofit' in e])\n    df_dict['**Hotel Rooms**'].append([e.split('rooms')[0].strip() for e in i_list if 'rooms' in e])\ndf_dict['**Hotel Open**']=[np.nan if len(item)==0 else int(item[0]) for item in df_dict['**Hotel Open**']]\ndf_dict['**Hotel Retrofit**']=[np.nan if len(item)==0 else int(item[0]) for item in df_dict['**Hotel Retrofit**']]\ndf_dict['**Hotel Rooms**']=[np.nan if len(item)==0 else int(item[0]) for item in df_dict['**Hotel Rooms**']]\nnew_df = pd.DataFrame(df_dict)\nnew_df\n\n    **Hotel Open**  **Hotel Retrofit**  **Hotel Rooms**\n0   2014            2016                50\n1   NaN             NaN                 60\n2   2012            NaN                 100\n3   NaN             NaN                 80\n4   2010            NaN                 NaN\n"
'library(dplyr)\nlibrary(tidyr)\n\nstr &lt;- "\ntime, time1, time,  time2,  time, time3\nbuy,       , buy,        ,  buy,    \nfactor1,  1, factor1,    2, factor1,  3\nfactor2,  4, factor2,    5, factor2,  6\nfactor1,  7, factor1,    8, factor1,  9\nfactor2, 10, factor2,   11, factor2, 12\nfactor1, 13, sell,        , factor1, 14\nfactor2, 15, factor1,   16, factor2, 17\nsell,      , factor2,   18, factor1, 19\nfactor1, 20, ,            , factor2, 21,\nfactor2, 22, ,            , sell,     \n,          , ,            , factor1, 23\n,          , ,            , factor2, 24\n,          , ,            , factor1, 25\n,          , ,            , factor2, 26\n"\n\nstrfile &lt;- textConnection(str)\n\nraw &lt;- read.table(strfile, header = F, sep = ",", stringsAsFactors = F)\n\nlibrary(dplyr)\nlibrary(tidyr)\ndt &lt;- do.call(rbind, lapply(1:3, function(x) {\n  p &lt;- raw[,c(x*2-1,x*2)]\n  names(p) &lt;- c(\'factor\', \'value\')\n  p$time &lt;- x\n  p\n  })\n)\n\ndt %&gt;% \n  mutate(position = if_else(trimws(factor) %in% c(\'buy\',\'sell\'),as.character(factor),as.character(NA)),\n         value = as.numeric(value)) %&gt;%\n  fill(position) %&gt;% filter(!is.na(value))\n\nfactor value time position\n1   factor1     1    1      buy\n2   factor2     4    1      buy\n3   factor1     7    1      buy\n4   factor2    10    1      buy\n5   factor1    13    1      buy\n6   factor2    15    1      buy\n7   factor1    20    1     sell\n8   factor2    22    1     sell\n9   factor1     2    2      buy\n10  factor2     5    2      buy\n11  factor1     8    2      buy\n12  factor2    11    2      buy\n13  factor1    16    2     sell\n14  factor2    18    2     sell\n15  factor1     3    3      buy\n16  factor2     6    3      buy\n17  factor1     9    3      buy\n18  factor2    12    3      buy\n19  factor1    14    3      buy\n20  factor2    17    3      buy\n21  factor1    19    3      buy\n22  factor2    21    3      buy\n23  factor1    23    3     sell\n24  factor2    24    3     sell\n25  factor1    25    3     sell\n26  factor2    26    3     sell\n'
"dfKey.merge(df, left_index=True, right_index=True, how='outer')\n\n     Ticker              Name  Metric1  Metric999\nCode                                             \n200     CRM        SalesForce    350.0     375.00\n250    NVID        NVIDA Corp      1.4       1.20\n875    TSLA     Tesla Company      0.2       0.22\n1200   ATVI        Activision    500.0     505.00\n2899  GOOGL        Googlyness      NaN        NaN\n5005     GE  General Electric      NaN        NaN\n"
'(?:(?P&lt;age&gt;[0-9]+) +)?(?:(?P&lt;birthday&gt;\\d\\d\\d\\d\\-\\d\\d\\-\\d\\d) +)?(?P&lt;name&gt;[\\w ]+)\n'
"df['date'] = df['date'].replace('1900-01-01',np.nan).ffill()\n\n&gt;&gt;&gt; df\n   id        date  quantity\n0   1  2017-08-01        22\n1   2  2017-08-01        31\n2   3  2017-08-01        44\n3   4  2017-08-02        12\n4   5  2017-08-02        22\n5   6  2017-08-02        31\n6   7  2017-08-02        44\n7   8  2017-08-03        12\n"
"df[df[['W', 'L']].ne(0).all(1)]\n\n\nplayerID        W   L\n0   aardsda01   2   5\n1   aasedo01    3   8\n3   abernte02   8   19\n"
"df[df.status == 'a'].groupby('code').size()\n\ncode\nAZ    1\nMO    1\nNV    1\nNY    2\ndtype: int64\n"
"d1 = {'Year': \n['2008','2008','2008','2008','2008','2008','2008','2008','2008','2008'],\n  'Month':['1','1','2','6','7','8','8','11','12','12'],\n'Day':['6','22','6','18','3','10','14','6','16','24'],\n'Subject_A':['','30','','45','','35','','','',''],\n'Subject_B':['','','','','','','','40','',''],\n'Subject_C': ['','','','','','65','','50','','']}\nd1 = pd.DataFrame(d1)\nd1 = pd.DataFrame(d1)\n## Create a variable named date\nd1['date']= pd.to_datetime(d1['Year']+'/'+d1['Month']+'/'+d1['Day'])\n# convert to float, to calculate mean\nd1['Subject_A'] = d1['Subject_A'].replace('',np.nan).astype(float)\n# index of the not null rows\nsubja = d1['Subject_A'].notnull()\n\n### max and min index row with notnull value\nmax_id_subja = d1.loc[subja,'date'].idxmax()\nmin_id_subja = d1.loc[subja,'date'].idxmin()\n### max and min date for Sub A with notnull value\nmax_date_subja = d1.loc[subja,'date'].max()\nmin_date_subja = d1.loc[subja,'date'].min()\n### value for max and min date\nmax_val_subja = d1.loc[max_id_subja,'Subject_A']\nmin_val_subja = d1.loc[min_id_subja,'Subject_A']\n#### Cutoffs\nmin_cutoff = min_date_subja-pd.Timedelta(6, unit='M')\nmax_cutoff = max_date_subja+pd.Timedelta(3, unit='M')\n\n## PART I.a\nd1.loc[(d1['date']&lt;min_date_subja) &amp; (d1['date']&gt;min_cutoff),'Subject_A'] = min_val_subja\n## PART I.b\nd1.loc[(d1['date']&gt;max_date_subja) &amp; (d1['date']&lt;max_cutoff),'Subject_A'] = max_val_subja\n## PART II\nd1_2i = d1.copy()\nd1_2ii = d1.copy()\n\nlower_date = min_date_subja\nlower_val = min_val_subja.copy()\nnext_dates_index = d1_2i.loc[(d1['date']&gt;min_date_subja) &amp; subja].index\nfor N in next_dates_index:\n    next_date = d1_2i.loc[N,'date']\n    next_val =  d1_2i.loc[N,'Subject_A']\n    #PART II.i\n    d1_2i.loc[(d1['date']&gt;lower_date) &amp; (d1['date']&lt;next_date),'Subject_A'] = np.mean([lower_val,next_val])\n    #PART II.ii\n    mean_time_a = pd.Timedelta((next_date-lower_date).days/2, unit='d')\n    d1_2ii.loc[(d1['date']&gt;lower_date) &amp; (d1['date']&lt;=lower_date+mean_time_a),'Subject_A'] = lower_val\n    d1_2ii.loc[(d1['date']&gt;lower_date+mean_time_a) &amp; (d1['date']&lt;=next_date),'Subject_A'] = next_val\n    lower_date = next_date\n    lower_val = next_val\nprint(d1_2i)\nprint(d1_2ii)\n"
"import re\nimport fileinput\n\n# For each line\nfor line in fileinput.input(files='example.csv', inplace=True, backup='.bak'):\n    # Replace it with the regex you provided\n    print(re.sub('(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)', '', line), end='')\n\nBefore:\n1,  5273249, 1061/72, 150-CF, S/O:XVZ, 1, ABX, 45, 0, Husband, 9213\n1,  5272849, 1063/36, 150-AS, S/O:XVZ, 1, ABX, 45, 0, Wife, 9253\n1,  5274549, 10626/12, 150-RT, S/O:XVZ, 1, ABX, 45, 0, Son, 9214\n\nAfter:\n1,  5273249, 1061/72, 150-CF S/O:XVZ, 1, ABX, 45, 0, Husband, 9213\n1,  5272849, 1063/36, 150-AS S/O:XVZ, 1, ABX, 45, 0, Wife, 9253\n1,  5274549, 10626/12, 150-RT S/O:XVZ, 1, ABX, 45, 0, Son, 9214\n"
"pd.DataFrame(df_A.set_index(['PM', 'ProjectID', 'Category']).sort_index().stack()).T.stack(2)\n\nOut[4]:\nPM                        Amy                    Bob        ...              Jill\nProjectID                   6                      1        ...                 5                      7\n                     Comments Score         Comments Score  ...          Comments Score         Comments Score\n  Category                                                  ...\n0 Category A              NaN   NaN  Justification 1    10  ...   Justification 5    15              NaN   NaN\n  Category B              NaN   NaN              NaN   NaN  ...               NaN   NaN  Justification 7     0\n  Category C              NaN   NaN              NaN   NaN  ...               NaN   NaN              NaN   NaN\n  Category D  Justification 6    10              NaN   NaN  ...               NaN   NaN              NaN   NaN\n\n[4 rows x 16 columns]\n\nIn [3]: df_A_transformed = pd.DataFrame(df_A.set_index(['PM', 'ProjectID', 'Category']).sort_index().stack()).T.stack(2).xs(0)\n\nIn [4]: df_A_transformed\nOut[4]:\nPM                      Amy                    Bob        ...              Jill\nProjectID                 6                      1        ...                 5                      7\n                   Comments Score         Comments Score  ...          Comments Score         Comments Score\nCategory                                                  ...\nCategory A              NaN   NaN  Justification 1    10  ...   Justification 5    15              NaN   NaN\nCategory B              NaN   NaN              NaN   NaN  ...               NaN   NaN  Justification 7     0\nCategory C              NaN   NaN              NaN   NaN  ...               NaN   NaN              NaN   NaN\nCategory D  Justification 6    10              NaN   NaN  ...               NaN   NaN              NaN   NaN\n\n[4 rows x 16 columns]\n\nIn [5]: df_A_transformed.loc['Category B']\nOut[5]:\nPM    ProjectID\nAmy   6          Comments                NaN\n                 Score                   NaN\nBob   1          Comments                NaN\n                 Score                   NaN\nJack  3          Comments                NaN\n                 Score                   NaN\n      4          Comments    Justification 4\n                 Score                     5\n      8          Comments    Justification 8\n                 Score                     2\nJill  2          Comments    Justification 2\n                 Score                     7\n      5          Comments                NaN\n                 Score                   NaN\n      7          Comments    Justification 7\n                 Score                     0\nName: Category B, dtype: object\n"
"df.ingredients.str.split(', ', expand=True).stack().reset_index(drop=True)\n\n0       eggs\n1       spam\n2    lobster\n3       eggs\n4    lobster\ndtype: object\n"
'a = np.array([\'BROOKLYN\', \'BRONX\', \'07 BRONX\', \'Unspecified\', \'05 BRONX\',\n       \'QUEENS\', \'MANHATTAN\', \'07 MANHATTAN\', \'STATEN ISLAND\',\n       \'17 BROOKLYN\', \'0 Unspecified\', \'Unspecified MANHATTAN\',\n       \'12 BROOKLYN\', \'07 BROOKLYN\', \'09 MANHATTAN\', \'01 STATEN ISLAND\',\n       \'12 MANHATTAN\', \'04 QUEENS\', \'06 BROOKLYN\',\n       \'01/04/2016 01:45:00 PM\', \'01/02/2016 05:43:34 AM\', \'07 QUEENS\',\n       \'11 BRONX\', \'01/04/2016 03:45:00 PM\', \'10 MANHATTAN\', \'03 BRONX\',\n       \'04 BRONX\', \' or 311 Online."\', \'01/13/2016 12:00:00 AM\',\n       \'04 BROOKLYN\', \'03 BROOKLYN\', \'01 QUEENS\',\n       \'01/04/2016 03:34:55 PM\', \'08 MANHATTAN\', \'14 BROOKLYN\',\n       \'10 QUEENS\', \'Unspecified STATEN ISLAND\', \'02 BRONX\', \'09 BRONX\',\n       \'08 QUEENS\', \'10 BRONX\', \'03 MANHATTAN\', \'12 QUEENS\',\n       \' please call (212) NEW-YORK (212-639-9675)."\',\n       \'Unspecified BROOKLYN\', \'01/11/2016 04:45:00 PM\', \'04 MANHATTAN\',\n       \'01 BRONX\', \'09 BROOKLYN\', \'01/05/2016 07:00:00 AM\', \'18 BROOKLYN\',\n       \'01/08/2016 09:00:00 AM\', \'01 BROOKLYN\', \'06 BRONX\',\n       \'01 MANHATTAN\', \'01/06/2016 12:15:00 PM\', \'02/04/2016 08:45:00 PM\',\n       \'01/05/2016 12:45:00 PM\', \' no action was taken."\', \'05 BROOKLYN\',\n       \'08 BROOKLYN\', \'Unspecified QUEENS\', \'01/08/2016 03:00:00 PM\',\n       \'08/22/2016 12:00:00 AM\', \'13 BROOKLYN\', \'02 QUEENS\', \'14 QUEENS\',\n       \'01/05/2016 08:45:00 AM\', \'11 QUEENS\', \'02 MANHATTAN\',\n       \'01/08/2016 10:05:00 AM\', \'01/05/2016 01:05:00 PM\',\n       \'Unspecified BRONX\', \'06 QUEENS\', \'09 QUEENS\', \'15 BROOKLYN\',\n       \'01/07/2016 09:25:00 AM\', \'02 STATEN ISLAND\',\n       \'01/02/2016 12:00:00 PM\', \'01/06/2016 08:45:00 PM\',\n       \'04/04/2016 12:00:00 AM\', \'01/06/2016 08:30:00 AM\'])\ndf=pd.DataFrame({ \'zone\':a })\n\nd = {\'MANHATTAN\':1, \'BROOKLYN\':2, \'QUEENS\' : 3, \'STATEN ISLAND\' : 4, \'BRONX\' : 5}\npat = \'(\' + \'|\'.join(d.keys()) + \')\'\ndf[\'code\'] = df[\'zone\'].str.extract(pat, expand=False).map(d).fillna(0, downcast=\'int\')\nprint (df.head(10))\n            zone  code\n0       BROOKLYN     2\n1          BRONX     5\n2       07 BRONX     5\n3    Unspecified     0\n4       05 BRONX     5\n5         QUEENS     3\n6      MANHATTAN     1\n7   07 MANHATTAN     1\n8  STATEN ISLAND     4\n9    17 BROOKLYN     2\n'
"df = pd.DataFrame({'Year_Q': ['2010 Q1', '2015 Q2']})\n\ndf['Dates']  = pd.PeriodIndex(df['Year_Q'].str.replace(' ', ''), freq='Q').to_timestamp()\nprint (df)\n    Year_Q      Dates\n0  2010 Q1 2010-01-01\n1  2015 Q2 2015-04-01\n\ndf['Dates']  = pd.PeriodIndex(df['Year_Q'].str.replace(' ', ''), freq='Q').to_timestamp(how='e')\nprint (df)\n    Year_Q      Dates\n0  2010 Q1 2010-03-31\n1  2015 Q2 2015-06-30\n"
"cols = ['A', 'B', 'C', 'D']\nmask_1 = df['UNIX_TS'] &gt; df['UNIX_TS'].cummax().shift().fillna(0)\nmask_2 = mask_2 = (df[cols] &gt;= df[cols].cummax().shift().fillna(0)).all(1)\n\ndf[mask_1 &amp; mask_2]\n\n    UNIX_TS     A   B   C   D\n0   1515288240  100 50  90  70\n1   1515288241  101 60  95  75\n2   1515288242  110 70  100 80\n"
"df = pd.read_csv('PL2.csv', encoding='cp1252', engine='python')\n\n\n#create helper df for total strings\ndf1 = df.loc[df.iloc[:, 0].str.startswith('Total', na=False), df.columns[0]].to_frame('total')\n#first column without Total - \ndf1['first'] = df1['total'].str.replace('Total - ', '')\nprint (df1.head(10))\n                                    total                          first\n17                   Total - 4000 - Sales                   4000 - Sales\n21  Total - 4200 - Discounts &amp; Allowances  4200 - Discounts &amp; Allowances\n24       Total - 4400 - Excise and Duties       4400 - Excise and Duties\n25                          Total - Sales                          Sales\n37      Total - 5000 - Cost of Goods Sold      5000 - Cost of Goods Sold\n\n#create index by first column\ndf = df.set_index(df.columns[0])\n\n#filter function - if not matched return empty df\ndef get_dict(df, first, last):\n    try:\n        df = df.loc[first: last]\n        df['Sub-Category'] = first\n    except KeyError:\n        df = pd.DataFrame()\n    return df\n\n#in dictionary comprehension create dict of DataFrames     \nd = {k: get_dict(df, k, v) for k, v in zip(df1['first'], df1['total'])}\n#print (d)\n\n#select Sales df\nprint (d['Sales'])\n"
'with open("file.csv", "r") as file:\n    for line in file:\n        sline = line.split(",")\n        if sline[n] == statecode:\n            # Where n is the position of the statecode column zero indexed\n            # and statecode is your target state code\n            pass # this is where you do something with the data\n'
"import pandas as pd\ndf = pd.DataFrame({'order': [1, 1, 2], 'content': ['hello', 'world', 'sof']})\ndf\nOut[4]: \n   order content\n0      1   hello\n1      1   world\n2      2     sof\ndf.groupby(by=['order']).agg(lambda x: ' '.join(x))\nOut[5]: \n           content\norder             \n1      hello world\n2              sof\n"
"df.loc[df['rating'].str.contains('-', na = False), 'rating'] = np.nan\n\ndf['rating'] = pd.to_numeric(df['rating'], errors = 'coerce')\n"
"# df2:\n#       neighbourhood Postcode\n#     0            WH      BS9\n#     1            SB      BS9\n#     2            HF      BS9\n#     3            WH      BS9\n#     4            WH      BS9\n#     5            SB      BS9\n#     6            HH      BS8\n#     7          SGTH      NaN\n\nconditions = [\n    ((df2['neighbourhood'] == 'HH') &amp; (df2['Postcode'].isna())),\n]\n\nchoices = [\n    'BS8'\n]\n\ndf2['Postcode'] = np.select(conditions, choices, df2['Postcode'])\n\n  neighbourhood Postcode\n0            WH      BS9\n1            SB      BS9\n2            HF      BS9\n3            WH      BS9\n4            WH      BS9\n5            SB      BS9\n6            HH      BS8\n7          SGTH      NaN\n"
"arr = df2.values\nm1 = (arr &gt;= 0) &amp; (arr &lt; 1.5)\nm2 = (arr &gt;= 1.2) &amp; (arr &lt;= 2)\n\na1 = df1['ColX'].values[:, None]\na2 = df1['ColY'].values[:, None]\n\ndf = pd.DataFrame(np.select([m1, m2], [a1, a2]), index=df2.index, columns=df2.columns)\nprint (df)\n    ColA ColB ColC ColD\n100    C    C    C    C\n101    Z    T    T    Z\n102    A    Y    Y    A\n"
"    positive_mask = df&gt;0\n\n    sequence_groups = positive_mask.astype(int).diff(1).fillna(0).abs().cumsum().squeeze()\n\n    sequence_size = positive_mask.groupby(sequence_groups).transform(len)\n\n    df_extended = pd.concat([df, positive_mask, sequence_groups, sequence_size], axis=1)\n    df_extended.columns = ['value', 'is_positive', 'sequence_group', 'sequence_size']\n    df_extended\n\n        value  is_positive  sequence_group  sequence_size\n    0      -2        False             0.0              2\n    1       0        False             0.0              2\n    2       2         True             1.0              2\n    3       2         True             1.0              2\n    4       0        False             2.0              8\n    5       0        False             2.0              8\n    6       0        False             2.0              8\n    7       0        False             2.0              8\n    8       0        False             2.0              8\n    9       0        False             2.0              8\n    10      0        False             2.0              8\n    11      0        False             2.0              8\n    12      2         True             3.0              7\n    13      2         True             3.0              7\n    14      2         True             3.0              7\n    15      2         True             3.0              7\n    16      2         True             3.0              7\n    17      3         True             3.0              7\n    18      2         True             3.0              7\n    19      0        False             4.0              1\n    20      2         True             5.0              3\n    21      2         True             5.0              3\n    22      2         True             5.0              3\n    23      0        False             6.0              1\n    24      3         True             7.0              2\n    25      3         True             7.0              2\n    26      0        False             8.0              1\n\n    flat_mask = (df_extended.sequence_size &lt; 3) &amp; (df_extended.is_positive)\n    df_extended.loc[flat_mask, 'value'] = 0\n\n    df_extended.value.plot()\n"
"c = df.columns.str.contains('Commute')\ndf.loc[:, c] = df.loc[:, c].notna().astype(int)\nprint (df)\n    Passenger  Age  Gender  Commute_to_work  Commute_mode  Commute_time\n0  Passenger1   32    Male                1             1             1\n1  Passenger2   26  Female                1             1             0\n2  Passenger3   33  Female                0             0             1\n3  Passenger4   29  Female                1             1             0\n"
"from collections import OrderedDict\ndf[0].str.split().apply(lambda x: ','.join(OrderedDict.fromkeys(x).keys()))\n\n0    Yes,Absolutely\n1           No,Nope\n2          Win,Lose\n\ndf[0].str.split().apply(lambda x: ','.join(list(set(x))))\n"
'for r in words: \n    if not r in stop_words: \n        appendFile = open(\'stopwords_soccer.csv\',\'a\', encoding=\'utf-8\') \n        appendFile.write(r)\n        appendFile.write("\\n")\n        appendFile.close()\n'
"df.loc[df['Temp3'].isnull(), 'Temp3'] = df.loc[df['Temp3'].isnull(), ['Temp1', 'Temp2']].mean(axis=1)\n&gt;&gt;&gt; df\n   Temp1  Temp2  Temp3\n0     31   23.0    NaN\n1     22    NaN    NaN\n2     25   25.0   21.0\n&gt;&gt;&gt; df.loc[df['Temp3'].isnull(), 'Temp3'] = df.loc[df['Temp3'].isnull(), ['Temp1', 'Temp2']].mean(axis=1)\n&gt;&gt;&gt; df\n   Temp1  Temp2  Temp3\n0     31   23.0   27.0\n1     22    NaN   22.0\n2     25   25.0   21.0\n"
'import csv\nfrom pprint import pprint\n\nf=open("f0.csv","r")\n\nreader=csv.reader(f)\n\nd = {}\n\nfor row in reader:\n    row[0] = row[0].strip() # remove leading, trailing whitespace\n    row[1] = row[1].strip()\n\n    d.setdefault(row[1], []).append(row[0])\n\nf.close()\n\npprint(d)\n\n{\'CUSTID\': [\'ADS\', \'ADS_PARTY\', \'CUST_TABLE\'],\n \'FULLNAME\': [\'CUST_TABLE\', \'DEALER_TABLE\', \'SUPPLIER_TABLE\'],\n \'ROLEID\': [\'ADS\', \'SUPPLIER_TABLE\']}\n\nif row[1] not in d:\n    d[row[1]] = []\n\nd[row[1]].append(row[0])\n'
"df = df.fillna('')\ndf[['Phone', 'Email']] = [sorted(t, key=lambda x:re.findall(r'(^.*@.*$)', x)) \n                          for t in df[['Phone', 'Email']].values]\n\n    ID      Phone         Email\n0  A01     111111  abc@mail.com\n1  A02             bcd@mail.com\n2  A03  222222222  def@mail.com\n\n[('111111', 'abc@mail.com'), \n ('bcd@mail.com', ''),  \n ('def@mail.com', '222222222')]\n\nf = lambda x:re.findall(r'(^.*@.*$)', x)\n[[f(i) for i in t] for t in df[['Phone', 'Email']].values]\n# Output\n[[[], ['abc@mail.com']], \n [['bcd@mail.com'], []], \n [['def@mail.com'], []]]\n\n[sorted(t, key=f) for t in df[['Phone', 'Email']].values]\n\n[['111111', 'abc@mail.com'],\n ['', 'bcd@mail.com'],\n ['222222222', 'def@mail.com']]\n"
"import pandas as pd\ndf = pd.DataFrame({'A':[1,2,3,4],'B':[2,1,3,2],'C':[3,4,2,1],'D':[4,2,1,3]});\ndata = {'A':[1,2,3,4],'B':[2,1,3,2],'C':[3,4,2,1],'D':[4,2,1,3]}\n\nnew_data = dict()\nfor key, value in data.items():\n    for data1 in value:\n        if data1 in new_data.keys():\n            if key in new_data[data1]:\n                pass\n            else:\n                new_data[data1].append(key)\n        else:\n            new_data[data1]=[key]\n\nfor key ,value in new_data.items():\n    dif =  4 - len(value)\n    new_data[key] = value + [None]*dif\n\nnew_data\n\n{1: ['A', 'B', 'C', 'D'],\n 2: ['A', 'B', 'C', 'D'],\n 3: ['A', 'B', 'C', 'D'],\n 4: ['A', 'C', 'D', None]}\n\n    1   2   3   4\n0   A   A   A   A\n1   B   B   B   C\n2   C   C   C   D\n3   D   D   D   None\n"
"from collections import defaultdict\nimport pandas as pd\n\nwith open('reviews.txt', 'r') as f:\n    lines = f.readlines()\n    data = defaultdict(list)\n    for line in lines:\n        col,value = line.split(':')\n        data[col.strip()].append(value.strip())\n\n    df = pd.DataFrame(data)\n    print(df)\n\n   product/productId  product/title product/price review/userId review/text\n0          blah blah     blue shirt       unknown     blah blah   blah blah\n1          blah blah  pair of jeans       unknown     blah blah   blah blah\n"
'my_temp_string = ""\nwith open (\'a.txt\',"r+", encoding="utf-8") as fin:\n    for line in fin:\n        lemm = lemmatize_sentence(line)\n        my_temp_string += f\'{lemm} \\n\'\nprint (my_temp_string)\n'
"df['new column'] = [el[0][:5] if el[1] == 'USA' else el[0].replace(' ', '') for el in zip(df['Zip Code'], df['Country'])]\n\ndf['new column'] = df['Zip Code'].str.replace(' ', '')\nusa = df['Country'].str.contains('USA')\ndf['new column'][usa] = df['new column'][usa].str.slice(0,5)\n\ndf['new column'] = df['Zip Code'].str.replace(' ', '').str.replace('-', '')\nusa = df['Country'].str.contains('USA')\ndf['new column'][usa] = df['new column'][usa].str.slice(0,5)\n"
'import numpy\nimport pandas\n\ndef add_radians(df):\n    return df.assign(**{colname.rstrip("_deg"): numpy.radians(col) for colname, col in df.iteritems()})\n\nn_ref = 26\nref_traj = pandas.DataFrame({"lat_deg": -76 + numpy.linspace(-1, 1, n_ref),\n                             "lon_deg":   3 + numpy.linspace(-1, 1, n_ref)**2,\n                            }).pipe(add_radians)\n\nn = 500\ntraj = pandas.DataFrame({"lat_deg": -76 + numpy.cumsum(numpy.random.choice([-1, 1], size=n) * 0.05),\n                         "lon_deg":   3 + numpy.cumsum(numpy.random.choice([-1, 1], size=n) * 0.05),\n                        }).pipe(add_radians)\n\nax = traj.plot.scatter(x="lat_deg", y="lon_deg")\nax = ref_traj.plot.scatter(x="lat_deg", y="lon_deg", color="red", ax=ax)\n\ndef distance(lat1, lon1, lat2, lon2):\n    # TODO: check that shape of lat1, lon1, lat2, lon2 are all compatible.\n    R = 6371  # Radius of Earth in kilometers\n\n    # TODO: check this distance calculation\n\n    def hav(theta):\n        return numpy.sin(theta)**2\n\n    d_lat = lat2 - lat1\n    d_lon = lon2 - lon1\n    a = hav(d_lat / 2) + numpy.cos(lat1) * numpy.cos(lat2) * hav(d_lon / 2)\n    return 2 * R * numpy.sqrt(a)\n\ndef min_distance(ref_lat, ref_lon, lat, lon):\n    shape = (numpy.shape(lat)[0], numpy.shape(ref_lat)[0])\n\n    def broadcasted(a):\n        return numpy.broadcast_to(a, shape=shape)\n\n    d = distance(lat1=broadcasted(ref_lat), \n                 lon1=broadcasted(ref_lon), \n                 lat2=broadcasted(lat[:, numpy.newaxis]),\n                 lon2=broadcasted(lon[:, numpy.newaxis]))\n    return numpy.amin(d, axis=-1)\n\nd = min_distance(ref_traj[\'lat\'], ref_traj[\'lon\'], traj[\'lat\'], traj[\'lon\'])\ntolerance = 10  # in kilometers\nnear_ref = d &lt; tolerance\n\nax = ref_traj.plot.scatter(x="lat_deg", y="lon_deg", color="red")\ntraj[near_ref].plot.scatter(x="lat_deg", y="lon_deg", color="blue", ax=ax)\ntraj[~near_ref].plot.scatter(x="lat_deg", y="lon_deg", color="gray", ax=ax)\n'
"df['combined'] = df.apply(lambda x: list([x['Temp'],\n                                        x['Temperature'],\n                                        x['Degrees']]),axis=1) \n\ndf.apply(lambda x: x.Temp + ' / ' + x.Temperature + ' / ' + x.Degrees, axis=1)\n\n# or simply\n\ndf['combined'] = df.Temp + ' / ' + df.Temperature + ' / ' + df.Degrees\n\nimport numpy as np\ndef combine_with_nan(x):\n   try:\n      np.isnan(x.Temp)\n      Temp = 'NaN'\n   except:\n      Temp = x.Temp\n   try:\n      np.isnan(x.Temperature)\n      Temperature = 'NaN'\n   except:\n      Temperature = x.Temperature\n   try:\n      np.isnan(x.Degrees)\n      Degrees = 'NaN'\n   except:\n      Degrees = x.Degrees\n   return Temp + ' / ' + Temperature + ' / ' + Degrees\n\ndf.apply(combine_with_nan, axis=1)\n"
"import pandas as pd\n# define Data Frames\nleft = pd.DataFrame({\n    'key1': ['A', 'B', 'C'],\n    'valueZ': ['bob', 'jes', 'joe'],\n    'valueX': [1, 8, 3],\n    'valueY': [4, 5, 6]\n})\nright = pd.DataFrame({\n    'key1': ['A', 'B', 'C'],\n    'valueZ': ['sam', 'beth', 'joe'],\n    'valueX': [7, 8, 9],\n    'valueY': [4, 11, 12],\n    'valueK': ['hill town', 'market', 'mall']\n})\n\n# determine important columns\nkeyCol = 'key1'\ncommonCols = list(set(left.columns &amp; right.columns))\nfinalCols = list(set(left.columns | right.columns))\nprint('Common = ' + str(commonCols) + ', Final = ' + str(finalCols))\n\n# join dataframes with suffixes\nmergeDf = left.merge(right, how='left', on=keyCol, suffixes=('_left', '_right'))\n\n# combine the common columns\nfor col in commonCols:\n    if col != keyCol:\n        for i, row in mergeDf.iterrows():\n            leftVal = str(row[col + '_left'])\n            rightVal = str(row[col + '_right'])\n            print(leftVal + ',' + rightVal)\n            if leftVal == rightVal:\n                mergeDf.loc[i, col] = leftVal\n            else:\n                mergeDf.loc[i, col] = leftVal + '/' + rightVal\n\n# only use the finalCols\nmergeDf = mergeDf[finalCols]\n\n     valueZ key1     valueK valueX valueY\n0   bob/sam    A  hill town    1/7      4\n1  jes/beth    B     market      8   5/11\n2       joe    C       mall    3/9   6/12\n"
'import re\n\ndef parse_int(s):\n    """\n    A fast memoized function which builds a lookup dictionary then maps values to the series\n    """\n    map_dict = {x:float(re.findall(\'[0-9.]+\',x)[0]) for x in s.unique() if re.search(\'[0-9.]+\',x)}\n    return s.map(map_dict)\n\ndata[\'Result\'] = parse_int(data[\'Result\'])\n'
"df[['Amount', 'Currency']] = df['column'].str.extract(r'(\\d+)(\\D+)')\n\nexchange_rate = {'Euro': 1.2, 'pounds': 1.3, 'rupee': 0.05}\ndf['Amount_dollar'] = pd.to_numeric(df['Amount']) * df['Currency'].map(exchange_rate).fillna(1) \n\n      column  Amount Currency  Amount_dollar\n0  100Dollar     100   Dollar         100.00\n1  200Dollar     200   Dollar         200.00\n2    100Euro     100     Euro         120.00\n3    300Euro     300     Euro         360.00\n4  184pounds     184   pounds         239.20\n5  150pounds     150   pounds         195.00\n6    10rupee      10    rupee           0.50\n7    30rupee      30    rupee           1.50\n"
"mask = df.mycolumn.str.contains('/')\ndf.mycolumn.loc[mask] = df.mycolumn[mask].str.split('/', 1).str[1]\n"
'from pyspark.sql.functions import *\n\ndf:\n\n+----+----+\n|   A|   B|\n+----+----+\n| 0.4| 0.3|\n|null|null|\n| 9.7|null|\n|null|null|\n+----+----+\n\n# Generic solution for all columns\namount_missing_df = df.select([(count(when(isnan(c) | col(c).isNull(), c))/count(lit(1))).alias(c) for c in df.columns])\namount_missing_df.show()\n\namount_missing_df:\n\n+---+----+\n|  A|   B|\n+---+----+\n|0.5|0.75|\n+---+----+\n'
'import re\nr = re.compile(r"^\\s+")\n\ndirty_list = [...]\n# iterate over dirty_list substituting\n# any whitespace with an empty string\nclean_list = [\n  r.sub("", s)\n  for s in dirty_list\n]\n'
"Date    Price    Volume\n2018-12-28  172.0   800\n2018-12-27  173.6   400\n2018-12-26  170.4   500\n2018-12-25  171.0   2200\n2018-12-21  172.8   800\n\ndates = pd.date_range('2018-12-15', '2018-12-31')\n\ndf['Date'] = pd.to_datetime(df['Date'])\n\ndf = df.set_index('Date')\n\ndf = df.reindex(dates, fill_value=0.0)\n\ndf = df.reset_index()\n\n        index  Price  Volume\n0  2018-12-15    0.0     0.0\n1  2018-12-16    0.0     0.0\n2  2018-12-17    0.0     0.0\n3  2018-12-18    0.0     0.0\n4  2018-12-19    0.0     0.0\n5  2018-12-20    0.0     0.0\n6  2018-12-21  172.8   800.0\n7  2018-12-22    0.0     0.0\n8  2018-12-23    0.0     0.0\n9  2018-12-24    0.0     0.0\n10 2018-12-25  171.0  2200.0\n11 2018-12-26  170.4   500.0\n12 2018-12-27  173.6   400.0\n13 2018-12-28  172.0   800.0\n14 2018-12-29    0.0     0.0\n15 2018-12-30    0.0     0.0\n16 2018-12-31    0.0     0.0\n\ndf['weekday'] = df['index'].dt.dayofweek\n\nmissing_weekdays = df[(~df['weekday'].isin([5,6])) &amp; (df['Volume'] == 0.0)]\n\n&gt;&gt;&gt; missing_weekdays\n        index  Price  Volume  weekday\n2  2018-12-17    0.0     0.0        0\n3  2018-12-18    0.0     0.0        1\n4  2018-12-19    0.0     0.0        2\n5  2018-12-20    0.0     0.0        3\n9  2018-12-24    0.0     0.0        0\n16 2018-12-31    0.0     0.0        0\n"
"def str_ops(x):\n    for y in real_model_names: \n        if y in x: \n            return y \n    return x\n\ncars_em_df['commercial_name_cleaned'] = cars_em_df['commercial_name'].apply(str_ops)\n\n# Result\ncars_em_df\n  manufacturer_name_mapped                   commercial_name fuel_type_mapped  file_year  emissions    commercial_name_cleaned\n0                     FIAT              124 gt multiair auto           Petrol       2018        153       124 gt multiair auto\n1                     FIAT         500l wagon pop star t-jet           Petrol       2018        158  500l wagon pop star t-jet\n2                     FIAT                doblo combi 1.4 95           Petrol       2018        165         doblo combi 1.4 95\n3                     FIAT  panda  0.9t sge 85 natural power    NG-Biomethane       2018         86                      panda\n4                     FIAT                 punto 1.4  77 lpg              LPG       2018        114                      punto\n"
"def str_ops(commercial_name_cleaned,commercial_name):\n    if commercial_name_cleaned == None:\n        if '216' in commercial_name:\n            return '2-series'\n        elif '220' in commercial_name:\n            return '2-series'\n        elif '320' in commercial_name:\n            return '3-series'\n\ndef str_ops(commercial_name_cleaned,commercial_name):\n    if commercial_name_cleaned == 'None':\n        if '216' in commercial_name:\n            return '2-series'\n        elif '220' in commercial_name:\n            return '2-series'\n        elif '320' in commercial_name:\n            return '3-series'\n    else:\n        return commercial_name_cleaned\n\nmanufacturer_name_mapped                   commercial_name  ... emissions  commercial_name_cleaned\n0                     FIAT              124 gt multiair auto  ...       153                      124\n1                     FIAT         500l wagon pop star t-jet  ...       158                      500\n2                     FIAT                doblo combi 1.4 95  ...       165                     None\n3                     FIAT  panda  0.9t sge 85 natural power  ...        86                    panda\n4                     FIAT                 punto 1.4  77 lpg  ...       114                    punto\n5                   BMW AG              x4 xdrive20d se auto  ...       131                       x4\n6                   BMW AG        216d active tourer b37 f45  ...       166                 2-series\n7                   BMW AG          220d gran tourer b47 f46  ...       200                 2-series\n8                   BMW AG                x1 xdrive18d sport  ...       151                       x1\n9                   BMW AG       320i xdrive m sport gt auto  ...       149                 3-series\n"
'with open("your_file.txt", "r") as f:\n    new_lines = []\n    for idx, line in enumerate(f):\n        if idx in [x for x in range(2,5)]: #[2,3,4]\n            new_lines.append(line)\n\nwith open("your_new_file.txt", "w") as f:\n    for line in new_lines:\n        f.write(line)\n'
'import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n#%matplotlib inline\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, auc, roc_curve\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nimport graphviz\n\ndf = pd.read_csv(\'C:\\\\Users\\\\ryans\\\\OneDrive\\\\Desktop\\\\mushrooms.csv\')\n\ndf.columns\n\ndf.head(5)\n\n# The data is categorial so I convert it with LabelEncoder to transfer to ordinal.\n\nlabelencoder=LabelEncoder()\nfor column in df.columns:\n    df[column] = labelencoder.fit_transform(df[column])\n\n#df.describe()\n\n\n#df=df.drop(["veil-type"],axis=1)\n\n#df_div = pd.melt(df, "class", var_name="Characteristics")\n#fig, ax = plt.subplots(figsize=(10,5))\n#p = sns.violinplot(ax = ax, x="Characteristics", y="value", hue="class", split = True, data=df_div, inner = \'quartile\', palette = \'Set1\')\n#df_no_class = df.drop(["class"],axis = 1)\n#p.set_xticklabels(rotation = 90, labels = list(df_no_class.columns));\n\n#plt.figure()\n#pd.Series(df[\'class\']).value_counts().sort_index().plot(kind = \'bar\')\n#plt.ylabel("Count")\n#plt.xlabel("class")\n#plt.title(\'Number of poisonous/edible mushrooms (0=edible, 1=poisonous)\');\n\n\nplt.figure(figsize=(14,12))\nsns.heatmap(df.corr(),linewidths=.1,cmap="YlGnBu", annot=True)\nplt.yticks(rotation=0);\n\ndfDummies = pd.get_dummies(df)\n\nplt.figure(figsize=(14,12))\nsns.heatmap(dfDummies.corr(),linewidths=.1,cmap="YlGnBu", annot=True)\nplt.yticks(rotation=0);\n'
'df.apply(lambda x: x.where(x.between(*(my_dict[x.name])) ) )\n\n   TriGly   Age  Chol\n0     1.0  10.0   NaN\n1     0.0  12.0   0.4\n2     NaN   NaN   0.9\n'
"df.filter((f.col('_c0')).isin([x for x in range(1,df.count()+1)]))\n"
"import numpy as np\n\nm = df.isna().groupby(df.columns.str.split('_').str[0], axis=1).transform('all')\ndf_final = df.fillna(999).mask(m, np.nan)\n\n\nOut[74]:\n   ID   Q1_1  Q1_2   Q1_3  Q2_1   Q2_2\n0   1    NaN   NaN    NaN     2  999.0\n1   2  999.0   3.0  999.0     3    1.0\n2   3    5.0   4.0    4.0     5    5.0\n3   4    2.0   4.0    3.0     5    4.0\n"
"df3['Day of the Week'] = pd.to_datetime(df3['Date']).dt.day_name()\n"
'parts = df[[\'ADDRESS1\', \'ADDRESS2\', \'ADDRESS3\', \'POSTCODE\']].replace(np.nan, \'\')\n\nfull = parts.apply(\' \'.join, axis=1)\n\nclean = full.str.replace("[,\'.]", "").upper().strip()\n\ndf[\'FULL ADDRESS\'] = clean\n'
"# extract the suffixes `_x, _y`\nsuffixes = df.columns.str.extract('(_.*)$')[0]\n\n# output\npd.concat([pd.get_dummies(df.iloc[:,i+1])\n             .add_suffix(suffixes[i+1])\n             .mul(df.iloc[:,i],axis=0) \n           for i in range(0,df.shape[1], 2)],\n          axis=1\n         )\n\n   ball_x  bat_x  gloves_x  ball_y  gloves_y  mitt_y\n0       0     10         0       0         0      45\n1      12      0         0      30         0       0\n2       0      0        13       0        25       0\n3       0     14         0       0         0      20\n"
"df['historical_rank_new']=df['historical_rank'].str.extract('(^[\\d]{1,4})')\ndf\n"
"columns = 'Name'\ndf = df.groupby(columns).apply(lambda x: x.ffill().bfill()).drop_duplicates(columns)\nprint (df)\n    Name   Bool  Int Char\n0   Anne   True  1.0    A\n1   Bert   True  2.0    B\n2  Conan  False  0.0    C\n"
"import pandas as pd\n\nfilename = 'filename.csv'\nlines =open(filename).read().split('\\n')   # reading the csv file\n\nlist_ = [e for e in lines if e!='' ]  # removing '' characters from lines list\n\nlist_ = [e for e in list_ if e[0].isdigit()]  # removing string starting with non-numeric characters \n\nTime = [float(i.split(';')[0]) for i in list_]   # use int or float depending upon the requirements\n\nData = [float(i.split(';')[1].strip()) for i in list_]\n\n\ndf = pd.DataFrame({'Time':Time, 'Data':Data})    #making the dataframe \ndf\n"
"import pandas as pd \nimport re\n\ndf =  pd.read_csv(\n    'test.txt', \n    sep = '[\\s]{2,}', \n    engine = 'python', \n    header = None, \n    index_col = False, \n    names= [\n        &quot;FirstN&quot;,&quot;LastN&quot;,&quot;FULLSID&quot;,&quot;TeacherData&quot;,&quot;TeacherLastN&quot;\n    ]\n)\nsid_pattern = re.compile(r'(\\d{9})(\\d+-\\d+-\\d+)(.*)', re.IGNORECASE)\ndf['SID'] = df.apply(lambda row: sid_pattern.search(row.FULLSID).group(1), axis = 1)\ndf['Birth'] = df.apply(lambda row: sid_pattern.search(row.FULLSID).group(2), axis = 1)\ndf['City'] = df.apply(lambda row: sid_pattern.search(row.FULLSID).group(3), axis = 1)\n\nteacherdata_pattern = re.compile(r'(.{2})([\\dA-Z]+\\d)(.*)', re.IGNORECASE)\ndf['States'] = df.apply(lambda row: teacherdata_pattern.search(row.TeacherData).group(1), axis = 1)\ndf['Postal'] = df.apply(lambda row: teacherdata_pattern.search(row.TeacherData).group(2)[-4:], axis = 1)\ndf['TeacherFirstN'] = df.apply(lambda row: teacherdata_pattern.search(row.TeacherData).group(3), axis = 1)\n\ndel df['FULLSID']\ndel df['TeacherData']\n\nprint(df)\n\n  FirstN  LastN TeacherLastN        SID       Birth        City States Postal TeacherFirstN\n0    Ann   Gosh         Ryan  123456789  2008-12-15      Irvine     CA   A9Z5         Steve\n1   Yosh   Dave         Tuck  987654321  2009-04-18     St. Elf     NY   P8G0          Brad\n2  Clair  Simon         John  324567457  2008-12-29  New Jersey     NJ   R9B3           Dan\n"
"def fx(s):\n    if s.isna().all():\n        return s\n    elif pd.isna(s.iloc[0]):\n        s.iloc[0] = 0\n        s = s.ffill().bfill()\n    return s\n\ndf[['B', 'C']] = df.groupby('A')[['B', 'C']].transform(fx)\n\n# print(df)\n    A    B     C\n0   A  0.0   0.0\n1   A  0.0   0.0\n2   A  1.0  10.0\n3   A  1.0  10.0\n4   B  0.0   0.0\n5   B  2.0  20.0\n6   B  2.0  20.0\n7   B  2.0  20.0\n8   C  NaN   0.0\n9   C  NaN   0.0\n10  C  NaN   0.0\n11  C  NaN  30.0\n"
"assessments['PreviousAssessmentDate'] = assessments.groupby(['ClientID', 'Program']).AssessmentDate.shift(1, fill_value='0000-00-00')\ndf = assessments.merge(services, on='ClientID', how='left')\ndf[df.columns[5:]] = df[df.columns[5:]].multiply((df.AssessmentDate &gt; df.ServiceDate) &amp; (df.PreviousAssessmentDate &lt; df.ServiceDate), axis=0)\ndf = df.groupby(['ClientID', 'AssessmentDate', 'Program']).sum().reset_index()\n\n  ClientID AssessmentDate             Program  Assistance Navigation  Basic Needs\n0      212     2018-01-04           Case Mgmt                      0            1\n1      212     2018-07-03           Case Mgmt                      2            0\n2      212     2019-06-10           Case Mgmt                      0            0\n3      292     2017-08-08  Coordinated Access                      0            0\n4      292     2017-12-21  Coordinated Access                      1            3\n"
"df1['label_item'] = df1['comment'].str.findall(f'({&quot;|&quot;.join(df2.label)})').str.join(',')\n"
"import re\nimport csv\nimport json\nimport datetime as dt\nfrom bs4 import BeautifulSoup\n\nnow = dt.date.today()\ndata = &quot;&quot;&quot;&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;{&quot;Flashpoint Swindon&quot;:{&quot;count&quot;:0,&quot;capacity&quot;:88,&quot;lastUpdated&quot;:&quot;Last updated: 1 hour ago (7:07 PM)&quot;},&quot;Oakwood&quot;:{&quot;count&quot;:0,&quot;capacity&quot;:75,&quot;lastUpdated&quot;:&quot;Last updated: 2 hours ago (6:11 PM)&quot;},&quot;Big Depot Leeds&quot;:{&quot;count&quot;:11,&quot;capacity&quot;:105,&quot;lastUpdated&quot;:&quot;Last updated: 1 min ago (8:20 PM)&quot;},&quot;Depot Birmingham&quot;:{&quot;count&quot;:8,&quot;capacity&quot;:180,&quot;lastUpdated&quot;:&quot;Last updated: 1 min ago (8:20 PM)&quot;}}&lt;/body&gt;&lt;/html&gt;&quot;&quot;&quot;\nsoup = BeautifulSoup(data, 'html.parser')\n\ncontent = json.loads(soup.body.text)\n\nwith open('test.csv', 'w') as csvfile:\n    writer = csv.writer(csvfile, delimiter=&quot;;&quot;)\n    writer.writerow([&quot;Date&quot;, &quot;Centre&quot;, &quot;Count&quot;, &quot;Capacity&quot;, &quot;Time&quot;])  # write headers\n    for item in content:\n        # parse the time and create a datetime object\n        _time = re.search(r&quot;\\((.*)\\)&quot;, content[item]['lastUpdated']).groups()[0]\n        _timeObj = dt.datetime.strptime(_time, &quot;%I:%M %p&quot;)\n        writer.writerow([now, item, content[item]['count'], content[item]['capacity'], _timeObj.strftime('%H:%M')])\n\nprint(open('test.csv').read())\n\nDate;Centre;Count;Capacity;Time\n2020-08-23;Flashpoint Swindon;0;88;19:07\n2020-08-23;Oakwood;0;75;18:11\n2020-08-23;Big Depot Leeds;11;105;20:20\n2020-08-23;Depot Birmingham;8;180;20:20\n"
"import itertools\nimport re\n\nseparator_re = re.compile(r&quot;\\s*\\\\N\\s*$&quot;, re.MULTILINE)\n\nwith open('other.csv') as infp:\n    with open('other-conv.csv', 'w') as outfp:\n        for hassep, subiter in itertools.groupby(infp, separator_re.search):\n            if hassep:\n                outfp.writelines(&quot;{}\\n&quot;.format(separator_re.sub(&quot;&quot;,line))\n                    for line in subiter)\n            else:\n                for line in subiter:\n                    if line.endswith(&quot;\\\\\\n&quot;):\n                        line = line[:-2] + &quot; &quot;\n                    else:\n                        line = line.strip()\n                    outfp.write(line)\n"
"mask = df.sort_values('Date')\\\n  .groupby(['ITEM_ID', 'VALUE'])['TYPE']\\\n  .apply(lambda x: ((x == 'O') &amp; (x.shift(-1) == 'I')) | (x == 'I') &amp; (x.shift(1) == 'O'))\ndf.loc[~mask]\n"
"df.loc[(df['answer']!='first') &amp; (df['question']=='1.1'),'question'] = '1.10'\n"
'a = np.diag(df)[None, :]\nb = np.diag(df)[:, None]\n\nc = a+b\nnp.fill_diagonal(c, np.diag(df))\n\ndf_out = df.div(c)\ndf_out\n\n          10        25        26\n10  1.000000  0.001692  0.053488\n25  0.001692  1.000000  0.156010\n26  0.053488  0.156010  1.000000\n'
"s = 'street.Random, street.RandomTwo, street.Randomer: 2, 4'\n\ncount = 0\nfor ch in s:\n    if ch == ':':\n        break\n    elif ch == ',':\n        count += 1\n\nprint( [*map(str.strip, s.split(',', maxsplit=count))] )\n\n['street.Random', 'street.RandomTwo', 'street.Randomer: 2, 4']\n"
"d = {'No':0, 'yes':1, 'Yes':1, 'no':0}\ndf1 = df.rename(columns = dict(zip(df.columns, df.columns.str.split('+').str[0])))\ndf2 = df.rename(columns = dict(zip(df.columns, df.columns.str.split('+').str[1])))\n\ndf = pd.concat([df1, df2],axis=1).replace(d).max(axis=1, level=0).replace({0:'no', 1:'yes'})\nprint (df)\n     a    b    c    d\n0   no  yes  yes   no\n1  yes  yes   no   no\n2   no   no  yes  yes\n"
"^[^-]+-\\s+\n\nimport re\n\nstrings = ['Reuters - Life is beautiful.',\n           'agency.com - China\\'s currency remains pegged to the dollar and the US currency\\'s sharp falls in recent months have therefore made - Chinese export prices highly competitive.',\n           'AP - The number of days that beaches closed or posted warnings because of pollution rose sharply in 2003 due to more rainfall, increased monitoring and tougher -standards, an environmental group said on Thursday.',\n           'CNN - Warming water temperatures - in the central equatorial Pacific last month may indicate the start of a new El Nino.']\n\nrx = re.compile(r'^[^-]+-\\s+')\n\nstrings = list(map(lambda string: rx.sub(&quot;&quot;, string), strings))\nprint(strings)\n\n['Life is beautiful.', &quot;China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made - Chinese export prices highly competitive.&quot;, 'The number of days that beaches closed or posted warnings because of pollution rose sharply in 2003 due to more rainfall, increased monitoring and tougher -standards, an environmental group said on Thursday.', 'Warming water temperatures - in the central equatorial Pacific last month may indicate the start of a new El Nino.']\n"
'df[&quot;Date of Publication&quot;].astype(str).str.slice(0, 4)\n'
'z = np.abs(stats.zscore(X))\nmask = (z &lt; 3).all(axis=1)\nX = X[mask]\nY = Y[mask]\n'
"c1 = old.columns[1::2]\nc2 = old.columns[2::2]\n\ndf = pd.lreshape(old, {'a':c1, 'b':c2}).pivot('ID','a','b')\n#alternative if duplicates in `ID`, `a` pairs\n#df = pd.lreshape(old, {'a':c1, 'b':c2}).pivot_table(index='ID',columns='a',values='b', aggfunc='mean')\nprint (df)\na   colname1  colname2  colname3\nID                              \nA        3.0       NaN       0.0\nB        NaN       4.0       2.0\nC        0.0       NaN       1.0\n"
"s.str.replace('\\s+', '').str.get_dummies(',').sum()\n\na    36\nb    49\nc    35\nd    15\ne     9\nf    13\ng    18\nh     5\ni     1\nj     1\nk     4\nl     5\nm     2\nn     5\no    13\ndtype: int64\n"
"df = pd.DataFrame(df['salary.percentages'].tolist(), columns=df['salary.labels'].iloc[0])\nprint (df)\n   Not Impacted  Salary Not Paid  Salary Cut  Variables Impacted  \\\n0          29.0              0.9         2.2                11.3   \n1          74.5              1.1         1.4                12.0   \n2          63.4              1.9         2.2                11.2   \n3          58.3              0.6         1.9                 7.9   \n4          80.4              1.4         2.2                 4.7   \n5          71.2              0.9         1.2                 6.3   \n6          39.9              1.6         5.8                15.8   \n7          56.5              0.8         2.3                 9.8   \n8          42.9              2.3         5.1                14.1   \n\n   Appraisal Delayed  \n0               56.6  \n1               11.0  \n2               21.3  \n3               31.3  \n4               11.3  \n5               20.4  \n6               36.9  \n7               30.6  \n8               35.6  \n"
'clean_text = []\nfor sentence in uncleaned_text:\n  for word in sentence.split():\n    if word in fulltext: \n      sentence = sentence.replace(word, fulltext[word])\n  clean_text.append(sentence)\n\n'
"import re\n[re.sub(r'@\\w+', '', x) for x in tweets]\n#['thank you', 'this reminds me of', 'this tweet has no username tag in it']\n"
"&gt;&gt;&gt; import re\n&gt;&gt;&gt; s = 'hello please help r me with this s question'\n&gt;&gt;&gt; re.sub(' . ', ' ', s)\n'hello please help me with this question'\n"
"from difflib import SequenceMatcher, _nlargest  # necessary imports of functions used by modified get_close_matches\n\ndef get_close_matches_lower(word, possibilities, n=3, cutoff=0.99):\n    if not n &gt;  0:\n        raise ValueError(&quot;n must be &gt; 0: %r&quot; % (n,))\n    if not 0.0 &lt;= cutoff &lt;= 1.0:\n        raise ValueError(&quot;cutoff must be in [0.0, 1.0]: %r&quot; % (cutoff,))\n    result = []\n    s = SequenceMatcher()\n    s.set_seq2(word)\n    for x in possibilities:\n        s.set_seq1(x.lower())  # lower-case for comparison\n        if s.real_quick_ratio() &gt;= cutoff and \\\n           s.quick_ratio() &gt;= cutoff and \\\n           s.ratio() &gt;= cutoff:\n            result.append((s.ratio(), x))\n\n    # Move the best scorers to head of list\n    result = _nlargest(n, result)\n    # Strip scores for the best n matches\n    return [x for score, x in result]\n\nprint(get_close_matches_lower('rfid alert', ['profile Caller','RFID alert']))\n\n['RFID alert']\n"
'df[&quot;colname&quot;] = df[&quot;colname&quot;].replace(sub_dict,regex=True)\n'
"def expand_singles(item):\n    if isinstance(item, list):\n        if len(item) == 1:\n            return expand_singles(item[0])\n        return [expand_singles(i) for i in item]\n    return item\n\n&gt;&gt;&gt; def expand_singles(item):\n...     if isinstance(item, list):\n...         if len(item) == 1:\n...             return expand_singles(item[0])\n...         return [expand_singles(i) for i in item]\n...     return item\n... \n&gt;&gt;&gt; l = [1, 2, 3, ['a', [[9, 10, 11]], 'c', 'd'], 5, ['e', 'f', [6, 7, [8]]]]\n&gt;&gt;&gt; expand_singles(l)\n[1, 2, 3, ['a', [9, 10, 11], 'c', 'd'], 5, ['e', 'f', [6, 7, 8]]]\n"
'data = json.loads(json_string)\ncleanup(data)\njson_string = json.dumps(data)\nprint json_string\n'
"DELETE FROM some_table WHERE tstamp &lt; (CURRENT_DATE - INTERVAL '180 days');\n\nCREATE OR REPLACE FUNCTION delete_180days(_to_date date)\nRETURNS void AS $$\n  DELETE FROM some_table\n    WHERE tstamp &lt; ($1 - INTERVAL '180 days')\n$$ LANGUAGE sql;\n"
"for line_number, line in enumerate(inputfile, 1):\n    if line_number &gt;= 80 and line_number &lt;= 2741: \n        outputfile.write(line)\n#                        ^^^^\n\nfrom itertools import islice\n\ndef removeparatext(inputFilename, outputFilename):\n    inputfile = open(inputFilename,'rt', encoding='utf-8')\n    outputfile = open(outputFilename, 'w', encoding='utf-8')\n\n    # use writelines to write sliced sequence of lines \n    outputfile.writelines(islice(inputfile, 79, 2741)) # indices start from zero\n\n    inputfile.close()\n    outputfile.close()\n\nfrom itertools import islice\n\ndef removeparatext(inputFilename, outputFilename):\n    with open(inputFilename,'rt', encoding='utf-8') as inputfile,\\\n         open(outputFilename, 'w', encoding='utf-8') as outputfile:    \n        # use writelines to write sliced sequence of lines \n        outputfile.writelines(islice(inputfile, 79, 2741))\n\n\nremoveparatext(inputFilename, outputFilename)\n"
"df.columns = pd.MultiIndex.from_arrays([range(len(df.columns)), df.columns])\n\nthreshold = .4\ndf[df.columns[df.isnull().mean() &lt; threshold]]\n\ndf_null_summary = pd.concat([df.isnull().sum(), df.isnull().mean()], axis=1, keys=['Missing Values', 'Percentage'])\n"
"bank_statement.columns.values[0] = 'Din'\n\nbank_statement = bank_statement.rename(columns={'Unnamed: 0':'Din'})\n\nIn [216]: df = pd.DataFrame(np.random.randn(3, 3), columns=list('abc'))\n\nIn [217]: df\nOut[217]:\n          a         b         c\n0 -0.972161 -0.484091 -0.289475\n1  1.081694  1.215217  0.241532\n2 -0.581193  0.691856  0.194182\n\nIn [218]: df.columns\nOut[218]: Index(['a', 'b', 'c'], dtype='object')\n\nIn [219]: df.columns.values[0] = 'Din'\n\nIn [220]: df.columns\nOut[220]: Index(['Din', 'b', 'c'], dtype='object')\n\nIn [221]: df['Din']\n...\nskipped\n...\nKeyError: 'Din'\n\nIn [222]: df['a']\nOut[222]:\n0   -0.972161\n1    1.081694\n2   -0.581193\nName: Din, dtype: float64\n\nIn [224]: df.columns = ['Din'] + df.columns.tolist()[1:]\n\nIn [225]: df.columns\nOut[225]: Index(['Din', 'b', 'c'], dtype='object')\n\nIn [226]: df['Din']\nOut[226]:\n0   -0.972161\n1    1.081694\n2   -0.581193\nName: Din, dtype: float64\n"
"import pandas as pd\ns = pd.Series(list('abca'))\npd.get_dummies(s)\n\nimport pandas as pd\nimport numpy as np\n\ns = pd.Series(['a', 'b', 'c', 'a|b', 'a|d'])\nd = pd.get_dummies(s)\n\ncolumns = list(d)\nfor col in columns:\n    if '|' in col:\n        for l in col.split('|'):\n            if l in columns:\n                d[l] = np.maximum(d[l].values, d[col].values)\n            else:\n                d[l] = d[col]\n"
",(?!\\s+\\d$)\n\nIn [227]: text = '52A, XYZ Street, ABC District, 2'\n\nIn [228]: re.sub(',(?!\\s+\\d$)', '', text)\nOut[228]: '52A XYZ Street ABC District, 2'\n\n(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)\n\nIn [229]: text = '52A, XYZ Street, ABC District, 2, M, Brown'\n\nIn [230]: re.sub('(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)', '', text)\nOut[230]: '52A XYZ Street ABC District, 2, M, Brown'\n\nIn [231]: text = '52A, XYZ Street, ABC District, 2'\n\nIn [232]: re.sub('(?&lt;!\\s[\\dA-Z]),(?!\\s+\\d,?)', '', text)\nOut[232]: '52A XYZ Street ABC District, 2'\n"
"&gt;&gt;&gt; s = 'Adam, 30 M, Husband'\n&gt;&gt;&gt; re.sub(r'(?is)(\\d+)(\\s)', '\\\\1, ', s)\n'Adam, 30, M, Husband'\n"
"SEyes = (df['Self_Employed']=='Yes').sum() + 1\nSEno = (df['Self_Employed']=='No').sum()\n\ndef calc():\n    rand_SE = np.random.randint(0,(SEno+SEyes))\n    if(rand_SE &gt;= 81):\n        return 'No'\n    else:\n        return 'Yes'\n\ndf.loc[df['Self_Employed'].isna(), 'Self_Employed'] = df.loc[df['Self_Employed'].isna(), 'Self_Employed'].apply(lambda x: calc())\n"
"df = df.set_index(['game_id','date','country'])\n\ndf.columns = df.columns.str.split('_', expand=True)\ndf = df.stack(0).reset_index().rename(columns={'level_3':'competitor'})\nprint (df) \n   game_id    date country competitor  age     name  ranking\n0        1  1/2/10      UK      loser   22  Michael       13\n1        1  1/2/10      UK     winner   21      Ben       12\n"
"l1 = ['Wirkstoffliste', 'Seite','Version']\n#i am with lines[:] (slicing) to play with the fact a list is mutable\nlines[:] = list(filter(str.strip,lines)) #suppress items whitespace or empty\nlines[:] = [x for x in lines if not any(sub in x for sub in l1)]\n\n# you could write these lines too if using a new list:\n#lines = list(filter(str.strip,lines))\n#lines = [x for x in lines if not any(sub in x for sub in l1)]\nprint(lines)\n\n['Gaschromatographie (GC) ', 'LOQ ', '[mg/kg] ', 'Acibenzolar-S-methyl', \n 'Aclonifen', 'Acrinathrin', 'Alachlor', 'Aldrin', 'Allethrin', 'Ametryn', \n 'Antrachinon', 'Atrazin', 'Atrazin-desethyl', 'Atrazin-desisopropyl', \n 'Azinphos (-ethyl)', 'Azinphos-methyl', 'Benalaxyl', \n 'Benfluralin', 'Benzoylprop-ethyl']\n\ndef remove_whitespaces_and_items(item):\n    if item.strip() == '': return False # if item is blank, dont keep\n    for x in l1:\n        if x in item:\n            return False                # if item of l1 is in lines, dont keep\n    return True                         # item is not blank and not in l1, so keep it\n\nlines =list(filter(remove_whitespaces_and_items,lines))\n"
"import time\nimport pandas as pd\nimport numpy as np\n\n# Create fake data (this should take around 9s)\ntic = time.time()\nvalue2 = []\nfor x in range(10000):\n    value1 = []\n    for y in range(10000):\n        value1.append(x)\n    value2.append(value1)\n\nprint(time.time() - tic)\ntic = time.time()\ntemp_dataset_1 = pd.DataFrame(value2)\n\n\nfor col in temp_dataset_1.iloc[:,:10]:\n    max_value = max(temp_dataset_1[col])\n    a = np.array(temp_dataset_1[col].values.tolist())\n    temp_dataset_1[col] = np.where(a &gt;= max_value*.9, 1, 0).tolist()\n\nprint(temp_dataset_1.shape)\ntoc = time.time() - tic\nprint('Calculating 10,000 out of 5,810,172 rows took %d seconds' %toc)\n\nCalculating 10,000 out of 5,810,172 rows took 19 seconds\n"
'new_date = pd.to_datetime("2250-05-05")\ndf[\'date\'] = pd.to_datetime(df.date, errors=\'coerce\').fillna(new_date)\n\n    col col2    date\n0    1  b1a2 2250-05-05\n1    2  bal2 2250-05-05\n2    3  a3l2 2250-05-05\n3    4  a5l2 2019-09-24\n4    5  a8l2 2012-09-28\n5    6  a1l2 2250-05-05\n6    7  a0l2 2250-05-05\n7    8  a2l2 2250-05-05\n8    9  a6l2 2250-05-05\n9   10  a5l2 2012-09-24\n\nto_dt = pd.to_datetime(df.date, errors=\'coerce\')\ninvalid_list = df.loc[to_dt[to_dt.isna()].index, \'date\'].dropna().values.tolist()\ndf[\'date\'] = to_dt.fillna(new_date)\n\nprint(invalid_list)\n[\'12012-09-14\',\n \'12017-09-14\',\n \'12113-09-14\',\n \'12012-09-24\',\n \'2500-09-28\',\n \'2500-09-14\']\n'
"import re, itertools as it\ndef get_vals(d):\n   r = [(a, list(b)) for a, b in it.groupby(re.findall('\\w+\\=|[^\\s,]+', d), key=lambda x:x[-1] == '=')]\n   return {r[i][-1][0][:-1]:', '.join(r[i+1][-1]) for i in range(0, len(r), 2)}\n\ntests = ['key1=value1, key2=value2, key3=value3', 'key1=va, lue1, key2=valu, e2, test, key3=value3']\nprint(list(map(get_vals, tests)))\n\n[{'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}, \n{'key1': 'va, lue1', 'key2': 'valu, e2, test', 'key3': 'value3'}]\n"
"temp = df['multi'].str.extract('\\(\\(\\((.*)\\)\\)\\)')[0].str.split(',', expand=True).T\n\ndf_new = pd.concat(\n    [temp[col].str.strip().str.split(' ', expand=True)\\\n     .rename(columns={0:f'a_{n+1}', 1:f'b_{n+1}'}) \n     for n, col in enumerate(temp.columns)], axis=1\n)\n\n   a_1  b_1  a_2  b_2  a_3  b_3\n0  1.1  1.2  3.1  3.2  5.1  5.2\n1  2.1  2.2  4.1  4.2  6.1  6.2\n\nnewdata = []\nfor n, col in enumerate(temp.columns):\n    dftemp = temp[col].str.strip().str.split(' ', expand=True)\n    dftemp = dftemp.rename(columns={0:f'a_{n+1}', 1:f'b_{n+1}'}) \n\n    newdata.append(dftemp)\n\nnewdf = pd.concat(newdata, axis=1)\n"
"apply_dict = {'Amount': ['sum', ('std', lambda s: s.std(ddof=0))], \n              'TransactionDate': ['min']}\n\n        Amount     TransactionDate\n           sum std             min\nAccount                           \nA           30   5      10-20-2018\n"
"x = {'column1': ['a','a','b','b','b','c','c','c','d'],\n    'column2': [22000,25000,27000,350,0,3,5,4,312]\n    }\ndf = pd.DataFrame(x, columns = ['column1', 'column2'])\nprint (df)\n\ns = df.groupby('column1')['column1'].transform('size')\n#alternative\n#s = df['column1'].map(df['column1'].value_counts())\nprint (s)\n0    2\n1    2\n2    3\n3    3\n4    3\n5    3\n6    3\n7    3\n8    1\nName: column1, dtype: int64\n\nL = [df[s &gt;= i].groupby(['column1',s]).head(i) if i &gt; 1 else g for i, g in df.groupby(s)]\nprint (L[0])\nprint (L[1])\nprint (L[2])\n  column1  column2\n8       d      312\n  column1  column2\n0       a    22000\n1       a    25000\n2       b    27000\n3       b      350\n5       c        3\n6       c        5\n  column1  column2\n2       b    27000\n3       b      350\n4       b        0\n5       c        3\n6       c        5\n7       c        4\n\ndef func(x, g):\n    if x == 1:\n        return g\n    else:\n        df1 = df[s &gt;= x].groupby(['column1',s]).head(x)\n        if x == 3:\n            return (df1.groupby(['column1',s], group_keys=False)\n                       .nth([0, -1])\n                       .reset_index(level=1, drop=True)\n                       .reset_index())\n\n        else:\n            return df1\n\nL = [func(i, g) for i, g in df.groupby(s)]\nprint (L[0])\nprint (L[1])\nprint (L[2])\n  column1  column2\n8       d      312\n  column1  column2\n0       a    22000\n1       a    25000\n2       b    27000\n3       b      350\n5       c        3\n6       c        5\n  column1  column2\n0       b    27000\n1       b        0\n2       c        3\n3       c        4\n"
"new_df = (df.groupby(df['token'].eq('.').shift(fill_value=False).cumsum(),\n        as_index=False)\n            .agg({'token' : ' '.join, 'annotation': list}))\nprint(new_df)\n                                               token  \\\n0  Effect of ginseng extract supplementation on t...   \n1                                   OBJECTIVE It was   \n\n                             annotation  \n0  [O, O, i, i, i, O, o, o, O, p, p, p]  \n1                             [O, O, O]\n\nm = df['token'].eq('.')\nnew_df = (df.groupby(m.shift(fill_value=False).cumsum().loc[~m],as_index=False)\n            .agg({'token' : ' '.join, 'annotation': list}))\n"
'm = df.loc[:,df.columns.str.endswith("Price")]\ndf[\'actual_Price\'] = m.lookup(df.index,df[\'Region\'].add(" Price"))\n\nprint(df)\n   ID Supplier    Region  Av Price  Eastern Price  Southern Price  \\\n0   1      EDF   Eastern       400            500             300   \n1   2      EDF  Southern       200            100             300   \n2   3   NPower   Eastern       600            500             700   \n\n   actual_Price  \n0           500  \n1           300  \n2           500  \n'
"salary_cleaned_values = []\ncolumnSeriesObj = needtoclean['Salary']\nfor value in columnSeriesObj.values:\n    #print(value)\n    value = value.replace('Nuo',&quot;&quot;).replace('Iki','')\n    value = value.strip()\n    value = re.split(r'[\\s,-]+', value)\n    if len(value) &gt; 1:  #\n        value = (int(value[0])+int(value[1]))/2\n    else:\n        value = float(value[0])\n    value = round(value)\n    salary_cleaned_values.append(value)\nneedtoclean['Salary'] = salary_cleaned_values\n"
"(df.merge(df, on='event_id')\n   .groupby(['people_id_x', 'people_id_y'])\n   .size()\n   .unstack('people_id_x', fill_value=0)\n)\n\npeople_id_x  30   77   92   96   130\npeople_id_y                         \n30             1    1    0    1    1\n77             1    2    0    1    1\n92             0    0    1    0    0\n96             1    1    0    1    1\n130            1    1    0    1    2\n"
"df['time'] = pd.to_datetime(df['time']).dt.strftime('%-m %d, %Y')\nprint(df)\n\n               place         time\n0             canada   2 11, 2018\n1             canada  12 09, 2017\n2  the united states   1 18, 2018\n"
"filtered_text = s.str.replace('[^\\w\\s]','')\n"
'import json\n\nmessy_string = file("addresses.txt").readlines()\n\nfor line in messy_string:\n  try:\n    parsed = json.loads(line)\n    column_names = parsed.keys()\n    column_values = parsed.values()\n    print parsed\n  except:\n    raise \'Could not parse line\'\n'
"d = a[0]\nd['text']\n"
'for elm in soup.find("span", id="job_summary").p.find_all("b"):\n    label, text = elm.get_text().split(" : ")\n\n    print(label.strip(), text.strip())\n'
'convertRaw = function(x) paste(x,collapse = \'\') # works identical in sapply\ntest = as.raw(as.hexmode(x = c("56","cd","5f","02","b8","9b","5b","d0","26","cb","39","c9"))) # line copied from your sample\nconvertRaw(test)\n[1] "56cd5f02b89b5bd026cb39c9"\n\ntest = as.raw(as.hexmode(x = c("56","cd","5f","02","b8","9b","5b","d0","26","cb","39","c9")))\ntestList = list(list(),list(test,test)) # here I create a short nested list\nres = lapply(testList,function(y) sapply(y,function(x) paste(x,collapse = \'\')))\nprint(res) \n\n[[1]] list() \n\n[[2]] [1] "56cd5f02b89b5bd026cb39c9" "56cd5f02b89b5bd026cb39c9"\n\n[[1]] list()\n\n[[2]] [[2]][[1]] \n[1] "56cd5f02b89b5bd026cb39c9"\n\n[[2]][[2]] \n[1] "56cd5f02b89b5bd026cb39c9"\n\nlapply(testList,function(y) lapply(y,function(x) paste(x,collapse = \'\')))\n'
"\\{(?P&lt;individual&gt;[^{}]+)\\}@(?P&lt;domain&gt;\\S+)\n# looks for {\n# captures everything not } into the group individual\n# looks for @ afterwards\n# saves everything not a whitespace into the group domain\n\nimport re\nrx = r'\\{(?P&lt;individual&gt;[^{}]+)\\}@(?P&lt;domain&gt;\\S+)'\nstring = 'gibberish {kevin.knerr, sam.mcgettrick, mike.grahs}@google.com.au gibberish'\nfor match in re.finditer(rx, string):\n    print match.group('individual')\n    print match.group('domain')\n"
"import pandas as pd\n\ndf1 = pd.read_csv('./Turkey_28.csv')\n\ncoords = df1[['tweetID', 'Coordinates']].set_index('tweetID')['Coordinates']\n\ncoords = coords.dropna().apply(lambda x: eval(x))\ncoords = coords[coords.apply(type) == dict]\n\ndef get_coords(x):\n    return pd.Series(x['coordinates'], index=['Coordinate_one', 'Coordinate_two'])\n\ncoords = coords.apply(get_coords)\n\ndf2 = pd.concat([coords, df1.set_index('tweetID').reindex(coords.index)], axis=1)\n\nprint df2.head(2).T\n\ntweetID                                         714602054988275712\nCoordinate_one                                             23.2745\nCoordinate_two                                             56.6165\ntweetText        I'm at MK Appartaments in Dobele https://t.co/...\ntweetRetweetCt                                                   0\ntweetFavoriteCt                                                  0\ntweetSource                                             Foursquare\ntweetCreated                                   2016-03-28 23:56:21\nuserID                                                   782541481\nuserScreen                                            MartinsKnops\nuserName                                             Martins Knops\nuserCreateDt                                   2012-08-26 14:24:29\nuserDesc         I See Them Try But They Can't Do What I Do. Be...\nuserFollowerCt                                                 137\nuserFriendsCt                                                  164\nuserLocation                                        DOB Till I Die\nuserTimezone                                            Casablanca\nCoordinates      {u'type': u'Point', u'coordinates': [23.274462...\nGeoEnabled                                                    True\nLanguage                                                        en\n"
"filter = ['Like', 'Liked']\n\ndf.apply(lambda column: \n    column[~(column.isnull() | column.isin(filter))].reset_index(drop=True)\n)\n\nimport numpy as np\nfilter = [np.nan, 'Like', 'Liked']\n\ndf.apply(lambda column: column[~column.isin(filter)].reset_index(drop=True))\n\n        Aman Aggarwal    Amar Jannela        Vipin Kumar         Roshan Pati\n0           BlackBuck       DJ CHETAS       WOW Editions              MensXP\n1   Transport/Freight   Musician/Band          Furniture  News/Media Website\n2         GiveMeSport   Celina Jaitly       500 Startups         No Abuse KG\n3  News/Media Website  Actor/Director  Internet/Software           Community\n4      Anushka Sharma    Durjoy Datta     Jitendra Kumar   Monogatari Series\n5      Actor/Director          Author     Actor/Director             TV Show\n"
"re.compile(', \\'(?=\\w*\\': )')\n"
"isnumeric = lambda s: pd.to_numeric(s, errors='coerce').notnull()\ndf[isnumeric(df['Age']) &amp; ~isnumeric(df['product'])]\n\n#  Age    product\n#1  21      apple\n#2  11     orange\n#4  35  pineapple\n"
'l=[\'\\n\\r\\n\\tThis article is about sweet bananas. For the genus to which banana plants belong, see Musa (genus).\\n\\r\\n\\tFor starchier bananas used in cooking, see Cooking banana. For other uses, see Banana (disambiguation)\\n\\r\\n\\tMusa species are native to tropical Indomalaya and Australia, and are likely to have been first domesticated in Papua New Guinea.\\n\\r\\n\\tThey are grown in 135 countries.\\n\\n\\n\\r\\n\\tWorldwide, there is no sharp distinction between "bananas" and "plantains".\\n\\nDescription\\n\\r\\n\\tThe banana plant is the largest herbaceous flowering plant.\\n\\r\\n\\tAll the above-ground parts of a banana plant grow from a structure usually called a "corm".\\n\\nEtymology\\n\\r\\n\\tThe word banana is thought to be of West African origin, possibly from the Wolof word banaana, and passed into English via Spanish or Portuguese.\\n\']\n\nimport re\nregex=re.findall("\\n\\n.*.\\n\\r\\n\\t",l[0])\nprint(regex)\n\nfor x in regex:\n    l = [r.replace(x,"&lt;subtitles&gt;") for r in l]\n\nrep = [\'\\n\',\'\\t\',\'\\r\']\nfor y in rep:\n    l = [r.replace(y, \'\') for r in l]\n\nfor x in regex:\n    l = [r.replace(\'&lt;subtitles&gt;\', x, 1) for r in l]\nprint(l)\n\n[\'\\n\\nDescription\\n\\r\\n\\t\', \'\\n\\nEtymology\\n\\r\\n\\t\']\n\n[\'This article is about sweet bananas. For the genus to which banana plants belong, see Musa (genus).For starchier bananas used in cooking, see Cooking banana. For other uses, see Banana (disambiguation)Musa species are native to tropical Indomalaya and Australia, and are likely to have been first domesticated in Papua New Guinea.They are grown in 135 countries.Worldwide, there is no sharp distinction between "bananas" and "plantains".\\n\\nDescription\\n\\r\\n\\tThe banana plant is the largest herbaceous flowering plant.All the above-ground parts of a banana plant grow from a structure usually called a "corm".\\n\\nEtymology\\n\\r\\n\\tThe word banana is thought to be of West African origin, possibly from the Wolof word banaana, and passed into English via Spanish or Portuguese.\']\n'
'v = df.values * df_test.amount.values[:, None]\n\nv\narray([[    0,    38,     0,     0],\n       [ 2179,     0,  2179,  2179],\n       [    0,     0,     0,   191],\n       [    0,     0,     0,     0],\n       [19823, 19823, 19823, 19823]])\n\ndf = pd.DataFrame(v, columns=df.columns, index=df.index)\ndf\n\nfirst     bar           baz       \nsecond    one    two    one    two\n0           0     38      0      0\n1        2179      0   2179   2179\n2           0      0      0    191\n3           0      0      0      0\n4       19823  19823  19823  19823\n'
"## using applymap here\nweather_list[numeric_cols] = weather_list[numeric_cols].applymap(lambda x: re.sub(r'[^0-9]', '', str(x)))\n\n## now we pass series to pd.to_numeric instead of data frame\nweather_list[numeric_cols] = weather_list[numeric_cols].apply(lambda x: pd.to_numeric(x, errors='coerce'))\n\nweather_list[numeric_cols] = weather_list[numeric_cols] / 10\n"
"def parsedate(s):\n    try:\n        return datetime.strptime(s, '%y-%b')\n    except ValueError:\n        pass\n    try:\n        return datetime.strptime(s, '%b-%y')\n    except ValueError:\n        pass\n    return datetime.now() # &lt;whatever you actually want to do for des-3 here&gt;\ndf.end = df.end.apply(parsedate)\n\nMONTHS = {\n    'jan': 1, 'feb': 2, 'mar': 3, 'apr': 4, 'may': 5, 'jun': 6,\n    'jul': 7, 'aug': 8, 'sep': 9, 'oct': 10, 'nov': 11, 'dec': 12, 'des': 12 }\ndef parsedate(s):\n    part1, _, part2 = s.partition('-')\n    if part2.isdigit():\n        part1, part2 = part2, part1\n    return datetime(year=2000+int(part1), month=MONTHS[part2], day=1)\n"
"Barbecue = {'Cheese': ['earthyCamembert', 'Fontina', 'nuttyAsiago', 'Colby', 'Parmesan'],\n            'General': ['Chocolate'],\n            }\n"
'text = "art"\ntext = text.encode("ascii", "ignore").decode()\nprint(text)     # art\n\nbites = "art".encode("utf8")\ntext = bites.decode("ascii", "ignore")\nprint(text)     # art\n'
"    A   B        \n0   1   2         \n1   0  \\N      \n2  \\N   8       \n3  11   5       \n4  11  Kud   \n\ndf = df.apply(pd.to_numeric, errors='coerce')\n\n     A     B        \n0    1     2         \n1    0   NaN      \n2  NaN     8       \n3   11     5       \n4   11   NaN   \n"
'df = dataset[dataset.date.str.contains(" 2:00:00",regex=False)\n'
'df["column"].apply(lambda x: x if x.isalpha() else None)\n\nmy_cities = [\'Melbourne\', \'Sydney\']\n\nis_city = df[\'column\'].isin(my_cities)\n\ndf.loc[~is_city, \'column\'] = None\n\ndf["new_column"].fillna(method="ffill")\n\ndf.dropna()\n'
"import numpy as np\ndef binner(df,num_bins):\n    for c in df.columns:\n        cbins = np.linspace(min(df[c]),max(df[c]),num_bins+1)\n        df[c + '_binned'] = np.digitize(df[c],cbins)\n    return df\n"
'if \'RestaurantsPriceRange2\' in data2[entry][\'attributes\']:\n\nprint data2[21]\n\n{u\'city\': u\'Cleveland\', u\'neighborhood\': u\'Central\', u\'name\': u"Rally\'s Hamburgers", u\'business_id\': u\'gJ5xSt6147gkcZ9Es0WxlA\', u\'longitude\': -81.6663746, u\'hours\': None, u\'state\': u\'OH\', u\'postal_code\': u\'44115\', u\'categories\': u\'Fast Food, Burgers, Restaurants\', u\'stars\': 3.0, u\'address\': u\'3040 Carnegie Ave\', u\'latitude\': 41.4999894, u\'review_count\': 5, u\'attributes\': None, u\'is_open\': 1}\n\nif data2[entry][\'attributes\'] != None:\n\nimport json;\n\ndata2 = []\nwith open(\'yelp_academic_dataset_business.json\') as f:\n    for line in f:\n        data2.append(json.loads(line))\nlen(data2)\n\nbusiness_id = []\ncity = []\nstate = []\nstars = []\nreview_count = []\ncategories = []\npostal_code = []\nlatitude = []\nlongitude = []\npricerange = []\nis_open = []\nname = []\n\nfor entry in range(0, len(data2)):\n    if data2[entry]["categories"] != None:\n        if "Restaurants" in data2[entry]["categories"]:\n            business_id.append(data2[entry][\'business_id\'])\n            name.append(data2[entry][\'name\'])\n            city.append(data2[entry][\'city\'])\n            state.append(data2[entry][\'state\'])\n            stars.append(data2[entry][\'stars\'])\n            postal_code.append(data2[entry][\'postal_code\'])\n            review_count.append(data2[entry][\'review_count\'])\n            categories.append(data2[entry][\'categories\'])\n            latitude.append(data2[entry][\'latitude\'])\n            longitude.append(data2[entry][\'longitude\'])\n            is_open.append(data2[entry][\'is_open\'])\n            if data2[entry][\'attributes\'] != None:\n                if \'RestaurantsPriceRange2\' in data2[entry][\'attributes\']:\n                     pricerange.append(data2[entry][\'attributes\'][\'RestaurantsPriceRange2\'])\n                else:\n                    pricerange.append(0)\n\ndata2 = {\'business_id \':business_id,\'name\':name,\'city\':city,\'state\':state,\'stars\':stars,\'review_count\':review_count,\'categories\':categories,\'latitude\':latitude,\'longitude\':longitude,\'is_open\':is_open,\'pricerange\':pricerange,\'postal_code\':postal_code}\n'
"city_st = dict(zip(state,city))\n\naa = pd.dataframe({'state': state,'city': city})\naa['State' ] = range(aa.shape[0])\n"
'import re\ndf.col1 = df.col1.apply(lambda x: re.sub(r\'\\x95\',"",x))\n'
'import requests\n\ndown_url = "https://downsub.com/index.php?title=5+Am+Club+by+Robin+Sharma+%7C%7C+Review%2C+Takeaways+and+Discussion&amp;url=ujcNaH9TaDy8U56iM_1ZReCKk1h83rjIvE7i146GYHUNIwZwKM02q9oUSieWkZ47Tw2OOJgFBvlU0he-sxkGIASxRnTcdMwE1QrZ3CAsyI5gLS6A0ovxFMmFJx5EAC5wtwexy0R1vzZfNdt6dBse3H-vOhq8xnqL-LdhSbiePZ5E_KEYrYuFzPvF2JpEARuCOA6XlqQQzV7iooSEObb9AejBkNj_uHhNnO0RVJ0E-pVAJjWLdjUnIdXGPkJUsd5Ceg5qeTVKjtBQhWyf6qCuwE_BAezDSDAF6DgLCFRnwc2Uc9onnorwYncvzIge1soln3FnkifpyHiPB3cK0h0f5yMUy-DJHervcQXQEHdUf-npkCzRgeba283yoN7orAovE0iaIihvFMectGYKT27eXLdrLdQQ3sUcWFqRB6SjZ8g"\n\ndata = requests.get(down_url).text\n\n\nclean = [\'&lt;font color="#CCCCCC"&gt;\',\n         \'&lt;font color="#E5E5E5"&gt;\',\n         \'&lt;font color="#EEE"&gt;\',\n         \'&lt;/font&gt;\',\n         0,1,2,3,4,5,6,7,8,9,\n         \'::, --&gt; ::,\',]\n\n\nfor s in clean:\n    data = data.replace(str(s), \'\')\n\ndata = data.replace(\'\\n\\n\\n\\n\', \'\\n\')\n\nprint(data)\n'
'import re\nproduct = [int(re.search("\\d+",e).group()) for l in products for e in l]\n\n[6836518,\n 5965878,\n 3851171,\n 6455623,\n 8024914,\n 2871360,\n 6694729,\n 6760262,\n 6466698,\n 5340641,\n 6071996,\n 5379225,\n 6683916,\n 6690577,\n 7117851,\n 7094467,\n 6628351,\n 5897930,\n 6812437,\n 5379225,\n 7918467,\n 7918466]\n'
"import pandas as pd\ndf=pd.DataFrame([[1,2,3],[4,5,6],[7,'F',8]],columns=['col1','col2','col3'])\ndf['col2']=pd.to_numeric(df['col2'],errors='coerce')\ndf.dropna(inplace=True)\n"
"def myfunc(row):\n    if row['t1_check']:\n        return row['t1']\n    elif row['t2_check']:\n        return row['t2']\n    elif row['t3_check']:\n        return row['t3']\n    return np.nan\n\ndf['type']=df.apply(myfunc,axis=1)\ndf[['name','type']]\n\nindex    name       type\n----------------------------\n0        cow        animal\n1        apple      fruit\n2        carrot     veg\n3        dog        pet\n4        horse      animal\n5        car        NaN\n"
"df2 = df[df.duplicated(['Product', 'User'], keep=False)]\n\ndf2.groupby(['Product', 'User']).count()\n"
"df1['ListCol']=df1['ColC']# Here I am try to record the original data \nYourdf=unnesting(df1,['ColC']).merge(df2, on=['ColA','ColC'],how='inner')\nYourdf\n   ColC ColA  ColB    ListCol\n0     2    A     1  [1, 2, 3]\n1     3    A     1  [1, 2, 3]\n2     6    A     2  [4, 5, 6]\n3     2    B     4  [1, 2, 3]\n4     5    B     5  [3, 4, 5]\n\ndef unnesting(df, explode):\n    idx = df.index.repeat(df[explode[0]].str.len())\n    df1 = pd.concat([\n        pd.DataFrame({x: np.concatenate(df[x].values)}) for x in explode], axis=1)\n    df1.index = idx\n\n    return df1.join(df.drop(explode, 1), how='left')\n"
'f1 = open(\'Sample - Sheet1.csv\')\nf2 = open(\'temp.csv\', \'w\')\nfor row in f1:\n    row = row.strip() # remove "\\n"\n    row = row[1:-1] # remove " on both ends\n    row = row.replace(\'""\', \'"\') # conver "" into "\n    f2.write(row + \'\\n\')\nf2.close()\nf1.close()\n\ndf = pd.read_csv(\'temp.csv\')\n\nprint(len(df.columns))\nprint(df)\n\nimport csv\n\nf1 = open(\'Sample - Sheet1.csv\')\nf2 = open(\'temp.csv\', \'w\')\n\nreader = csv.reader(f1)\nfor row in reader:\n    f2.write(row[0] + \'\\n\')\n\nf2.close()\nf1.close()\n\n\ndf = pd.read_csv(\'temp.csv\')\n\nprint(len(df.columns))\nprint(df)\n'
"conditions = [\n    df['Date'].between('2008-08-30', '2009-05-31', inclusive=True),\n    df['Date'].between('2009-08-22', '2010-05-16', inclusive=True),\n    df['Date'].between('2010-08-28', '2011-05-22', inclusive=True)\n]\n\nchoices = ['08-09', '09-10', '10-11']\n\ndf['season'] = np.select(conditions, choices, default='99-99')\n\ndef get_mean(df, group, mean_col):\n\n    df['mean'] = df.groupby(group)[mean_col].transform('mean')\n\n    return df\n\n# Example dataframe\ndf = pd.DataFrame({'Fruit':['Banana', 'Strawberry', 'Apple', 'Banana', 'Apple'],\n                   'Weight':[10, 12, 8, 9, 14]})\n\n        Fruit  Weight\n0      Banana      10\n1  Strawberry      12\n2       Apple       8\n3      Banana       9\n4       Apple      14\n\nget_mean(df, 'Fruit', 'Weight')\n\n        Fruit  Weight  mean\n0      Banana      10   9.5\n1  Strawberry      12  12.0\n2       Apple       8  11.0\n3      Banana       9   9.5\n4       Apple      14  11.0\n"
'from operator import attrgetter\n\ndef extract_rows(df, column_name, value):\n    return df[attrgetter(column_name)(df) == value]\n\ndef extract_rows(df, column_name, value):\n    return df[df[column_name] == value]\n'
"'I want a cat'\n"
'python3 -m cProfile -o so57333255.py.prof so57333255.py\npython3 -m pstats  so57333255.py.prof\n\n         2351652 function calls (2335973 primitive calls) in 9.843 seconds\n\n   Ordered by: cumulative time\n   List reduced from 4964 to 5 due to restriction &lt;5&gt;\n\n   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n   1373/1    0.145    0.000    9.852    9.852 {built-in method exec}\n        1    0.079    0.079    9.852    9.852 so57333255.py:2(&lt;module&gt;)\n        9    0.003    0.000    5.592    0.621 {pandas._libs.lib.map_infer}\n        8    0.001    0.000    5.582    0.698 /usr/local/lib/python3.4/dist-packages/pandas/core/series.py:2230(apply)\n      100    0.001    0.000    5.341    0.053 /usr/local/lib/python3.4/dist-packages/langdetect/detector_factory.py:126(detect)\n\nTotal time: 8.59578 s\nFile: so57333255a.py\nFunction: runit at line 8\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     8                                           @profile\n     9                                           def runit():\n    10                                           \n    11                                               # define corpus\n    12         1     385710.0 385710.0      4.5      words = set(nltk.corpus.words.words())\n    13                                           \n    14                                               # define stopwords\n    15         1       2068.0   2068.0      0.0      stop = stopwords.words(\'english\')\n    16         1         10.0     10.0      0.0      newStopWords = [\'oz\',\'stopWord2\']\n    17         1          9.0      9.0      0.0      stop.extend(newStopWords)\n    18                                           \n    19                                               # read csv into dataframe\n    20         1      46880.0  46880.0      0.5      df=pd.read_csv(\'reviews.csv\', names=[\'reviews\'], header=None, nrows=100)\n    21                                           \n    22                                               # unescape reviews (fix html encoding)\n    23         1      16922.0  16922.0      0.2      df[\'clean_reviews\'] = df[\'reviews\'].apply(unescape, unicode_snob=True)\n    24                                           \n    25                                               # remove non-ASCII characters\n    26         1      15133.0  15133.0      0.2      df[\'clean_reviews\'] = df["clean_reviews"].apply(lambda x: \'\'.join([" " if ord(i) &lt; 32 or ord(i) &gt; 126 else i for i in x]))\n    27                                           \n    28                                               # calculate number of stop words in raw reviews\n    29         1      20721.0  20721.0      0.2      df[\'stopwords\'] = df[\'reviews\'].apply(lambda x: len([x for x in x.split() if x in stop]))\n    30                                           \n    31                                               # lowercase reviews\n    32         1       5325.0   5325.0      0.1      df[\'clean_reviews\'] = df[\'clean_reviews\'].apply(lambda x: " ".join(x.lower() for x in x.split()))\n    33                                           \n    34                                               # add a space before and after every punctuation \n    35         1       9834.0   9834.0      0.1      df[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(r\'([^\\w\\s]+)\', \' \\\\1 \')\n    36                                           \n    37                                               # remove punctuation\n    38         1       3262.0   3262.0      0.0      df[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(\'[^\\w\\s]\',\'\')\n    39                                           \n    40                                               # remove stopwords\n    41         1      20259.0  20259.0      0.2      df[\'clean_reviews\'] = df[\'clean_reviews\'].apply(lambda x: " ".join(x for x in x.split() if x not in stop))\n    42                                           \n    43                                               # remove digits\n    44         1       2897.0   2897.0      0.0      df[\'clean_reviews\'] = df[\'clean_reviews\'].str.replace(\'\\d+\', \'\')\n    45                                           \n    46                                               # remove non-corpus words\n    47         1          9.0      9.0      0.0      def remove_noncorpus(sentence):\n    48                                                   #print(sentence)\n    49                                                   return " ".join(w for w in nltk.wordpunct_tokenize(sentence) if w.lower() in words or not w.isalpha())\n    50                                           \n    51         1       6698.0   6698.0      0.1      df[\'clean_reviews\'] = df[\'clean_reviews\'].map(remove_noncorpus)\n    52                                           \n    53                                               # count number of characters\n    54         1       1912.0   1912.0      0.0      df[\'character_count\'] = df[\'clean_reviews\'].apply(len)\n    55                                           \n    56                                               # count number of words\n    57         1       3641.0   3641.0      0.0      df[\'word_count\'] = df[\'clean_reviews\'].str.split().str.len()\n    58                                           \n    59                                               # average word length\n    60         1          9.0      9.0      0.0      def avg_word(sentence):\n    61                                                 words = sentence.split()\n    62                                                 #print(sentence)\n    63                                                 return (sum(len(word) for word in words)/len(words)) if len(words)&gt;0 else 0\n    64                                           \n    65         1       3445.0   3445.0      0.0      df[\'avg_word\'] = df[\'clean_reviews\'].apply(lambda x: avg_word(x))\n    66         1       3786.0   3786.0      0.0      df[[\'clean_reviews\',\'avg_word\']].head()\n    67                                           \n    68                                               # detect language of reviews\n    69         1    8037362.0 8037362.0     93.5      df[\'language\'] = df[\'clean_reviews\'].apply(detect)\n    70                                           \n    71                                               # filter out non-English reviews\n    72         1       1453.0   1453.0      0.0      msk = (df[\'language\'] == \'en\')\n    73         1       2353.0   2353.0      0.0      df_range = df[msk]\n    74                                           \n    75                                               # write dataframe to csv\n    76         1       6087.0   6087.0      0.1      df_range.to_csv(\'dataclean.csv\', index=False) \n'
"(dat &gt; dat.median()).astype('int')\n\nOut[23]: \n   a  b\n0  0  1\n1  0  0\n2  0  1\n3  1  0\n4  1  0\n5  1  1\n\nnp.random.seed(1234)\ndat = pd.DataFrame({'a' : np.arange(6), 'b': np.random.randn(6)})\n\ndat\nOut[24]: \n   a         b\n0  0  0.471435\n1  1 -1.190976\n2  2  1.432707\n3  3 -0.312652\n4  4 -0.720589\n5  5  0.887163\n"
'&gt;&gt;&gt; import pandas as pd                                                         \n&gt;&gt;&gt; survey = pd.DataFrame( \n...     ["Virginia", "VA", "VA", "Penns.", "PA", "Pennsylvania"], \n...     columns=["State"] \n... )                                                                           \n&gt;&gt;&gt; survey                                                                      \n          State\n0      Virginia\n1            VA\n2            VA\n3        Penns.\n4            PA\n5  Pennsylvania\n\n&gt;&gt;&gt; to_abbrev = { \n...     "Virginia": "VA", \n...     "Pennsylvania": "PA", \n...     "Penns.": "PA", \n... }\n\n&gt;&gt;&gt; to_abbrev.update({v: v for v in to_abbrev.values()})          \n&gt;&gt;&gt; to_abbrev                                                                                                                                                                                                                                                \n{\'Virginia\': \'VA\',\n \'Pennsylvania\': \'PA\',\n \'Penns.\': \'PA\',\n \'VA\': \'VA\',\n \'PA\': \'PA\'}\n\n&gt;&gt;&gt; survey["State"].map(to_abbrev)                                                                                                                                                                                                                           \n0    VA\n1    VA\n2    VA\n3    PA\n4    PA\n5    PA\nName: State, dtype: object\n\n&gt;&gt;&gt; survey.append({"State": "Wisconsin"}, ignore_index=True)["State"].map(to_abbrev)                                                                                                                                                                         \n0     VA\n1     VA\n2     VA\n3     PA\n4     PA\n5     PA\n6    NaN\nName: State, dtype: object\n'
'files = glob.glob(\'*.csv\')\n\nfor file in files:\n    df = pd.read_csv(file)\n    try:\n        df[\'Condition\'] == "After food"\n        # do something.\n        df.to_csv(f\'{file}.csv\',index=False)\n        print(f\'{file} has been altered\')\n    except KeyError:\n        print(f\'{file} has not met the condition, therefore has not been changed.\')\n    except EmptyDataError:\n        print(f"this {file} has no data to parse")\n\nPayCodes.csv has been altered\nbirmingham.csv has not met the condition\nCF44.csv has not met the condition\nDE11.csv has not met the condition\nDublin.csv has not met the condition\nDY8.csv has not met the condition\n'
'df.iloc[:, :int(df.count(axis=1).mean())]\n\n    0   1   2   3     4     5     6\n0  11  22  33  44  55.0  66.0  77.0\n1  12  13  14  15   NaN   NaN   NaN\n2  11  22  33  44  55.0  66.0  77.0\n'
"df = pd.DataFrame({'Cleaned': ['Alabama', 'Auburn (Auburn University)', 'Alaska']})\ndf[['State', 'University']] = df.Cleaned.str.split('(', n = 1, expand = True)\ndf.University = df.University.str.rstrip(')')\n\ndf.State = np.where(df.University.map(lambda u: u is None), df.State, np.nan)\ndf.State = df.State.fillna(method = 'ffill')\n"
"df = pd.DataFrame(abc)\ndf['is_1st_exist?'] = df.eq(1).any(axis=1).astype(int)\n#alternative\n#df['is_1st_exist?'] = np.where(df.eq(1).any(axis=1), 1, 0)\nprint (df)\n   p1  p2  p3  is_1st_exist?\n0   1   2   3              1\n1   2   3   4              0\n2   3   4   5              0\n3   4   5   6              0\n4   5   6   7              0\n5   6   7   8              0\n6   7   8   9              0\n7   8   9   1              1\n8   9   1   2              1\n9   1   2   3              1\n\ncols = ['p1','p2','p3']\ndf['is_1st_exist?'] = df[cols].eq(1).any(axis=1).astype(int)\n"
'df1=df.copy()\ndf1["WIN_COUNTRY_CODE"]=df[\'WIN_COUNTRY_CODE\'].str.split(\'---\')\ndf1["Max_code"]=df1["WIN_COUNTRY_CODE"].apply(lambda x: max(set(x), key = x.count))\n'
'                                0\n0  Action Adventure RPG Roguelike\n1      Action Shoot\'em Up Wargame\n\nsep = ["Action", "Adventure", "RPG", "Roguelike", "Shoot\'em Up", "Wargame"]\npattern = \'|\'.join(sep)\n\n\npd.DataFrame(df[0].str.findall(pattern).tolist())\n\n        0            1        2          3\n0  Action    Adventure      RPG  Roguelike\n1  Action  Shoot\'em Up  Wargame       None\n'
"replace_map = {\n    '[Q|q]uick[P|p]ay with [Z|z]elle payment to ': 'Payment to',\n    '[Q|q]uick[P|p]ay with [Z|z]elle payment from ': 'Payment from'\n}\n\ndf.replace({'title': replace_map}, regex=True, inplace=True)\n\n&gt;&gt;&gt; df\n                               title\n0    QuickPay with Zelle payment to \n1    quickPay with zelle payment to \n2    quickpay with zelle payment to \n3  QuickPay with Zelle payment from \n4  Quickpay with zelle payment from \n\n&gt;&gt;&gt; replace_map = {\n...     '[Q|q]uick[P|p]ay with [Z|z]elle payment to ': 'Payment to',\n...     '[Q|q]uick[P|p]ay with [Z|z]elle payment from ': 'Payment from'\n... }\n&gt;&gt;&gt; df.replace({'title': replace_map}, regex=True, inplace=True)\n&gt;&gt;&gt; df\n          title\n0    Payment to\n1    Payment to\n2    Payment to\n3  Payment from\n4  Payment from\n"
'# if index not datetime object, then\n# df.index = pd.to_datetime(df.index)\n&gt;&gt;&gt; pd.Series(df.index).diff().mean().components.minutes\n20\n#or,\n&gt;&gt;&gt; pd.Series(df.index).diff().iloc[-1].components.minutes\n20\n'
'aviationdata = aviationdata[["Location", "Country", "Make", "Weather.Condition", "Year", "Month"]]\n\naviationdata = aviationdata.merge(currentlocation, on=[\'Location\'])\n\naviationdata.head(10)\n'
'import bs4\nfrom urllib.request import urlopen as req\nfrom bs4 import BeautifulSoup as soup\nimport csv\n#Link de la pgina on farem webscraping\nurl = \'https://www.newegg.com/Laptops-Notebooks/SubCategory/ID-32?Tid=6740\'\n\n#Obrim una connexi amb la pgina web\nClient = req(url)\n#Offloads the content of the page into a variable\npagina = Client.read()\n#Closes the client\nClient.close()\n#html parser\npagina_soup=soup(pagina,"html.parser")\n#grabs each product\nproductes = pagina_soup.findAll("div",{"class":"item-container"})\n\n #Obrim un axiu .csv\n#Capaleres del meu arxiu .csv\nresult_file = open("ordinadors.csv",\'a\',encoding=\'utf-8\',newline=\'\')\n #Escrivim la capalera\nhead = ["Marca","Producte","PreuActual","PreuAnterior","CostEnvio","Rebaixa"]\nwriting_csv = csv.DictWriter(result_file, fieldnames=head)\nwriting_csv.writeheader()\n\n#Fem un loop sobre tots els productes\nfor producte in productes:\n\n    #Agafem la marca del producte\n    marca_productes = producte.findAll("div",{"class":"item-info"})\n    marca = marca_productes[0].div.a.img["title"]\n\n    #Agafem el nom del producte\n    name = producte.a.img["title"] \n\n    #Preu Actual\n    actual_productes = producte.findAll("li",{"class":"price-current"})\n    preuActual = actual_productes[0].strong.text\n\n    #Preu anterior    \n    try:\n        #preuAbans = producte.find("li", class_="price-was").next_element.strip()\n        preuAbans = producte.find(\'span\',class_=\'price-was-data\').get_text()\n        percentage = producte.find(\'span\',class_=\'price-save-percent\').get_text()\n    except:\n        preuAbans = "NA"\n        percentage = "NA"\n\n    #Agafem els costes de envio\n    costos_productes = producte.findAll("li",{"class":"price-ship"})\n    #Com que es tracta d\'un vector, agafem el primer element i el netegem.\n    costos = costos_productes[0].text.strip()  \n\n    #Writing the file\n    writing_csv.writerow({"Marca": marca, "Producte": name, "PreuActual": preuActual, "PreuAnterior": preuAbans,"CostEnvio":costos,"Rebaixa":percentage})\n\nresult_file.close()  \n'
'import pandas as pd\nimport re\n# reproducing table 1\ndf1 = pd.DataFrame({"Location": ["MAB", "MEB"],\n                    "Type" : ["Ant", "Ant"],\n                    "Supplier":["A","B"],\n                     "ID": ["A123","A123"],\n                    "Serial": ["456/56","456/56"]})\n# then table 2\ndf = pd.DataFrame({"Location": ["MAB+MEB", "MAB+MEB", "MAB+MEB"],\n                   "Type": ["Ant", "Ant", "Ant"],\n                   "Supplier": ["A/B", "A/B","A/B"],\n                   "ID": ["A123", "A123/B123", "A123/B123"],\n                   "Serial":[\'456/56\',\'456/56\',\'456/56\'],\n                   "values_rand":[1,2,3]})\n\n# First I split the column I am interested in based on regexp you can tweak according\n# to what you want:\nr = re.compile(r"[a-zA-Z0-9]+")\ndf[\'Supplier\'], df["ID"], df["Location"] = df[\'Supplier\'].str.findall(r),\\\n                                           df[\'ID\'].str.findall(r), \\\n                                           df[\'Location\'].str.findall(r)\ntable2 = pd.merge(df[\'Supplier\'].explode().reset_index(), \n                  df["ID"].explode().reset_index(),on="index", how="outer")\ntable2 = pd.merge(table2, df["Location"].explode().reset_index(), \n                  on="index", how="outer")\ntable2 = pd.merge(table2, df.loc[:,["Type","Serial",\n                                    "values_rand"]].reset_index(), on="index",how="left")\nresult = (pd.merge(table2,df1, on=[\'Location\' , \'Supplier\' , \'ID\' , \'Serial\',"Type"])\n         .drop(columns="index"))\n\n  Supplier    ID Location Type  Serial  values_rand\n0        A  A123      MAB  Ant  456/56            1\n1        A  A123      MAB  Ant  456/56            2\n2        A  A123      MAB  Ant  456/56            3\n3        B  A123      MEB  Ant  456/56            1\n4        B  A123      MEB  Ant  456/56            2\n5        B  A123      MEB  Ant  456/56            3\n'
"cols = ['Size','Installs']\ndf[cols] = df[cols].replace('[^\\d.]', '', regex = True).replace('',np.nan).astype(float)\n\nprint (df)\n       Rating  Reviews  Size    Installs  Type  Price\n0         4.1      159  19.0     10000.0  Free      0\n1         3.9      967  14.0    500000.0  Free      0\n2         4.7    87510   8.7   5000000.0  Free      0\n3         4.5   215644  25.0  50000000.0  Free      0\n4         4.3      967   2.8    100000.0  Free      0\n10836     4.5       38  53.0      5000.0  Free      0\n10837     5.0        4   3.6       100.0  Free      0\n10838     0.0        3   9.5      1000.0  Free      0\n10839     4.5      114   NaN      1000.0  Free      0\n10840     4.5   398307  19.0  10000000.0  Free      0\n"
'import pandas as pd\nimport numpy as np\nl1 = [12, 134, 23, np.nan, np.nan, 324, np.nan,np.nan,np.nan,np.nan]\nl2 = ["Apple","Vegies","Oranges","Apples","Vegies","Sugar","Apples","Melon","Melon","Grapes"]\ndf = pd.DataFrame(l1, columns=["col1"])\ndf["col2"] = pd.DataFrame(l2)\n\ndf\nOut[26]: \n    col1     col2\n0   12.0    Apple\n1  134.0   Vegies\n2   23.0  Oranges\n3    NaN   Apples\n4    NaN   Vegies\n5  324.0    Sugar\n6    NaN   Apples\n7    NaN    Melon\n8    NaN    Melon\n9    NaN   Grapes\n\ndf.loc[df.col2 == "Vegies", \'col1\'] = 134\ndf.loc[df.col2 == "Apple", \'col1\'] = 12\n\nitem_dict = {"Apples":12, "Melon":65, "Vegies":134, "Grapes":78}\n\n def item_mapping(df, dictionary, colsource, coltarget):\n    dict_keys = list(dictionary.keys())\n    dict_values = list(dictionary.values())\n    for x in range(len(dict_keys)):\n        df.loc[df[colsource]==dict_keys[x], coltarget] = dict_values[x]\n    return(df)\n\nitem_mapping(df, item_dict, "col2", "col1")\n    col1     col2\n0   12.0    Apple\n1  134.0   Vegies\n2   23.0  Oranges\n3   12.0   Apples\n4  134.0   Vegies\n5  324.0    Sugar\n6   12.0   Apples\n7   65.0    Melon\n8   65.0    Melon\n9   78.0   Grapes\n'
"import re\nt = [re.findall(r'\\d+', item[0])[0] for item in a if re.findall(r'\\d+', item[0])]\n\n['23', '42']\n\nt = [re.findall(r'\\d+', item[0])[0] if re.findall(r'\\d+', item[0]) else '' for item in a]\n\n['23', '', '42', '', '']\n\nt = [int(re.findall(r'\\d+', item[0])[0]) if re.findall(r'\\d+', item[0]) else '' for item in a]\n\n[23, '', 42, '', '']\n"
'df = da[~((no_charges &lt; (Q1 - 1.5 * IQR)) |(no_charges &gt; (Q3 + 1.5 * IQR))).any(axis=1)]\n'
'# Cleaning function  \ndef cleaner(df, dict):\n    for col in df.columns:\n        if col in dict.keys(): \n            for row in df.index:\n                if type(re.match(dict[col], str(df[col][row]))) is re.Match:\n                    df[col][row] = df[col][row] \n                    print(df[col][row])\n                else:\n                    df[col][row] = np.nan\n    return(df)\nprint(cleaner(df1, dict1))\ncleaned_df = cleaner(df1, dict1)\n'
"df2['Market_Category'].value_counts() \n"
"df['Revenue'] = df['Revenue'].replace({'\\$':'', ',':''}, regex=True)\ndf['Revenue'] = df['Revenue'].astype(float)\ndf_mean = df.groupby(['Industry'], as_index = False)['Revenue'].mean()\n\ndf_mean\n    Industry    Revenue\n0   Construction    4.358071e+06\n1   Financial Services  8.858420e+06\n2   IT Services 1.175702e+07\n\ndf_mean_nan = df.groupby(['Industry'], as_index = False)['Revenue'].agg({'Sum':np.sum, 'Size':np.size})\ndf_mean_nan['Mean_nan'] = df_mean_nan['Sum'] / df_mean_nan['Size']\n\ndf_mean_nan\n\n    Industry    Sum Size    Mean_nan\n0   Construction    13074212.0  5.0 2614842.4\n1   Financial Services  17716840.0  2.0 8858420.0\n2   IT Services 11757018.0  1.0 11757018.0\n\ndf.loc[df['Revenue'].isna(),['Revenue']] = df_mean_nan.loc[df_mean_nan['Industry'] == 'Construction',['Mean_nan']].values\n\ndf\n    ID  Name    Industry    Year    Revenue\n0   1   Treslam Financial Services  2009    5387469.0\n1   2   Rednimdox   Construction    2013    2614842.4\n2   3   Lamtone IT Services 2009    11757018.0\n3   4   Stripfind   Financial Services  2010    12329371.0\n4   5   Openjocon   Construction    2013    4273207.0\n5   6   Villadox    Construction    2012    1097353.0\n6   7   Sumzoomit   Construction    2010    7703652.0\n7   8   Abcddd  Construction    2019    2614842.4\n\ndf.loc[df['Revenue'].isna(),['Revenue']] = df_mean.loc[df_mean['Industry'] == 'Construction',['Revenue']].values\n\ndf\n    ID  Name    Industry    Year    Revenue\n0   1   Treslam Financial Services  2009    5.387469e+06\n1   2   Rednimdox   Construction    2013    4.358071e+06\n2   3   Lamtone IT Services 2009    1.175702e+07\n3   4   Stripfind   Financial Services  2010    1.232937e+07\n4   5   Openjocon   Construction    2013    4.273207e+06\n5   6   Villadox    Construction    2012    1.097353e+06\n6   7   Sumzoomit   Construction    2010    7.703652e+06\n7   8   Abcddd  Construction    2019    4.358071e+06\n"
"mode = df.groupby('Industry').Year\nmode = mode.transform(lambda x: x.mode().squeeze())\ndf.update(mode, overwrite=False)\n\nmode = df.groupby('Industry').Year.apply(lambda x: x.mode().squeeze())\n\n&gt;&gt;&gt; print(mode.apply(type))\n\nIndustry\nGovernment                 &lt;class 'numpy.float64'&gt;\nHealth                     &lt;class 'numpy.float64'&gt;\nIT Services                &lt;class 'numpy.float64'&gt;\nSoftware       &lt;class 'pandas.core.series.Series'&gt;\nName: Year, dtype: object\n\n&gt;&gt;&gt; mode = df.groupby('Industry').Year.apply(lambda x: x.mode().iloc[0])\nTraceback ...\nIndexError: single positional indexer is out-of-bounds\n\nmode = mode.explode()\n\n&gt;&gt;&gt; print(mode)\nIndustry\nGovernment     2012\nHealth         2008\nIT Services    2015\nSoftware        NaN\nName: Year, dtype: object\n\ndf = df.merge(mode, on='Industry') # default suffixes: ['_x', '_y']\nmode = df.pop('Year_y').rename('Year')\ndf.rename({'Year_x': 'Year'}, axis=1, inplace=True)\ndf.update(mode, overwrite=False)\n\n   ID      Name     Industry  Year  Employees   Expenses    Profit\n0   1     E-Zim       Health  2019        320  1,130,700   8553827\n1   4     Latho       Health  2008        103  4,631,808  10727561\n2   6    Quozap       Health  2008         21  4,626,275   8179177\n3   7  Tampware       Health  2008         13  2,127,984   3259485\n4   2  Daltfase     Software   NaN         78    804,035  13212508\n5   3   Hotlane   Government  2012         87  1,044,375   8701897\n6   5    Lambam  IT Services  2015        210  4,374,841   4193069\n"
"  table = [\n      ['1_john', 23, 'LoNDon_paris'],\n      ['2_bob', 34, 'Madrid_milan'],\n      ['3_abdellah', 26, 'Paris_Stockhom']\n  ]\n  df = pd.DataFrame(table, columns=['ID_Name', 'Score', 'From_to'])\n  df[['ID','Name']] = df.apply(lambda x: get_first_last(x['ID_Name']), axis=1, result_type='expand')\n"
'df=df.where(df.astype(bool),0)\n\ndf\nOut[26]: \n   a  b\n1  0  3\n'
"tmp = df[df['Season'].isnull()]\ntmp2 = df[~df['Season'].isnull()]\nnew_index = np.arange(0, df.index.max()+1)\nheader1 = tmp.reindex(new_index)\nheader1 = header1['State/Crop/District'].fillna(method='ffill')\nfinal = pd.concat([header1, tmp2], axis=1)\nfinal.dropna(axis=0, inplace=True)\nfinal['State'] = df['State/Crop/District'][0]\nfinal.columns = ['Crop', 'District', 'Season','Area (Hectare)', 'Production (Tonnes)', 'Yield (Tonnes/Hectare)','State']\nfinal = final[['State','Crop', 'District', 'Season','Area (Hectare)', 'Production (Tonnes)', 'Yield (Tonnes/Hectare)']]\nfinal['District'] = final['District'].str.replace(r'^\\d\\.','')\n\nfinal\n|    | State                       | Crop         | District                 | Season   |   Area (Hectare) |   Production (Tonnes) |   Yield (Tonnes/Hectare) |\n|---:|:----------------------------|:-------------|:-------------------------|:---------|-----------------:|----------------------:|-------------------------:|\n|  2 | Andaman and Nicobar Islands | Arecanut     | NICOBARS                 | Rabi     |           534.1  |                125.23 |                 0.234469 |\n|  3 | Andaman and Nicobar Islands | Arecanut     | NORTH AND MIDDLE ANDAMAN | Rabi     |          1744    |               4639.44 |                 2.66023  |\n|  4 | Andaman and Nicobar Islands | Arecanut     | SOUTH ANDAMANS           | Rabi     |          1220.2  |              10518.7  |                 8.62047  |\n|  7 | Andaman and Nicobar Islands | Arhar/Tur    | NORTH AND MIDDLE ANDAMAN | Rabi     |             1.2  |                  0.6  |                 0.5      |\n| 10 | Andaman and Nicobar Islands | Black pepper | NICOBARS                 | Rabi     |            12.4  |                  0.42 |                 0.033871 |\n| 11 | Andaman and Nicobar Islands | Black pepper | NORTH AND MIDDLE ANDAMAN | Rabi     |             8.76 |                  2.13 |                 0.243151 |\n| 12 | Andaman and Nicobar Islands | Black pepper | SOUTH ANDAMANS           | Rabi     |            69.46 |                349.72 |                 5.03484  |\n"
"max_df = df.groupby('RMD').max()\n"
"regex = r'(?P&lt;beds&gt;\\d)\\sbeds?\\s(?P&lt;bath&gt;\\d+)\\sbaths?\\s?(?P&lt;parking&gt;\\d)?'\ndf = pd.concat([df, df['Amenities'].str.extract(regex)], axis=1)\n\n                       Amenities beds bath parking\n0                  3 beds 1 bath    3    1     NaN\n1         1 bed 1 bath 1 parking    1    1       1\n2                  3 beds 1 bath    3    1     NaN\n3       2 beds 2 baths 2 parking    2    2       2\n4        3 beds 1 bath 2 parking    3    1       2\n5  3 beds 2 baths 1 parking 419m    3    2       1\n6        4 beds 1 bath 2 parking    4    1       2\n7       3 beds 2 baths 2 parking    3    2       2\n8       2 beds 2 baths 1 parking    2    2       1\n9  3 beds 2 baths 1 parking 590m    3    2       1\n"
"DOB_Permits[&quot;job_start_date&quot;] = pd.to_datetime(DOB_Permits[&quot;job_start_date&quot;], errors='coerce', format=&quot;%y%m%d&quot;)\n"
"sr = sr.str.replace('`', '').astype('float').astype('Int32')\n\n0       1\n1       2\n2       3\n3       4\n4    &lt;NA&gt;\n5    &lt;NA&gt;\n6    &lt;NA&gt;\ndtype: Int32\n"
's = &quot;00353541635351651651&quot;\n\ndef remove_prefix(string):\n    starters = [&quot;353&quot;, &quot;00353&quot;, &quot;0353&quot;, &quot;00&quot;]\n    for start in starters:\n        if string.startswith(start):\n            return string.replace(start, &quot;&quot;)\n\nprint(remove_prefix(s))\n'
'import numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.signal import find_peaks\nfrom scipy.signal import medfilt\ndata = np.sin(np.linspace(0, 8*np.pi))\nindices = find_peaks(data)[0]\nindices = np.unique(np.concatenate([[0, data.size-1], indices]))\nfor i in range(len(indices) - 1):\n  i0, i1 = indices[i: i+2]\n  plt.plot(np.arange(i0, i1 + 1), data[i0:i1 + 1])\n'
"s = pd.Series(['40,910,27', '3,479.29', '34,561.09', '132,634,98'], dtype='string')\nres = s.str.replace(',(\\d+)$', r'.\\1', regex=True)\nprint(res)\n\n0     40,910.27\n1      3,479.29\n2     34,561.09\n3    132,634.98\ndtype: string\n"
"df[~pd.to_datetime(df.index, errors='coerce').isnull()]\n\n                                       _id      id\nupdated     \n2021-01-14 20:12:01 6000a60cc299a51c09e20626    19\n2021-01-14 20:12:01 6000a60cc299a51c09e20627    21\n2021-01-14 20:12:01 6000a60cc299a51c09e20628    11\n2021-01-14 20:12:01 6000a60cc299a51c09e20629    16\n2021-01-14 20:12:01 6000a60cc299a51c09e2062a    14\n\ndf['_id'].apply(type)\n\npd.Series(df.index.values).apply(type)\n"
'from tkinter import *\nfrom tkinter import messagebox\n\nroot = Tk(className=&quot;button_click_label&quot;)\nroot.geometry(&quot;200x200&quot;)\n\nmessagebox.showinfo(&quot;Success&quot;,&quot;Test&quot;)\nemptyCells = (df[df.columns] == &quot; &quot;).sum()\nl1 = Label(root, text=&quot;Emptycells?&quot;)\n\ndef clickevent():\n    txt = &quot;there are&quot;, emptyCells\n    l1.config(text=txt)\n\n\nb1 = Button(root, text=&quot;clickhere&quot;, command=clickevent).pack()\n\nl1.pack()\n\nroot.mainloop()\n'
"df = pd.DataFrame({'date':pd.date_range('2019-03-19', periods=8),\n                 'Booking':[92,109,144,109,122,76,78, 3]}) \ndf.loc[7, 'date'] = pd.to_datetime('2020-03-22')\n\ndf = df.set_index('date')\n\ns = df['Booking'].asfreq('d').rolling(7, center=True).mean()\ns.index = s.index + pd.offsets.DateOffset(years=1)\ns = s.mean(level=0)\nprint (s)\ndate\n2020-03-19           NaN\n2020-03-20           NaN\n2020-03-21           NaN\n2020-03-22    104.285714\n2020-03-23           NaN\n   \n2021-03-18           NaN\n2021-03-19           NaN\n2021-03-20           NaN\n2021-03-21           NaN\n2021-03-22           NaN\nName: Booking, Length: 369, dtype: float64\n\nmask = df['Booking'].lt(25)\n\ndf.loc[mask, 'Booking'] = s\nprint (df)\n               Booking\ndate                  \n2019-03-19   92.000000\n2019-03-20  109.000000\n2019-03-21  144.000000\n2019-03-22  109.000000\n2019-03-23  122.000000\n2019-03-24   76.000000\n2019-03-25   78.000000\n2020-03-22  104.285714\n"
"test = [w[0] for w in dictSentCheck(sentCheck)]\n\ndef split_on_angle_brackets(words):\n    para = []\n    bracket_stack = 0\n    for word in words:\n        if bracket_stack:\n            if word == 'gt':\n                bracket_stack -= 1\n            elif word == 'lt':\n                bracket_stack += 1\n        else:\n            if word == 'lt':\n                if len(para) &gt;= 7:\n                    yield ' '.join(para)\n                para = []\n                bracket_stack = 1\n            else:\n                para.append(word)\n    if para:\n        yield ' '.join(para)\n\nprint('\\n'.join(split_on_angle_brackets(test)))\n\nEnglish cricket cuts ties with Zimbabwe Wednesday June text\nprint EMAIL THIS ARTICLE your name your email address recipient's name recipient's email address\nadd another recipient your comment Send Mail\nThe England and Wales Cricket Board ECB announced it was suspending all ties with Zimbabwe and was cancelling Zimbabwe's tour of England next year\n"
'for i in range(len(kk)):\n    df_dt=datetime.datetime.strptime(kk[i],&quot;%y%m%d%H&quot;)\n    print(df_dt)\n\n2014-10-21 01:00:00\n2014-10-21 02:00:00\n2014-10-21 03:00:00\n2014-10-21 03:00:00\n\n1000 loops, best of 3: 91.6 s per loop\n1000 loops, best of 3: 345 s per loop\n'
"df = pd.DataFrame({\n    1: {'agree': 1}, \n    2: {'disagree': 1}, \n    3: {'whatevs': 1}, \n    4: {'whatevs': 1}}).transpose()\ndf\n\nquestion_sets = {\n    'set_1': ['disagree', 'whatevs', 'agree'], # define these lists from 1 to whatever\n}\n\nfor setname, setcols in question_sets.items():\n    # plug the NaNs with 0\n    df[setcols].fillna(0)\n\n    # scale each column with 0 or 1 in the question set with an ascending value\n    for val, col in enumerate(setcols, start=1):\n        df[col] *= val\n\n    # create new column by summing all the question set columns\n    df[setname] = df[question_set_columns].sum(axis=1)\n\n    # delete all the old columns\n    df.drop(setcols, inplace=True, axis=1) \n\ndf\n"
'# [...] Same code here\n\nlinks_list = []\nfor link in soup.findAll(\'a\', href=re.compile(\'/d/part/of/thelink/ineed.aspx\')):\n    print link.get(\'href\')\n    complete_links = \'http://www.sitetoscrape.ch\' + link.get(\'href\')\n    print complete_links\n    link_list.append(complete_links)  # append new link to the list\n\nfor link in link_list:\n    page = requests.get(link)\n    tree = html.fromstring(page.text)\n\n    #Details\n    name = tree.xpath(\'//dl[@class="services"]\')\n\n    for i in name:\n        print i.text_content()\n'
"replacements = {'Geboren am':';', 'Nato/a il':';', 'N(e) le':';'}\n\nlines = []\nwith open('DATEN2.txt') as infile, open('DATENBEARBEITET2.txt', 'w') as outfile:\n    for line in infile:\n        line = line.strip()\n        if not line:\n            continue\n        for src, target in replacements.iteritems():\n            line = line.replace(src, target)\n        lines.append(line)\n    outfile.write(', '.join(lines))\n"
'\',\'.join(y for y in re.split("[- ,!~?]", x) if y)\n                                ^^\n                                ||\n                    List all the symbols here\n\ny = ["- memphis , tn! ", "~~~memphis,tn", ":) memphis , tn (:", ". - memphis,tn - .", "memphis tn?", ". - memphis,tn - .", "- . memphis,tn . -"]\n\nfor x in y:\n    print(\',\'.join(y for y in re.split("[- ,!~?:;)(.]", x) if y))\n\nprint(\',\'.join(y for y in re.split("_|[^\\w]", x) if y))\n'
"import uuid\n\nin_filename = 'test.txt'\nout_filename = 'parsed.txt'\n\nwith open(in_filename) as f_in:\n    with open(out_filename, 'w') as f_out:\n        for line in f_in:\n            try:\n                # Retrieve the line and split into UUID and data\n                line_uuid, data = line.split('\\t', maxsplit=1)\n                # Validate UUID\n                uuid.UUID(line_uuid)\n            except ValueError:\n                # Ignore this line\n                continue\n            # Write each individual piece of data to a separate line\n            for data_part in data.rstrip().split('\\x0b'):\n                f_out.write(line_uuid + '\\t' + data_part  + '\\n')\n"
'all_tokens = []\nfor i in range (0, len(comments)):\n\n        tokens = tokenizer.tokenize(no_html.lower())\n        #print tokens\n        all_tokens.extend(tokens)\n\nprint all_tokens\n'
"initial_str = 'Some text  and some more text'\nclean_str = ''.join([c for c in initial_str if ord(c) &lt; 128])\nprint(clean_str)  # Some text  and some more text\n"
"testStrings = ['1 win &amp; 1 nomination.','2 wins.','5 nominations.', '3 wins &amp; 8 nominations.', '2 wins.','9 wins.']\n\ntext = [i.split('&amp;') for i in testStrings]\n\ndata=[]\nfor row in text:\n    for t in row:\n        winIndex = t.find('win')\n        nomIndex = t.find('nom')\n        if winIndex&gt;0:\n            w=int(t[:winIndex-1] )\n        else:\n            w=0\n        if nomIndex&gt;0:\n            n=int(t[:nomIndex-1] )\n        else:\n            n=0\n    data.append([w,n])\n"
"cols_sel = df.select([func.first(col).alias(col) for col in df.columns]).collect()[0].asDict()\ncols = [col_name for (col_name, v) in cols_sel.items() if v in ['Y', 'N']]\n# return ['HT2', 'ALIVE', 'DBP', 'HT1', 'PREV', 'DBG']\n\ndef map_input(val):\n    map_dict = dict(zip(['Y', 'N'], [1, 0]))\n    return map_dict.get(val)\nudf_map_input = func.udf(map_input, returnType=typ.IntegerType())\n\nfor col in cols:\n    df = df.withColumn(col, udf_map_input(col))\ndf.show()\n\nout = df.select([func.sum(col).alias(col) for col in cols]).collect()\nout = out[0]\nprint([col_name for (col_name, val) in out.asDict().items() if val &gt; 0])\n\n['DBG', 'HT2', 'ALIVE', 'PREV']\n"
'import re\ng = re.findall("\\d{2}\\.\\d{2}\\.\\d{4}", value)\nreturn g\n'
"df.columns = df.columns.str.replace('\\s*&lt;.*&gt;\\s*$', '')\n"
"with open('Universities.txt', 'r') as f:\n\n    #Read non-empty lines:\n    data = (line.rstrip() for line in f)\n    lines = list(line for line in data if line)\n\n    #Get the index of states:\n    r_idx = [lines.index(line) for line in lines if '[edit]' in line]\n\n    #Separating states and university names using wrapping indexes:\n    university = []\n    region = [lines[i].replace('[edit]', '') for i in r_idx]\n    for i in range(len(r_idx)):\n        if i != len(r_idx)-1:\n            sub = lines[r_idx[i]+1:r_idx[i+1]]\n            university.append(sub)\n        else:\n            sub = lines[r_idx[i]+1:]\n            university.append(sub)\n\n\n    #Create dict:\n    uni = dict(zip(region, university))\n"
'rdict = {\n    "Truncating Mutations":"9999", \n    "Amplification":"9999",\n    "Fusion":""\n    }\n\ndf[0] = df[0].replace(rdict)\n'
"pd.to_datetime(df['Text'].str.extract(\n    r'(?P&lt;Date&gt;\\d+(?:\\/\\d+){2})', expand=False), errors='coerce')\n\n0    1993-03-25\n1    1985-06-18\n2    1971-07-08\n3    1975-09-27\n4    1996-02-06\n5    1979-07-06\n6    1978-05-18\n7    1989-10-24\n8    1986-03-07\n9    1971-04-10\n10   1985-05-11\n11   1975-04-09\n12   1998-08-01\n13   1972-01-26\n14   1990-05-24\n15   2011-01-25\nName: Date, dtype: datetime64[ns]\n\narray(['03/25/93', '6/18/85', '7/8/71', '9/27/75', '2/6/96', '7/06/79',\n       '5/18/78', '10/24/89', '3/7/86', '4/10/71', '5/11/85', '4/09/75',\n       '8/01/98', '1/26/72', '5/24/1990', '1/25/2011'], dtype=object)\n\n(?P&lt;Date&gt;\\d+(?:\\/\\d+){2})\n\n(?P&lt;Date&gt;(?:\\d+\\/)?\\d+/\\d+)\n"
"x_2016['W_index'] = x_2016['index'].str.extract('(W\\d\\d)', expand=True)\n\ny_2017['W_index'] = y_2017['index'].str.extract('(W\\d\\d)', expand=True)\n\npd.merge(\n    left=x_2016, \n    right=y_2017, \n    how='outer', \n    on='W_index', \n    suffixes=('_2016', '_2017'))[\n        ['W_index', '%_2016', '%_2017']\n    ].fillna(0).sort_values('W_index').reset_index(drop=True)\n\n# returns:\n   W_index  %_2016  %_2017\n0      W11     0.0    40.0\n1      W12     2.5    19.0\n2      W13    43.0     6.0\n3      W14    63.0    17.0\n4      W15     1.0     0.0\n"
"df = pd.DataFrame({'SUMMARY' : ['hello, world!', 'XXXXX test', '123four, five:; six...']})\n\ndf\n\n                  SUMMARY\n0           hello, world!\n1              XXXXX test\n2  123four, five:; six...\n\ndf.SUMMARY.str.replace(r'[^a-zA-Z\\s]+|X{2,}', '')\n\n0      hello world\n1             test\n2    four five six\nName: SUMMARY, dtype: object\n\ndf.SUMMARY = df.SUMMARY.str.replace(r'[^a-zA-Z\\s]+|X{2,}', '')\\\n                       .str.replace(r'\\s{2,}', ' ')\n"
"df.replace(dict(Y=True, N=False)) \\\n  .groupby('id').any() \\\n  .replace({True: 'Y', False: 'N'})\n\n       col1 col2 col3\nid                   \nENE80R    Y    N    Y\n\ndf.set_index('id').eq('Y').any(level=0).replace({True: 'Y', False: 'N'})\n\n       col1 col2 col3\nid                   \nENE80R    Y    N    Y\n"
' def pattern_matcher(y) :\n    if y.count(\';\')&lt;1 or y ==\';\':\n        #case of the string doesn\'t contain any \';\'\n        return True\n    else :\n        """\n        this will return True if it contain only \';\' without any empty word preceding it , using \n        strip to check if it is only \';\'\n        """\n        return False #all([not x.strip() for x in y.split(";")])\n\nout2 = df2.loc[(df2.part.apply(pattern_matcher))]\n\n\npart\n2   ;\n3   250\n\nout1 = df2.loc[~(df2.part.apply(pattern_matcher))]\n\n    part\n0   A;B;C\n1   ip;KL;JH\n'
"import pandas as pd\nfrom bs4 import BeautifulSoup as bs\nsoup = bs(table, 'html.parser')\ndf = pd.DataFrame() # you can add index and column details at this point too\nrow_index = -1\nfor row in soup.find_all('tr'):\n    if row.find('td').find('hr'): # few rows has a horizontal line; skipping them\n        continue\n    if len(row.find_all('td')) == 1: # skipping the row stating Record : 1 of ...\n    #if row.find_all('td')[0].get_text().startswith('Record :'):\n        row_index += 1\n        continue\n    tds = [td.get_text().strip() for td in row.find_all('td')]\n    df.at[row_index, tds[0]] = tds[2]\n    if len(tds) &gt; 3: #few rows have multiple tds; might have to make this dynamic if its more than 2 fields per row\n        df.at[row_index, tds[3]] = tds[5]\n"
"&gt;&gt;&gt; def hashtags(tweet):\n....    return list(filter(lambda token: token.startswith('#'), tweet.split()))\n\n&gt;&gt;&gt; [hashtags(tweet) for tweet in tweets]\n[['#ugh', '#yikes'], [], ['#hooray']]\n"
'df.region =  df.dep.apply(lambda x: dep_dict[x])\n'
"txt = text.translate(str.maketrans(string.punctuation, ' ' * len(string.punctuation))).replace(' '*4, ' ').replace(' '*3, ' ').replace(' '*2, ' ').strip()\n"
"goals_when_away = gameStats.groupby(['awayTeam'])['awayGoals', 'homeGoals'].agg('sum').reset_index().sort_values('awayTeam')\ngoals_when_home = gameStats.groupby(['homeTeam'])['homeGoals', 'awayGoals'].agg('sum').reset_index().sort_values('homeTeam')\n\nnp_result = goals_when_away.iloc[:, 1:].values + goals_when_home.iloc[:, 1:].values\npd_result = pd.DataFrame(np_result, columns=['goal_for', 'goal_against'])\nresult = pd.concat([goals_when_away.iloc[:, :1], pd_result], axis=1, ignore_index=True)\n"
'import re\n\n.\n.\n.\n\nndata = data[data[\'Tweets\'].str.contains("|".join(my_list), regex=True,                          \n                                         flags=re.IGNORECASE)].reset_index(drop=True)\n#                                        ^^^^^^^^^^^^^^^^^^^\n'
"def myFloat(str):\n    if str:\n       return float(str)\n    return &lt;your_default_value&gt; #e.g -1\n\nlines = [[myFloat(x) for x in line.strip().split(',')] for line in myfile.readlines()[1:3000]]\n\nlines = [[float(x) for x in line.strip().split(',') if x] for line in myfile.readlines()[1:3000]]\n"
'import string\ntest=[" course content good though textbook badly written.not got energy though seems good union.it distance course i\'m sure facilities.n/an/ain last year offer poor terms academic personal. seems un become overwhelmed trying become run"]\ntest2 = \'\'.join(w for w in test[0] if w not in string.punctuation )\nprint(test2)\n\nimport string\ntest=["Hi There!"," course content good though textbook badly written.not got energy though seems good union.it distance course i\'m sure facilities.n/an/ain last year offer poor terms academic personal. seems un become overwhelmed trying become run"]\n#if there are multiple string in the list\nfor x in test:\n    print(\'\'.join(w for w in x if w not in string.punctuation ))\n# If there are multiple strings in the list and you want to join all of them togather\nprint(\'\'.join(w for w in [x for x in test] if w not in string.punctuation )) \n\nimport string\ntest2=[]\ntest=["Hi There!"," course content good though textbook badly written.not got energy though seems good union.it distance course i\'m sure facilities.n/an/ain last year offer poor terms academic personal. seems un become overwhelmed trying become run"]\n#if there are multiple string in the list\nfor x in test:\n    test2.append(\'\'.join(w for w in x if w not in string.punctuation ))\nprint(test2)\n'
"import random\nimport pandas as pd\n\ndf = pd.DataFrame()\nfor month in range(1,5): #First 4 Months\n    for day in range(5,10): #5 Days\n        hour = random.randint(18,19)\n        minute = random.randint(1,59)\n        dt = datetime.datetime(2018,month,day,hour,minute,0)\n        dti = pd.date_range(dt, periods=60*60*4, freq='S')\n        values = [random.randrange(1, 101, 1) for _ in range(len(dti))]\n        df = df.append(pd.DataFrame(values, index=dti, columns=['Value']))\n\ndef first_value_per_day(df):\n    res_df = df.groupby(df.index.date).apply(lambda x: x.iloc[[0]])\n    res_df.index = res_df.index.droplevel(0)\n    return res_df\n\nprint(first_value_per_day(df))\n\n                     Value\n2018-01-05 18:31:00     85\n2018-01-06 18:25:00     40\n2018-01-07 19:54:00     52\n2018-01-08 18:23:00     46\n2018-01-09 18:08:00     51\n2018-02-05 18:58:00      6\n2018-02-06 19:12:00     16\n2018-02-07 18:18:00     10\n2018-02-08 18:32:00     50\n2018-02-09 18:38:00     69\n2018-03-05 19:54:00    100\n2018-03-06 18:37:00     70\n2018-03-07 18:58:00     26\n2018-03-08 18:28:00     30\n2018-03-09 18:34:00     71\n2018-04-05 18:54:00      2\n2018-04-06 19:16:00    100\n2018-04-07 18:52:00     85\n2018-04-08 19:08:00     66\n2018-04-09 18:11:00     22\n\nMONTHS_TO_MODIFY = [2,3]\nHOURS_TO_DROP = 1\n\nfvpd = first_value_per_day(df)\nfor m in MONTHS_TO_MODIFY:\n    fvpdm = fvpd[fvpd.index.month == m]\n    for idx, value in fvpdm.iterrows():\n        start_dt = idx\n        end_dt = idx + datetime.timedelta(hours=HOURS_TO_DROP)\n        index_list = df[(df.index &gt;= start_dt) &amp; (df.index &lt; end_dt)].index.tolist()\n        df.drop(index_list, inplace=True)\n\nprint(first_value_per_day(df))\n\n                     Value\n2018-01-05 18:31:00     85\n2018-01-06 18:25:00     40\n2018-01-07 19:54:00     52\n2018-01-08 18:23:00     46\n2018-01-09 18:08:00     51\n2018-02-05 19:58:00      1\n2018-02-06 20:12:00     42\n2018-02-07 19:18:00     34\n2018-02-08 19:32:00     34\n2018-02-09 19:38:00     61\n2018-03-05 20:54:00     15\n2018-03-06 19:37:00     88\n2018-03-07 19:58:00     36\n2018-03-08 19:28:00     38\n2018-03-09 19:34:00     42\n2018-04-05 18:54:00      2\n2018-04-06 19:16:00    100\n2018-04-07 18:52:00     85\n2018-04-08 19:08:00     66\n2018-04-09 18:11:00     22\n\n"
"import datetime\ndatetime.timedelta(days = 0.013391204)\nstr(datetime.timedelta(days = 0.013391204))\n\nOutput:'0:19:17.000026'\n"
"df = pd.DataFrame([['2013-01',     'Total',      'Total',      'Air',      984350],\n['2013-01',     'Total',      'Total',      'Sea',      129074],\n['2013-01',     'Total',      'Total',      'Land',     178294],\n['2013-02',     'Total',      'Total',     'Air',      916372],\n['2015-12',    'AMERICAS',     'USA',       'Land',      2698],\n['2015-12',    'AMERICAS',    'Canada',     'Land',       924],\n['2013-01',     'ASIA',        'China',     'Air',      136643],\n['2013-01',     'ASIA',        'India',     'Air',       55369],\n['2013-01',     'ASIA',        'Japan',     'Air',       51178]],\ncolumns = ['period', 'region', 'country', 'moa', 'arv_count'])\n\ndf['year'] = pd.to_datetime(df['period']).dt.year\n\ndf.groupby(['region', 'year', 'moa']).arv_count.sum()\n\nregion    year  moa \nAMERICAS  2015  Land       3622\nASIA      2013  Air      243190\nTotal     2013  Air     1900722\n                Land     178294\n                Sea      129074\n"
"hours = (df.index.to_series().dt.hour) # convert DateTimeIndex to hours\nmask = (hours &gt; 6)  &amp; (hours &lt; 18)\ndf.loc[~mask, 'Light'] = 0\n"
'    import numpy as np, pandas as pd\n    import string\n\n    # test data:\n    df=pd.DataFrame({"year":[1911,1923,1935,1911],"other_cols":["abc","def","ghi","jkl"]})\n  Out:\n           year other_cols\n    0  1911        abc\n    1  1923        def\n    2  1935        ghi\n    3  1911        jkl\n\n    #create the intervals:\n    cats=pd.cut(df.year,10)\n\n    Out: cats.dtypes.categories\n    IntervalIndex([(1910.976, 1913.4], (1913.4, 1915.8], (1915.8, 1918.2],...\n\n    # char generator:\n    gchar=(ch for ch in string.ascii_uppercase)\n    dlbls= { iv:next(gchar) for iv in cats.dtypes.categories } #EDIT1\n    # get the intervals and convert them to the specified strings:\n    df["year_gr"]=[ f"{dlbls[iv]} ({int(np.round(iv.left))} - {int(np.round(iv.right))})" for iv in cats ] #EDIT1\n   Out:\n          year other_cols          year_gr\n    0  1911        abc  A (1911 - 1913)\n    1  1923        def  B (1921 - 1923)\n    2  1935        ghi  C (1933 - 1935)\n    3  1911        jkl  A (1911 - 1913)\n\n    # align the columns:\n    df= df.reindex(["year_gr","year","other_cols"], axis=1)\n   Out:\n               year_gr  year other_cols\n    0  A (1911 - 1913)  1911        abc\n    1  B (1921 - 1923)  1923        def\n    2  C (1933 - 1935)  1935        ghi\n    3  A (1911 - 1913)  1911        jkl\n'
"import numpy as np \ndf['countries']=np.where(df['countries'].isin(developed),'developed','developing')\nprint(df)\n\n    countries  age  gender\n1  developing   21    Male\n2  developing   22  Female\n3   developed   23    Male\n4   developed   25    Male\n\nc=df['countries'].isin(developed)\ndf.loc[c,'countries']='developed'\ndf.loc[~c,'countries']='developing'\nprint(df)\n\n\n   countries  age  gender\n1  developing   21    Male\n2  developing   22  Female\n3   developed   23    Male\n4   developed   25    Male\n"
'import re\n\ndef parse_time(t):\n    t = t.strip()\n    hours = int(re.findall(\'^[0-9]+\', t)[0])\n    m = re.findall(\':([0-9]+)\', t)\n    if len(m) &gt; 0:\n        minutes = int(m[0])\n    else:\n        minutes = 0\n    afternoon = re.search(\'(pm)|(midnight)\', t)\n    if afternoon:\n        hours += 12\n    return (hours, minutes)\n\ndef get_parts(s):\n    x = re.split(\'|-\', s)\n    start, end = x[0].strip(), x[1].strip()\n    start_hours, start_minutes = parse_time(start)\n    end_hours, end_minutes = parse_time(end)\n    parts = []\n    if start_hours &lt; 11: # or whenever you think breakfast ends\n        parts.append("breakfast")\n    if 12 &lt; start_hours &lt; 15 or 12 &lt; end_hours &lt; 15:\n        parts.append("lunch")\n    if end_hours &gt; 17:\n        parts.append("dinner")\n    return parts\n\ndef get_all_parts(data):\n    x = [set(get_parts(s)) for s in data.split(",")]\n    return set.union(*x)\n\nprint(get_all_parts("10am-3:30pm"))\nprint(get_all_parts("11am - 3:30pm, 6:30pm - 12midnight"))\nprint(get_all_parts("10am - 11am, 5pm-7pm"))\n'
'def combine_with_nan(x, cols):\n    combined=\'\'\n    for column in cols:\n        try:\n            np.isnan(x[column])\n            Temp = \'\'\n        except:\n            Temp = x[column]\n        combined= combined + \' || \' + Temp\n\n    return combined \ncols=[\'Columns you want to merge\']\npracticedf = practicedf.apply(combine_with_nan, axis=1,args=(cols,)).to_frame().replace(r"\\\\n"," || ", regex=True)\n\n'
'customers=df["Customer"].unique().tolist()\nList=[]\n\nfor customer in customers: \n    List.append(df.loc[df["Customer"]==customer,"Contact"].tolist())\n\ndf=df.drop_duplicates("Customer",keep="first")\ndf["new"]=List\n\nOut[10]: \n     ID    Customer  ...        Contact                             new\n0  1234  Customer A  ...            NaN            [nan, nan, nan, nan]\n4  1233  Customer B  ...            NaN       [nan, nan, abc@email.com]\n7  1235  Customer C  ...  abc@email.com  [abc@email.com, abc@email.com]\n\n[3 rows x 6 columns]\n'
"df = pd.DataFrame({\n    'STATE/UT': ['TOTAL(STATES)', 'TOTAL(UTs)', 'Normal'],\n    'CRIME HEAD': ['RAPE', 'RAPE', 'RAPE']\n})\ndf[df['STATE/UT'].str.find('TOTAL') == -1]\n"
"df1\n        Job1\n0       20\n1       50\n2       70\n\njob = {'code': (list(range(17, 29)),\n                   list(range(47, 58)),\n                   list(range(67, 78))),\n       'true': ('Agriculture, Forestry &amp; Fisheries',\n                  'Mining',\n                  'Construction')}\n\npd_replace = pd.DataFrame(job).explode('code')\ndf1.replace(dict(zip(pd_replace['code'], pd_replace['true'])))\n\n                                Job1\n0  Agriculture, Forestry &amp; Fisheries\n1                             Mining\n2                       Construction\n"
"def get_last_as_first(grp):\n  return  pd.Series([grp.iloc[-1]] + [np.nan]*(grp.shape[0]-1))\n\ndf['Date'] = pd.to_datetime(df['Date']) \ndf = df.sort_values('Date')\nnew_col = df.groupby('Movie name')['Revenue accumulated'].transform(get_last_as_first)\n"
"df = pd.DataFrame(your_data)\ndf = pd.concat([df['time'], df['mid'].apply(pd.Series)], axis=1)\n\n                             time        o        h        l        c\n0  2020-01-17T19:15:00.000000000Z  1.10916  1.10917  1.10906  1.10912\n1  2020-01-17T19:30:00.000000000Z  1.10914  1.10922  1.10908  1.10919\n2  2020-01-17T19:45:00.000000000Z  1.10920  1.10946  1.10920  1.10930\n3  2020-01-17T20:00:00.000000000Z  1.10930  1.10931  1.10919  1.10928\n4  2020-01-17T20:15:00.000000000Z  1.10926  1.10934  1.10922  1.10926\n5  2020-01-17T20:30:00.000000000Z  1.10926  1.10928  1.10913  1.10920\n6  2020-01-17T20:45:00.000000000Z  1.10918  1.10929  1.10913  1.10928\n7  2020-01-17T21:00:00.000000000Z  1.10927  1.10929  1.10920  1.10924\n8  2020-01-17T21:15:00.000000000Z  1.10926  1.10926  1.10910  1.10912\n9  2020-01-17T21:30:00.000000000Z  1.10913  1.10918  1.10912  1.10913\n"
"df = df.explode('reference')\n\ndf = df['reference'].apply(pd.Series).merge(df, left_index=True, right_index=True, how ='outer')\n"
"df = df.join(df.pop('c').str.get_dummies(', ').astype(bool))\nprint (df)\n   a  b     In   Out\n0  0  1   True  True\n1  1  4  False  True\n2  2  7   True  True\n3  3  9   True  True\n"
'is_row_nan = df.isnull().all(1)\nis_two_row_nan = (is_row_nan &amp; is_row_nan.shift(1))\n\ndfs = [g for _, g in df.groupby(is_two_row_nan.cumsum())]\n\ndf = pd.DataFrame(np.random.choice((1, np.nan), (10, 2)))\n     0    1\n0  1.0  NaN\n1  NaN  1.0\n2  NaN  NaN\n3  NaN  NaN\n4  1.0  NaN\n5  NaN  NaN\n6  NaN  1.0\n7  1.0  NaN\n8  1.0  1.0\n9  NaN  1.0\n\ndfs[0]\n     0    1\n0  1.0  NaN\n1  NaN  1.0\n2  NaN  NaN\n\n\ndfs[1]\n\n     0    1\n3  NaN  NaN\n4  1.0  NaN\n5  NaN  NaN\n6  NaN  1.0\n7  1.0  NaN\n8  1.0  1.0\n9  NaN  1.0\n'
"import pandas as pd\nimport re\n\ndf = pd.DataFrame({'col':['Hello, world!', 'Warhammer 40,000', 'Codename 1,337']})\ndf['col'] = df['col'].apply(lambda x: re.sub(r'(\\d+),(\\d+)', r'\\1\\2', x))\n"
"#without restriction\npd.set_option('display.max_columns', None)\n"
'dummy = [[1 if x in [10, 15, 16, 17] else 0 for x in row] for row in arr]\n\ndummy_func = np.vectorize(lambda x: 1 if x in [10, 15, 16, 17] else 0)\ndummy = dummy_func(arr)\n\ndummy = np.vectorize(lambda x: 1 if x in [10, 15, 16, 17] else 0)(arr)\n'
"elements = ['Mystik Lifestyle (Save 34%)',\n'Chalets Chamarel (Adults Only)',\n'Andrea Lodge (Save 18%)',\n'Hibiscus Beach Resort &amp; Spa (Save 18%)',\n'Lagoon Attitude (Adults Only)',\n'Ocean V Hotel (Adults Only)']\n\nfor element in elements:\n    without_text_after_unwanted_character = element.split('(')[0] \n    # This will get everything before the '(' as we splitted\n    print(without_text_after_unwanted_character)\n\n# If you want to create a new list the new values, you can do:\nclean_list = [x.split('(')[0] for x in elements]\n\nfor clean_text in clean_list:\n    print(clean_text)\n"
'min_order = element.find_element_by_class_name(\'gallery-offer-minorder\').find_element_by_tag_name(\'span\').text.split(" ")[0]\n'
'airbnb.loc[(airbnb[\'host_name\'].isnull()) &amp; (airbnb["neighbourhood_group"]=="Bronx"), "host_name"] = "Vie"\nairbnb.loc[(airbnb[\'host_name\'].isnull()) &amp; (airbnb["neighbourhood_group"]=="Manhattan"), "host_name"] = "Sonder (NYC)"\nairbnb.loc[airbnb[\'host_name\'].isnull(), "host_name"] = "Michael"\n'
"from google.colab import files\ncompression_opts = dict(method='zip',archive_name='out.csv')  \nxyz.groupby('Country/Region').sum().to_csv('out.zip', index=True,compression=compression_opts)\nfiles.download('out.zip')\n"
"['brill building pop',\n 'quiet storm',\n 'ballad',\n 'easy listening',\n...\n 'american',\n '80s',\n '90s',\n 'ambient']\n"
"df.loc[(df['col1'].isna()) &amp; (df['col2'] == 0), 'col1'] = df['col2']\n\n   col1  col2\n0   0.0     0\n1  19.0     1\n2  32.0     0\n3   NaN     1\n4  54.0     1\n5  67.0     1\n"
"# operation a\ndf = df.asfreq(freq='D', fill_value=np.nan)\ndf['col'].fillna(method='ffill', inplace=True)\ndf['changed'].fillna('a', inplace=True)\n\n# operation b\ndf.loc[df['col'].diff()&lt;-3, ['col', 'changed']] = [np.nan, 'b']\ndf['col'].fillna(method='ffill', inplace=True)\n"
"df1.sort_values(by=['Subnum','Trials'], ascending=True)\ndf2..sort_values(by=['Subnum','Trials'], ascending=True)\npd.concat([df1,df2],axis=1)\n"
"df['Customer_Name'] = df['Customer_Name'].str.replace('.', ' ')\n"
"df_path = df_path[df_path.Price != 'Everyone']\n"
"df['Index_Number'] = np.repeat(np.arange(df.shape[0]/6), 6)[:df.shape[0]]+1\n"
"import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\n#url = 'https://www.cvbankas.lt/?padalinys%5B0%5D=76&amp;page=1'\n#soup = BeautifulSoup(requests.get(url).content, 'html.parser')\nall_data = []\nfor i in range(1, 9):\n    url = 'https://www.cvbankas.lt/?padalinys%5B0%5D=76&amp;page='+str(i)\n    print(url)\n    soup = BeautifulSoup(requests.get(url).content, 'html.parser')\n    for h3 in soup.select('h3.list_h3'):\n        try:\n            job_title = h3.get_text(strip=True)\n            company = h3.find_next(class_=&quot;heading_secondary&quot;).get_text(strip=True)\n            salary = h3.find_next(class_=&quot;salary_amount&quot;).get_text(strip=True)\n            location = h3.find_next(class_=&quot;list_city&quot;).get_text(strip=True)\n            print('{:&lt;50} {:&lt;15} {:&lt;15} {}'.format(company, salary, location, job_title))\n            all_data.append({\n                    'Job Title': job_title,\n                    'Company': company,\n                    'Salary': salary,\n                    'Location': location\n                })\n        except AttributeError:\n            pass\n            \n        \ndf = pd.DataFrame(all_data)\ndf.to_csv('data.csv')\n"
"import requests\nfrom bs4 import BeautifulSoup\ncountries = []\nvegetables = []\nremove = ['(', ')']\nr = requests.get('https://www.nrtcfresh.com/products/whole/vegetables-whole', headers={'User-agent': 'Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:61.0) Gecko/20100101 Firefox/61.0'})\nc = r.content\n\nsoup=BeautifulSoup(c,&quot;html.parser&quot;)\ntext = ''\nall = soup.select(&quot;div.col-sm-3.nrtc-p-10 h4&quot;)\n\n# Vegetables\nprint('Vegetables:\\n')\nfor vegetable in all:\n    print(vegetable.find(text=True, recursive=False).strip())\n    vegetables.append(vegetable.find(text=True, recursive=False).strip())\n\n\n# Countries:\nprint('\\n\\nCountries:\\n')\n\nfor span in all:\n    for t in span.find('span').get_text(strip=True):\n        if not t in remove:\n            text += t\n    print(text)\n    countries.append(text)\n    text= ''\n\n\n# Vegetables and Countries\nfor v, c in zip(vegetables, countries):\n    print(f'{v} - {c}')\n\nVegetables:\n\nTapioca\nTomatoes\nRosemary\nBeef Tomatoes\nRed Cherry Tomatoes\nRed Cherry Tomatoes (Vine)\nYellow Cherry Tomatoes\nPlum Tomatoes\nPlum Cherry Tomatoes\nVine Tomatoes\n....\n\nCountries:\n\nSrilanka\nTurkey\nKenya\nHolland\nNetherland\nNetherland\nNetherland\nNetherland\nHolland\nNetherland\n....\n\n\nTapioca - Srilanka\nTomatoes - Turkey\nRosemary - Kenya\nBeef Tomatoes - Holland\nRed Cherry Tomatoes - Netherland\nRed Cherry Tomatoes (Vine) - Netherland\nYellow Cherry Tomatoes - Netherland\nPlum Tomatoes - Netherland\nPlum Cherry Tomatoes - Holland\nVine Tomatoes - Netherland\nTurnip - Iran\nBaby Turnip - South Africa\nYams (Suran) - India\nGreen Baby Zucchini - South Africa\n....\n"
"df.groupby([pd.to_datetime(df.date).dt.strftime('%B'),df.State]).Murders.sum().reset_index()\n"
"s = pd.crosstab(df.morning,df.evening,normalize='index').stack()\nOut[84]: \nmorning  evening\nhigh     high       0.250000\n         low        0.250000\n         medium     0.500000\nlow      high       0.000000\n         low        0.000000\n         medium     1.000000\nmedium   high       0.333333\n         low        0.666667\n         medium     0.000000\ndtype: float64\n"
"# Get new_bill_no on the basis of [bill_no, date]\ndf1 = df[['bill_no', 'date']].drop_duplicates().reset_index()\ndf1.rename({'index': 'new_bill_no'}, axis=1, inplace=True)\n\n# On Merging you will get new_bill_no in original df\ndf = df.merge(df1, on=['bill_no', 'date'], how='left'])\n"
"import pandas as pd\n\ndf = pd.DataFrame([{'Time':'2020-01-01 21:15:00', 'LOCA':3,'LOCB':0},\n                   {'Time':'2020-01-01 21:00:00', 'LOCA':4, 'LOCB':10},\n                   {'Time':'2020-01-01 20:00:00', 'LOCA':3,'LOCB':1},\n                   {'Time':'2020-01-01 20:03:00', 'LOCA':12, 'LOCB':0},\n                   {'Time':'2020-01-05 21:15:00', 'LOCA':3,'LOCB':0}])\n\n#df.set_index(df['Time'],inplace = True)\ndf.set_index('Time', inplace = True)   #That is the proper way to do it\ndf.index = pd.to_datetime(df.index)\n#df.drop(['Time'], axis = 1, inplace = True)\n\ndf = df.groupby([df.index.date, df.index.hour]).max()\n\nprint(df)       \n"
"import pandas as pd\nimport numpy as np\n\na = {'code':['C001']*7+['C002']*2+['C003']*2+['C004']*2,\n     'typ':['ACT','AL','AL','SET','AL','AL','AL','ACT','SET','ACT','SET','ACT','SET'],\n     'name':['Exhaust Blower Drive',None,None,'Exhaust Blower Drive',np.nan,np.nan,np.nan,\n             'Spray Pump Motor 1 Pump','Spray Pump Motor 1 Pump',\n             'Spray Pump Motor 2 Pump','Spray Pump Motor 2 Pump',\n             'Spray Pump Motor 3 Pump','Spray Pump Motor 3 Pump']}\n\ndf = pd.DataFrame(a)\n\n#copy all the values  from name to final_component' with ffill()\n#it will fill the values where data does not exist\n#this will work only if you think all values above are part of the same set\n\ndf['final_component'] = df['name'].ffill()\n\nimport pandas as pd\nimport numpy as np\na = {'code':['C001']*7+['C002']*2+['C003']*2+['C004']*2,\n     'typ':['ACT','AL','AL','SET','AL','AL','AL','ACT','SET','ACT','SET','ACT','SET'],\n     'name':['Exhaust Blower Drive',np.nan,np.nan,'Exhaust Blower Drive',np.nan,np.nan,np.nan,\n             'Spray Pump Motor 1 Pump','Spray Pump Motor 1 Pump',\n             'Spray Pump Motor 2 Pump','Spray Pump Motor 2 Pump',\n             'Spray Pump Motor 3 Pump','Spray Pump Motor 3 Pump']}\n\ndf = pd.DataFrame(a)\n\n#copy all the values  from name to final_component' including nulls\ndf['final_component'] = df['name']\n#create a sublist of items based on unique values in code\nlookup = df[['code', 'final_component']].groupby('code').first()['final_component']\n#identify all the null values that need to be replaced\nnoname=df['final_component'].isnull()\n#replace all null values with correct value based on lookup\ndf['final_component'].loc[noname] = df.loc[noname].apply(lambda x: lookup[x['code']], axis=1)\n\nprint(df)\n\n    code  typ                     name          final_component\n0   C001  ACT     Exhaust Blower Drive     Exhaust Blower Drive\n1   C001   AL                      NaN     Exhaust Blower Drive\n2   C001   AL                      NaN     Exhaust Blower Drive\n3   C001  SET     Exhaust Blower Drive     Exhaust Blower Drive\n4   C001   AL                      NaN     Exhaust Blower Drive\n5   C001   AL                      NaN     Exhaust Blower Drive\n6   C001   AL                      NaN     Exhaust Blower Drive\n7   C002  ACT  Spray Pump Motor 1 Pump  Spray Pump Motor 1 Pump\n8   C002  SET  Spray Pump Motor 1 Pump  Spray Pump Motor 1 Pump\n9   C003  ACT  Spray Pump Motor 2 Pump  Spray Pump Motor 2 Pump\n10  C003  SET  Spray Pump Motor 2 Pump  Spray Pump Motor 2 Pump\n11  C004  ACT  Spray Pump Motor 3 Pump  Spray Pump Motor 3 Pump\n12  C004  SET  Spray Pump Motor 3 Pump  Spray Pump Motor 3 Pump\n"
'{\n    lat: 41.8781,\n    long: -87.6298,\n    city: &quot;Chicago&quot;\n}\n\njson_data = [\n    {&quot;location&quot;: [41.8781, -87.6298], &quot;city&quot;: &quot;chicago&quot;},\n    {&quot;location&quot;: [44.8141, 20.1234], &quot;city&quot;: &quot;somewhere&quot;}\n]\n\ndf = pd.DataFrame.from_records(json_data)\n\nprint(df)\n    location            city\n0   [41.8781, -87.6298] chicago\n1   [44.8141, 20.1234]  somewhere\n\nprint(df.dtypes)\nlocation    object\ncity        object\ndtype: object\n\ndf[[&quot;lat&quot;, &quot;long&quot;]] = pd.DataFrame(df[&quot;location&quot;].tolist(), columns=[&quot;lat&quot;, &quot;long&quot;])\n\nprint(df)\n    location            city      lat       long\n0   [41.8781, -87.6298] chicago   41.8781   -87.6298\n1   [44.8141, 20.1234]  somewhere 44.8141   20.1234\n\nprint(df.dtypes)\nlocation     object\ncity         object\nlat         float64\nlong        float64\ndtype: object\n\ndf[&quot;lat&quot;] = pd.to_numeric(df[&quot;lat&quot;])\ndf[&quot;long&quot;] = pd.to_numeric(df[&quot;long&quot;])\n\nprint(df)\n              location       city      lat     long\n0  [41.8781, -87.6298]    chicago  41.8781 -87.6298\n1   [44.8141, 20.1234]  somewhere  44.8141  20.1234\n\nprint(df.dtypes)\nlocation     object\ncity         object\nlat         float64\nlong        float64\ndtype: object\n'
"new_df = pd.DataFrame({f&quot;value_t{i}&quot;: df['value'].shift(i) for i in range(len(df))})\n"
"import time\nimport pandas as pd\n\nyears = [i for i in range(0, 500000)]\nyears[23] = None\nyears[44] = None\nyears[151] = None\n\nstart = time.perf_counter()\n\ndf_int = pd.DataFrame(data = years, dtype = pd.Int32Dtype())\n\nfinish = time.perf_counter()\nprint(f'Finished in {finish-start}')\n\n\n# Finished in 0.17838970199999993\n\nstart = time.perf_counter()\n\ndf_float = pd.DataFrame(data = years)\n\nfinish = time.perf_counter()\nprint(f'Finished in {finish-start}')\n\n\n# Finished in 0.07671123900000004\n\nyears = [None if i%200 == 0 else i for i in range(0, 500000)]\n\ntimes = []\nfor i in range(0,200):\n    df_int = pd.DataFrame(data = years, dtype = pd.Int32Dtype(), columns=['years'])\n\n    start = time.perf_counter()\n    df_int.dropna(inplace=True)\n    df_int.reset_index(drop=True, inplace=True)\n    finish = time.perf_counter()\n\n    times.append((finish-start))\n\ntimes = []\nfor i in range(0,200):\n    df_float = pd.DataFrame(data = years, columns=['years'])\n\n    start = time.perf_counter()\n    df_float.dropna(inplace=True)\n    df_float.reset_index(drop=True, inplace=True)\n    finish = time.perf_counter()\n\n    times.append((finish-start))\n\navg = sum(times)/len(times)\n\n# Float: 0.009853112889999979\n# Int:   0.009194320404999973\n"
'df = pd.DataFrame({&quot;Unit&quot;:[&quot;gm&quot;, np.NaN, np.NaN],\n                   &quot;Price&quot;:[&quot;40&quot;,&quot;80 per piece&quot;, &quot;110 per pack&quot;]})\n\n  Unit         Price\n0   gm            40\n1  NaN  80 per piece\n2  NaN  110 per pack\n\ns = df.loc[df[&quot;Unit&quot;].isnull(),&quot;Price&quot;].str.extract(&quot;(?P&lt;Price&gt;\\d+)\\sper\\s(?P&lt;Unit&gt;[A-Za-z]+)&quot;).dropna()\n\ndf.update(s)\n\nprint (df)\n\n    Unit Price\n0     gm    40\n1  piece    80\n2   pack   110\n'
'def na_randomfill(series):\n    na_mask = pd.isnull(series)   # boolean mask for null values\n    n_null = na_mask.sum()        # number of nulls in the Series\n    \n    if n_null == 0:\n        return series             # if there are no nulls, no need to resample\n    \n    # Randomly sample the non-null values from our series\n    #  only sample this Series as many times as we have nulls \n    fill_values = series[~na_mask].sample(n=n_null, replace=True, random_state=0)\n\n    # This ensures our new values will replace NaNs in the correct locations\n    fill_values.index = series.index[na_mask]\n    \n    return series.fillna(fill_values) \n\nout = na_randomfill(df[&quot;Apple_cat&quot;])\n\nprint(out)\n0    cat_1\n1    cat_2\n2    cat_3\n3    cat_3\n4    cat_2\n5    cat_2\nName: Apple_cat, dtype: object\n\nout = df.apply(na_randomfill)\n\nprint(out)\n   ClientId Apple_cat Region  Price\n0        21     cat_1  Reg_A      5\n1        15     cat_2  Reg_A      6\n2         6     cat_3  Reg_B      7\n3        91     cat_3  Reg_A      3\n4        45     cat_2  Reg_C      7\n5        89     cat_2  Reg_C      6\n'
"df['Title'].str.replace(r'(\\bRunning\\b|\\bCycling\\b)','',regex=True)\n"
"df['col'].apply(lambda x: x.split()[-1])\n"
"lst = ['papaya', 'avocado', 'cherry', 'mango']\ndf.loc[df['B'].isin(lst), 'A'] = 'foo'\nprint(df)\n\n     A       B\n0  foo   apple\n1  foo  banana\n2  foo  cherry\n3  foo  orange\n4  bar   melon\n5  foo  papaya\n"
'df.groupby([&quot;Sex&quot;, &quot;Children&quot;, &quot;Smoker&quot;],as_index=False)[&quot;Insurance&quot;].mean()\n\n#output\n\n    Sex Children Smoker Insurance\n0   Female  1     No    58.32\n1   Female  1     Yes   125.98\n2   Female  4     No    356.12\n3   Female  4     Yes   585.12\n4   Male    3     Yes   392.48\n5   Male    6     No    438.21\n6   Male    6     Yes   782.68\n\n     Sex       Children Smoker  size mean\n0     Female       1    No      1   58.32\n1     Female       1    Yes     1   125.98\n2     Female       4    No      1   356.12\n3     Female       4    Yes     1   585.12\n4       Male       3    Yes     1   392.48\n5       Male       6    No      1   438.21\n6       Male       6    Yes     1   782.68\n'
"def remove_link(text): \n    remove_im = ' '.join([i for i in text.split() if i not in [image_link_1, image_link_2]])\n    return remove_im\n\ndf['title_and_abstract'] = df['title_and_abstract'].apply(lambda x: remove_link(x))\n"
"df = pd.DataFrame({'Col': [1, 2, [1, 2], 3, [1, 2, 3]]})\n\ndf_lists = df.Col.apply(lambda x: type(x) == list)\nlists = [(i, item) for i, item in zip(df.index, df.Col) if type(item) == list]\n\n0    False\n1    False\n2     True\n3    False\n4     True\nName: Col, dtype: bool\n\n[(2, [1, 2]), (4, [1, 2, 3])]\n"
"In [71]: df\nOut[71]:\n  ACTIVITY_DATE  OWNNER_ID    OWNER_NAME\n0      1/1/2020      23344  JAMES NELSON\n1      2/1/2020      33445  NIGEL THOMAS\n2      1/1/2020      23344  JAMES NELSON\n3      2/1/2020      33445  NIGEL THOMAS\n\nIn [72]: df.drop_duplicates()\nOut[72]:\n  ACTIVITY_DATE  OWNNER_ID    OWNER_NAME\n0      1/1/2020      23344  JAMES NELSON\n1      2/1/2020      33445  NIGEL THOMAS\n\nIn [77]: df.drop_duplicates(subset=['ACTIVITY_DATE','OWNNER_ID'])\nOut[77]:\n  ACTIVITY_DATE  OWNNER_ID    OWNER_NAME\n0      1/1/2020      23344  JAMES NELSON\n1      2/1/2020      33445  NIGEL THOMAS\n"
"df = df[['name','doggo' , 'floofer', 'puppo', 'pupper']].copy()\n\n#convert pet_name to index, if possible strings None replace and test not NaNs or not Nones\ndf1 = df.set_index('name').replace('None', np.nan).notna()\n\ndf1 = df1.dot(df1.columns + ',').str[:-1].reset_index(name='dog_stage')\nprint (df1)\n      name      dog_stage\n0        A        floofer\n1        B          doggo\n2        C          puppo\n3        D         pupper\n4        E  doggo,floofer\n5        F   puppo,pupper\n6        G         \n\ndf1 = (df.set_index('name')\n         .replace('None', np.nan)\n         .apply(lambda x: ','.join(x.dropna()), axis=1)\n         .reset_index(name='dog_stage'))\n"
"df=pd.DataFrame({'Address1':[np.nan, 'jkl','pol','city1', np.nan],'Address2':['lop',np.nan,'pola',np.nan, np.nan],'Address3':[np.nan, np.nan,np.nan,'city13', np.nan],'Address4':[np.nan, np.nan,np.nan,np.nan,'shaka']})\n\n    Address1 Address2 Address3 Address4\n0      NaN      lop      NaN      NaN\n1      jkl      NaN      NaN      NaN\n2      pol     pola      NaN      NaN\n3    city1      NaN   city13      NaN\n4      NaN      NaN      NaN    shaka\n\nlistofAdress=['lop','jkl','pola','city13']\n\ndf['temp']=df.iloc[:,:3].values.tolist()\n\n  df=df.assign(Address4=np.where(df.Address4.isna(),(df['temp'].map(set).apply(lambda x:x.intersection(set(listofAdress)))).map(list).str[0],df.Address4)).drop('temp',1)\n\n\n\n\n      Address1 Address2 Address3 Address4\n0      NaN      lop      NaN      lop\n1      jkl      NaN      NaN      jkl\n2      pol     pola      NaN     pola\n3    city1      NaN   city13   city13\n4      NaN      NaN      NaN    shaka\n"
"# necessary stmts for datetime index goes here\n\na = df.groupby(pd.Grouper(freq='60Min'))\n\ncol_list = ['Date-Hour']\nfor i in range(60):\n    col_list.append('min_'+str(i))\n\nnew_df = pd.DataFrame(columns=col_list)\n\nfor idx, i in enumerate(a):\n    if len(i[1])&gt;48: # The 80% from the step-2 hard-coded here\n        st = str(i[0])\n        nidx = pd.date_range(start=st, end = st[:-5]+'59:00', freq='1T')\n        ns = pd.Series(np.nan, index = nidx)\n        comb_series = pd.concat([i[1], ns])\n        comb_series = comb_series[~comb_series.index.duplicated(keep='first')]\n        comb_series.sort_index(inplace=True) #required due to the concat above\n        tmp = comb_series['x'].tolist()\n        tmp.insert(0,st[:-3])\n        new_df.loc[idx] = tmp\n"
"x = [None] * len(df['Text']) # create list with same length as df\ni = 0\nfor text in df['Text']:\n    if  all(j.isalpha() or j.isspace() for j in text):\n        x[i] = False # contains letters and whitespace\n    else:\n        x[i] = True # contains digits, special xters\n    i += 1\n\nno_text_df = df[x] # contains all rows with digits and special xters in Text column\n"
"d = {'months': 31, 'years':365, 'hours':1, 'days':1}\ndf1 = df['details'].str.extract('(\\d+)\\s+(years|months|hours|days)', expand=True)\ndf['Time'] = df1[0].astype(float).mul(df1[1].map(d)).astype('Int64').astype(str)\n\ndf['Unit'] = np.where(df1[1].isin(['years','months', 'days']), ' days', ' ' + df1[1])\n\ndf['Time'] += df.pop('Unit')  \nprint (df)\n     Name                      details       Time\n0    prem  6 months probation included   186 days\n1  shaves            3 years suspended  1095 days\n2  geroge           48 hours work time   48 hours\n3  julvie       4 years terms included  1460 days\n4     tiz                 80 days work    80 days\n5    lamp                 44 days work    44 days    \n\n#specified dictionary for extract to days\nd = {'months': 31, 'years':365, 'days':1}\n\n#extract anf multiple by dictionary\nout = {k: df['details'].str.extract(rf'(\\d+)\\s+{k}', expand=False).astype(float).mul(d[k])\n          for k, v in d.items()}\n#join together, sum and convert to days with replace 0 days \ndays = pd.concat(out, axis=1).sum(axis=1).astype(int).astype('str').add(' days').replace('0 days','')\n\n#extract hours\nhours = df['details'].str.extract(r'(\\d+\\s+hours)', expand=False).radd(' ').fillna('')\n\n#join together\ndf['Time'] = days + hours\nprint (df)\n     Name                                      details               Time\n0    john  2 years 1 months 10 days 15 hours work time  771 days 15 hours\n1    prem                  6 months probation included           186 days\n2  shaves                  3 years 6 months  suspended          1281 days\n3  geroge                           48 hours work time           48 hours\n4  julvie               4 years 20 days terms included          1480 days\n5     tiz                                 80 days work            80 days\n6    lamp                                 44 days work            44 days\n    \n"
'    # Function for retrieving data into a list from your .txt file\ndef get_data(filename):\n    # Open file in read mode\n    with open(f&quot;{filename}&quot;, &quot;r&quot;) as f:\n        # Reads data into a new list called raw_data with an item for every line\n        raw_data = f.readlines()\n    return raw_data\n\n\ndef clean(data_to_clean):\n    # Removes &lt; and &gt;\n    first_clean = [line.replace(&quot;&lt;&quot;, &quot;&quot;).replace(&quot;&gt;&quot;, &quot;&quot;) for line in data_to_clean]\n    # Removes ( and ) with restrictions\n    # Swap out exceptions with place holders\n    swap_out = [linea.replace(&quot;(.)&quot;, &quot;bdb&quot;).replace(&quot;(..)&quot;, &quot;bddb&quot;).replace(&quot;(...)&quot;, &quot;bdddb&quot;) for linea in first_clean]\n    # Clean remaining brackets out\n    second_clean = [lineb.replace(&quot;(&quot;, &quot;&quot;).replace(&quot;)&quot;, &quot;&quot;) for lineb in swap_out]\n    # Swap in exceptions again\n    swap_in = [linec.replace(&quot;bdb&quot;, &quot;(.)&quot;).replace(&quot;bddb&quot;, &quot;(..)&quot;).replace(&quot;bdddb&quot;, &quot;(...)&quot;) for linec in second_clean]\n    return swap_in\n\n \nprint(clean(get_data(&quot;mydata.txt&quot;)))\n'
"import requests\nimport pandas as pd\nfrom bs4 import BeautifulSoup\n\nheaders = {&quot;user-agent&quot;: &quot;Mozilla/5.0&quot;}\nurlList = [\n    'https://www.unep.org/news-and-stories/story/how-monitoring-sewage-could-prevent-return-coronavirus',\n    'https://www.unep.org/news-and-stories/story/caribbean-wrestles-mischievous-invaders-monkeys'\n          ]\ndata = []\n\nfor url in urlList:\n    r = requests.get(url,headers=headers)\n    soup = BeautifulSoup(r.text, &quot;html.parser&quot;)\n    \n    text = soup.select_one('div.para_content_text').get_text(strip=True)\n    topic = soup.select_one('div.article_tags_topics').get_text(strip=True)\n    tags = soup.select_one('div.article_tags_tags').get_text(strip=True)\n    data.append(\n        {\n        'text':text,\n        'topic': topic,\n        'tags':tags\n        }\n    )\n\npd.DataFrame(data).to_csv('text.csv', index = False, header=True)\n"
"from pyspark.sql import functions as F\n\ndf = df.withColumn(&quot;date&quot;, F.to_date(&quot;date&quot;, &quot;dd-MM-yyyy&quot;))\n\ndf1 = df.alias(&quot;df1&quot;).join(\n    df.alias(&quot;df2&quot;),\n    (F.col(&quot;df1.VenueName&quot;) == F.col(&quot;df2.VenueName&quot;)) &amp;\n    (F.expr(&quot;df1.date - INTERVAL 1 year&quot;) == F.col(&quot;df2.date&quot;)),\n    &quot;left&quot;\n).select(\n    &quot;df1.name_id&quot;, &quot;df1.VenueName&quot;, &quot;df1.date&quot;,\n    F.when(\n        F.col(&quot;df1.Booking&quot;) &lt; 25, F.coalesce(&quot;df2.movingAvg&quot;, &quot;df1.Booking&quot;)\n    ).otherwise(F.col(&quot;df1.Booking&quot;)).alias(&quot;Booking&quot;),\n    &quot;df1.movingAvg&quot;\n)\n\n# verify changes for dates = 2020-01-12 / 2019-01-12\ndf1.filter(&quot;date in ('2020-01-12', '2019-01-12')&quot;).show()\n"
"text1 = [['',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  '',\n  ''],\n ['', '', '', '', '', '', '', '', '']]\nfor i in range(len(text1)):\n    text1[i] = [j for j in text1[i] if j != '']\n"
"import pandas as pd\nimport io\n\ns = '''\\\nSymbol,Synonyms\nA1BG,A1B|ABG|GAB|HYST2477\nA2M,A2MD|CPAMD5|FWP007|S863-7\nA2MP1,A2MP\nNAT1,AAC1|MNAT|NAT-1|NATI\nNAT2,AAC2|NAT-2|PNAT\nNATP,AACP|NATP1\nSERPINA3,AACT|ACT|GIG24|GIG25'''\n\nmylist = ['GAB', 'A2M', 'GIG24']\ndf = pd.read_csv(io.StringIO(s))\n\n# Store the lookup serie\nlookup_serie = df['Symbol'].str.cat(df['Synonyms'],'|').str.split('|')\n\n# Create lambda function to return first value from mylist, No match! if stop-iteration\nf = lambda x: next((i for i in x if i in mylist), 'No match!')\n\ndf.insert(0,'Input_Symbol',lookup_serie.apply(f))\nprint(df)\n\n  Input_Symbol    Symbol                   Synonyms\n0          GAB      A1BG       A1B|ABG|GAB|HYST2477\n1          A2M       A2M  A2MD|CPAMD5|FWP007|S863-7\n2    No match!     A2MP1                       A2MP\n3    No match!      NAT1       AAC1|MNAT|NAT-1|NATI\n4    No match!      NAT2            AAC2|NAT-2|PNAT\n5    No match!      NATP                 AACP|NATP1\n6        GIG24  SERPINA3       AACT|ACT|GIG24|GIG25\n\nf = lambda x: [i for i in x.split('|') if i in mylist] != []\n\nm1 = df['Symbol'].apply(f)\nm2 = df['Synonyms'].apply(f)\n\ndf[m1 | m2]\n"
"vect_dict = {w: model.wv[w] for w in model.wv.vocab}\ndataframe = pd.DataFrame(vect_dict).T\n\nIn [1]: pd.DataFrame({'a': [1,2,3], 'b': [2,3,4]}).T\nOut[1]:\n   0  1  2\na  1  2  3\nb  2  3  4\n"
'import re\nfrom math import ceil\n\npat = re.compile(r\'(\\d+(?:\\.\\d+)?)\\D(\\d+(?:\\.\\d+)?)\')\nl = [(ceil(float(a)), ceil(float(b))) for a, b in pat.findall(your_post)]\nprint(l)\n# [(100, 200), (200, 300), (300, 400), (300, 400), (300, 400), (301, 401), (301, 401)]\n\nfor pair in l:\n    print("{}-{}".format(*pair))\n'
"number_ranges = [\n    '11.6', '665.690, 705.715', '740.54-830.18ABC;900-930ABC', '1200',\n    '2100 / 2200; 2320 / 2350', '2300-2400 / 2500-2560 / 2730-2740', '433.454', '345-654'\n]\n\nimport re\n\ndef outer_split(rangetext):\n    '''Split the input text to individual range text.'''\n    # Rule:\n    # if both characters are present, use the second one to split\n    # and switch the first one to '-'\n\n    doubleseparators = ['-/', '.,', '-;', '/;'] \n\n    for c in doubleseparators:\n        if c[0] in rangetext and c[1] in rangetext:\n            outersplit = rangetext.split(c[1])\n            outersplit = [s.replace(c[0], '-') for s in outersplit]\n            break\n    else:\n            outersplit = [rangetext, ]\n\n    return outersplit\n\n\ndef inner_split(rangetext):\n    '''Clean the range text and Split to [left, right] boundaries.'''\n\n    rangetext = re.sub(r'[a-zA-Z ]+', '', rangetext)\n\n    sep = '-'\n    if sep in rangetext:\n        innersplit = rangetext.split(sep)\n    else:\n        innersplit = [rangetext,]\n\n    # The special '.' case:\n    if len(innersplit)==1 and '.' in innersplit[0]:\n        l, r = innersplit[0].split('.')\n        if len(l)&gt;2 or len(r)&gt;2:\n            innersplit = [l, r]\n        else:\n            innersplit = [str(float(innersplit[0])*1000), ]\n\n    return innersplit\n\n\nindividualinputs = [individualinput for text in number_ranges\n                    for individualinput in outer_split(text)]\n\n[inner_split(textrange) for textrange in individualinputs]\n\n[['11600.0'],\n ['665', '690'],\n ['705', '715'],\n ['740.54', '830.18'],\n ['900', '930'],\n ['1200'],\n ['2100', '2200'],\n ['2320', '2350'],\n ['2300', '2400'],\n ['2500', '2560'],\n ['2730', '2740'],\n ['433', '454'],\n ['345', '654']]\n"
"df[Start] = df[Start].apply(lambda x: x.rstrip(' A'))\n"
"pd_frame = pd.read_csv('imdb.csv', error_bad_lines=False)\n\nimdb_data= spark.createDataFrame(pd_frame)\n\nimdb_data = spark.read.csv('imdb.csv', header='true', mode='DROPMALFORMED')\n"
'STU-ID QUESTION 1 RESPONSE 1 QUESTION 2 RESPONSE 2\n00001  tutoring?  True       lunch a?   False\n\nSTU-ID QUESTION 1 RESPONSE 1 QUESTION 2 RESPONSE 2\n00004  tutoring?  True        lunch a?  TRUE\n\nSTU-ID QUESTION 1    RESPONSE 1 Tutorer GPA\n00001  improvement?  True       Jim     3.5\n\nSTU-ID QUESTION 1    RESPONSE 1 Tutorer  GPA\n00004  improvement?  yes        Sally    2.8\n\nimport pandas as pd\n\ndf_s1s1 = pd.read_excel(\'survey1.xlsx\', na_values="Missing", sheet_names=\'sheet 1\', usecols=cols)\ndf.head()\ndf_s1s2 = pd.read_excel(\'survey1.xlsx\', na_values="Missing", sheet_names=\'sheet 2\', usecols=cols)\ndf_s1s2.head()\n\ndf_s2s1 = pd.read_excel(\'survey2.xlsx\', na_values="Missing", sheet_names=\'sheet 1\', usecols=cols)\ndf.head()\ndf_s2s2 = pd.read_excel(\'survey2.xlsx\', na_values="Missing", sheet_names=\'sheet 2\', usecols=cols)\ndf_s1s2.head()\n\ndf_survey_1 = pd.concat([df_s1s1, df_s1s2])\ndf_survey_1.head()\n\ndf_survey_2 = pd.concat([df_s2s1, df_s2s2])\ndf_survey_2.head()\n\nmaster_df = pd.merge(df_survey_1, df_survey2, left_on=\'STU_ID\', right_on=\'STU_ID\')\n\nmaster_df = master_df.dropna(axis = 0, how =\'any\')\n'
'df.iloc[:(df.y.shift()&lt;df.y).idxmax()[0]]\n\ndf.iloc[:(df.y.shift()&lt;df.y).to_numpy().argmax()]\n\nIn [106]: df\nOut[106]: \n      y         z\n7   118  0.149675\n8   118  0.386489\n9     3  0.449950\n10    3  0.902349\n11    8  0.969809\n12   11  0.170910\n\nIn [107]: df.iloc[:(df.y.shift()&lt;df.y).to_numpy().argmax()]\nOut[107]: \n      y         z\n7   118  0.149675\n8   118  0.386489\n9     3  0.449950\n10    3  0.902349\n\nIn [108]: a = df.y.to_numpy().ravel()\n\nIn [109]: df.iloc[:(a[:-1] &lt; a[1:]).argmax()+1]\nOut[109]: \n      y         z\n7   118  0.149675\n8   118  0.386489\n9     3  0.449950\n10    3  0.902349\n\nIn [110]: df.iloc[:(np.diff(a)&gt;0).argmax()+1]\nOut[110]: \n      y         z\n7   118  0.149675\n8   118  0.386489\n9     3  0.449950\n10    3  0.902349\n'
"for i in range(len(stackdata)):\n    stackdata['contact_id'][i] = str(stackdata2[(stackdata2['email'] == stackdata['from:'][i]) | (stackdata2['phone'] == stackdata['from:'][i]) | (stackdata2['slack'] == stackdata['from:'][i])]['id'].values).strip('[]')\n"
"list_a = [('1', '2', '3'), ('2', '3', '4'), ('3', '4', '5')]\n\nlist_a = [tuple(map(int, i)) for i in list_a]\n"
"df = df.ffill()\ndf = df.groupby(['A','B','C']).Value.sum().reset_index()\ndf_incident_local_count.head()\n\n        df:\n    A   B   C  Value\n0   10  aa  MN     11\n1   12  bb  MN     5\n2   13  cc  BK     15\n3   14  cc  SI     8\n"
"import re\n\npat = '|'.join(re.escape(x) for x in joblist)\n\ndf['Company'] = df['Title'].str.contains(pat)\n\ndf['Company'] = df['Title'].str.extract(f'({pat})', expand=False)\n"
"import io\nimport re\n\nwith io.open('a.csv') as fin:\n    for line in fin:\n        parts = line.split(';')\n        parts[13] = parts[13].replace('', ' ')\n        print ','.join(parts)\n"
'#!/usr/bin/env python\n\nimport glob\nimport random\nimport os\nimport pandas\n\ndef rm_minus_rf(dirname):\n    for r,d,f in os.walk(dirname):\n        for files in f:\n            os.remove(os.path.join(r, files))\n        os.removedirs(r)\n\ndef create_testfiles(path):\n    rm_minus_rf(path)\n    os.mkdir(path)\n\n    random.seed()\n    for i in range(10):\n        n = random.randint(10000,99999)\n        for j in range(random.randint(0,20)):\n            # year may repeat, doesn\'t matter\n            year = 2015 - random.randint(0,20)\n            with open("{}/{}-{}.csv".format(path, n, year), "w"):\n                pass\n\ndef find_filesets(path="."):\n    csv_files = {}\n    for name in glob.glob("{}/*-*.csv".format(path)):\n        # there\'s almost certainly a better way to do this\n        key = os.path.splitext(os.path.basename(name))[0].split(\'-\')[0]\n        csv_files.setdefault(key, []).append(name)\n\n    for key,filelist in csv_files.items(): \n        print key, filelist\n        # do something with filelist\n        create_merged_csv(key, filelist)\n\ndef create_merged_csv(key, filelist):\n    with open(\'{}-aggregate.csv\'.format(key), \'w+b\') as outfile:\n        for filename in filelist:\n            df = pandas.read_csv(filename, header=False)\n            df.to_csv(outfile, index=False, header=False)\n\nTEST_DIR_NAME="testfiles"\ncreate_testfiles(TEST_DIR_NAME)\nfind_filesets(TEST_DIR_NAME)\n'
"eps_table.replace(r'[%+,]', '', regex=True)\n"
'#Adding two timestamps is not supported and not logical\n#Probably, you really want to add the time rather than the timestamp itself\n#This is how to extract the time from the timestamp then summing it up\n\nimport datetime\nimport time\n\nt = [\'1995-07-01 00:00:01\',\'1995-07-01 00:00:06\',\'1995-07-01 00:00:09\',\'1995-07-01 00:00:09\',\'1995-07-01 00:00:09\']\ntSum = datetime.timedelta()\ndf = pd.DataFrame(t, columns=[\'timestamp\'])\nfor i in range(len(df)):\n    df[\'timestamp\'][i] = datetime.datetime.strptime(df[\'timestamp\'][i], "%Y-%m-%d %H:%M:%S").time()\n    dt=df[\'timestamp\'][i]\n    (hr, mi, sec) = (dt.hour, dt.minute, dt.second)\n    sum = datetime.timedelta(hours=int(hr), minutes=int(mi),seconds=int(sec))\n    tSum += sum\nif tSum.seconds &gt;= 60*60:\n    print("more than 1 hour")\nelse:\n    print("less than 1 hour")\n'
'import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport glob\n\n# split hurricanes into separate files\npartNum = 1\noutHandle = None\nfor line in open("data/atlantic_1851_2017_2.csv","r").readlines():\n    if line.startswith(\'AL\'):\n        if outHandle is not None:\n            outHandle.close()\n        outHandle = open("data/part%d.csv" % (partNum,), "w")\n        partNum += 1\n    outHandle.write(line)\noutHandle.close()\n\n\n# read in each file as data-frame\nfiles = glob.glob(\'data/part*.csv\')\nframes = []\nfor csv in files:  \n    with open(csv) as f:\n        first_line = f.readline() \n    first_line = first_line.split(\',\')    \n    df = pd.read_csv(csv, skiprows=[0], header=None)\n    df[\'ID\'] = first_line[0]\n    df[\'Name\'] = first_line[1]\n    frames.append(df)\n\n\n# concatenate into a single data-frame\ndf = pd.concat(frames)\ndf = df.drop(columns=[8,9,10,11,12,13,14,15,16,17,18,19,20])\ndf.columns = [\'Date\',\'Time\',\'Record_ID\',\'Strength\',\'Lat\',\'Long\',\'Max_Wind_Knots\',\'Max_Pressure_mb\',\'ID\',\'Name\']\nprint(df.head(5))\n'
'import pandas as pd\n\nhtml_text = \'\'\'&lt;table&gt;\n  &lt;tr&gt;\n    &lt;th colspan="6"&gt;#Receiver&lt;/th&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;#DateTime&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;#Address&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td colspan="6"&gt;&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;Col1&lt;/td&gt;\n    &lt;td&gt;Col2&lt;/td&gt;\n    &lt;td&gt;Col3&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;1&lt;/td&gt;\n    &lt;td&gt;A&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;10%&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;2&lt;/td&gt;\n    &lt;td&gt;B&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;20%&lt;/td&gt;\n  &lt;/tr&gt;\n  &lt;tr&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;&lt;/td&gt;\n    &lt;td&gt;3&lt;/td&gt;\n    &lt;td&gt;C&lt;/td&gt;\n    &lt;td&gt;2&lt;/td&gt;\n    &lt;td&gt;10%&lt;/td&gt;\n  &lt;/tr&gt;\n&lt;/table&gt;\'\'\'\nROWS_YOU_WANT_TO_SKIP=3\ndf = pd.read_html(html_text, skiprows = ROWS_YOU_WANT_TO_SKIP)\ndf.head()\n'
"d = pd.DataFrame({'a': ['aaaak', 'k jhs', 'anhdga', 'kjdhs']})\n\ndata.a.str[0]\n\n0    a\n1    k\n2    a\n3    k\n\ndata.a.str[0] == 'a'\n\n0     True\n1    False\n2     True\n3    False\n\ndata[data.a.str[0] == 'a']\n\n        a\n0   aaaak\n2  anhdga\n\ndata[data.a.str[0] == 'a'].a.str[2]\n\n0    a\n2    h\n"
'In [127]: df1.groupby("Date", as_index=False).sum()\nOut[127]: \n  Date    A   B    C\n0  1/1  163  75    0\n1  2/1    0   0  160\n\nIn [128]: df0.merge(df1.groupby("Date", as_index=False).sum())\nOut[128]: \n  Date  Col1  Col2    A   B    C\n0  1/1   NaN   NaN  163  75    0\n1  2/1   NaN   NaN    0   0  160\n'
"import pandas as pd\n\ndf = pd.DataFrame({'A': ['20*', 40, '30*' ], 'B': ['abc', 'bar', 'xyz']})\ndf.replace({'A': {r'(\\d+)\\*': r'\\1'}}, regex=True, inplace=True)\n\nprint(df)\n"
'df=df.assign(**dict.fromkeys(new_columns,[1,2]))\ndf\nOut[84]: \n    name  age  score  col1  col2\n0    Ken    5      1     1     1\n1  Bobby    6      2     2     2\n\nfor i in new_columns:\n     df[i] = [1,2]\n\ndf\nOut[86]: \n    name  age  score  col1  col2\n0    Ken    5      1     1     1\n1  Bobby    6      2     2     2\n\ndf.index=[100,101]\nfor i in new_columns:\n     df[i] = pd.Series([1,2])\n\ndf\nOut[89]: \n      name  age  score  col1  col2\n100    Ken    5      1   NaN   NaN\n101  Bobby    6      2   NaN   NaN\n'
'missing_values=["n/a", "na", "--"]\ndf = pd.read_csv("data/data_bestand_3.txt", sep="&amp;", na_values=missing_values, header=None)\n\ndf.columns = df.loc[0].str.split(\'=\').str[0].values\n'
'with open(\'urls.txt\', \'r\') as f:\n    for line in f:\n        line = line.strip()   # this might be optional\n        if line.startswith(\'/\'):\n            print(line.split("/")[1])\n'
'with open("words.txt", "r") as reader, open("words2.txt", "w") as writer:\n    index_dict = dict()      # stores the indices as keys and list of ITEMs as value\n    obj1 = re.compile(r\'^\\s*(\\d+)\\s+(.*)\')    # this pattern will match the lines starting with an integer(to ignore the header)\n    obj2 = re.compile(r\'(\\d+)\\s*(\\(\\s*(\\d+)\\s*\\)|-\\s*(\\d+))?\')   # this will match the ITEMs in the format \\d+|\\d+(\\d+)|\\d+-\\d+\n    for line in reader:\n        s_obj = obj1.search(line)             \n        if s_obj:               # If this line contains the input in the desired format, then go ahead and process it, otherwise ignore\n            list_index = int(s_obj.group(1))          \n            all = obj2.finditer(s_obj.group(2))    # s_obj.group(2) contains the matched items and their quantity such as 56(5) 57(3)| 56 | 67-45 \n            index_items = []                      # this will hold all the items for the current index and their quantities\n            for m in all:\n                item_quantity = [m.group(1), \'1\']    # if item is not followed by \'()\' or \'-\' assume it\'s 1\n                if m.group(2):                     # If it\'s followed by \'()\' or \'-\' update its count with the number following \'(\' or \'-\'\n                    item_quantity[1] = m.group(3) if not m.group(4) else m.group(4)\n                index_items.append(item_quantity)\n\n            index_dict[list_index] = index_items\n\n\n    sorted_keys = sorted(index_dict.keys())  # sort the keys as dictionary might not give keys in the ascending order and it\'s assumed that list indices would be in ascending order\n    writer.write("{0: &gt;10} {1: &gt;10} {2: &gt;10}\\n".format("LIST", "ITEM", "Q"))\n    for index in sorted_keys:\n        for val in index_dict[index]:\n            output_line = "{0: &gt;10} {1: &gt;10} {2: &gt;10}\\n".format(str(index), val[0], val[1])\n            writer.write(output_line)\n\nList  ITEM\n1    56(5) 57(1)\n2    23\n3    21-9\n4    9(3) 5(4) 89-9\n\n      LIST     ITEM         Q\n      1         56          5\n      1         57          1\n      2         23          1\n      3         21          9\n      4          9          3\n      4          5          4\n      4         89          9\n'
"s = pd.Series(['25k', '1.25m', '100k', '500', '2.5k'])\n\nvm = s.str.extract('(?P&lt;value&gt;.*?)(?P&lt;multiplier&gt;[km])?$')\n\n      value multiplier\n0    25          k\n1  1.25          m\n2   100          k\n3   500        NaN\n4   2.5          k\n\ndf['Numeric Vehicle Value'] = pd.to_numeric(vm['value']) * vm['multiplier'].replace({'k': 1000, 'm': 1000000}).fillna(1)\n"
"df.loc[~df['Ship To Customer Zip'].str.contains('[A-Za-z]'), 'ZipCleaned'] = df['Ship To Customer Zip'].str.slice(stop=5)\ndf.loc[df['Ship To Customer Zip'].str.contains('[A-Za-z]'), 'ZipCleaned'] = df['Ship To Customer Zip'].str.replace(' |-','')\n"
'crpDF = df2[df2.Group_Id == "CRP (mg/L)"]\nil6DF = df2[~(df2.Group_Id == "CRP (mg/L)")]\n'
"mrnList = [val1, val2, ...,val61]\n\ndf_filtered = df[df['MRN'].isin(mrnList)]\n"
'data1 = np.genfromtxt(\'text.b\', dtype=str, delimiter="\\t", comments=None)\n'
"df.groupby('movie_name').apply(lambda grp:\n    grp[grp.time.dt.date == grp.time.min().date()])\n"
'palabras = [elem.findAll("td")[1].get_text().rstrip() for elem in wiki_filas[1:]]\nfrecuencia = [elem.findAll("td")[2].get_text().rstrip() for elem in wiki_filas[1:]]\n'
's = pd.to_datetime(df[\'Date\'],errors=\'coerce\').isna() \n# gives us the error rows to filter.\n\n# split out our datetime column so we can extract the values.\ndate_err = (\n    df[s]["Date"]\n    .str.extract("\\d{2}-\\d{2}-\\d{4}\\s+(\\w+.*)")[0]\n    .str.split("\\s", expand=True)\n)\n\n# set your values with `.loc` \ndf.loc[s,\'Professional\'] = date_err[0]\ndf.loc[s,\'Description\'] = date_err[1]\n\n# extract date.\ndate = df[s][\'Date\'].str.extract(\'(\\d{2}-\\d{2}-\\d{4})\')[0] \ndf.loc[s,\'Date\'] = date\n#set datetime column.\ndf[\'Date\'] = pd.to_datetime(df[\'Date\'])\n\nthree_err = (\n    df[s]["3"].str.extract("([^\\[A-Za-z]+)")[0].str.strip().str.split("\\s", expand=True)\n)   \n\n# set values and replace \'3\' with nan.\ndf.loc[s,\'Hours\'] = three_err[0]\ndf.loc[s,\'Rate\'] = three_err[1]\ndf.loc[s,\'Amount\'] = three_err[2]\ndf.loc[s,\'3\'] = np.nan\n\nprint(df)\n\n         Date Professional                      Description    1    2 Hours  \\\n1  2019-12-19           KL                 Sib ad upoketewm  NaN  NaN   1.9   \n3  2019-12-20           JB    Mo wywcig tjovwip pwos es kib  NaN  NaN   0.8   \n5  2019-12-27           JB  sop tupherr eq NGINX geflar, ic  NaN  NaN   0.2   \n7  2019-12-27           JB   zvsyhebig bytwav xip jfiv cuoj  NaN  NaN   0.1   \n9  2019-12-30           JB       Bwijjykg iq kwic pyu febig  NaN  NaN   0.1   \n11 2019-12-30           JB                        Telephone  NaN  NaN  0.10   \n\n      3    4    Rate Amount  \n1   NaN  NaN     200    380  \n3   NaN  NaN     210    168  \n5   NaN  NaN     210     42  \n7   NaN  NaN     210     21  \n9   NaN  NaN     210     21  \n11  NaN  NaN  210.00  21.00  \n\ndate = df[\'Date\'].str.extract(\'(\\d{2}-\\d{2}-\\d{4})(\\s\\w+\\s\\w+)\\s(\\w+.*)\')[0]\nname = df[\'Date\'].str.extract(\'(\\d{2}-\\d{2}-\\d{4})(\\s\\w+\\s\\w+)\\s(\\w+.*)\')[1]\ndescription = df[\'Date\'].str.extract(\'(\\d{2}-\\d{2}-\\d{4})(\\s\\w+\\s\\w+)\\s(\\w+.*)\')[2]\n\n\ndf.loc[pd.to_datetime(df[\'Date\'],errors=\'coerce\').isnull(),\'Professional\'] = name\ndf.loc[pd.to_datetime(df[\'Date\'],errors=\'coerce\').isnull(),\'Description\'] = description\ndf.loc[pd.to_datetime(df[\'Date\'],errors=\'coerce\').isnull(),\'Date\'] = date\n\nprint(df)\n\n\n     Date    Professional  \\\n1   2019-12-19 00:00:00      Katie Cool   \n3   2019-12-20 00:00:00   Jenn Blossoms   \n5   2019-12-27 00:00:00   Jenn Blossoms   \n7   2019-12-27 00:00:00   Jenn Blossoms   \n9   2019-12-30 00:00:00   Jenn Blossoms   \n11           12-30-2019   Jenn Blossoms   \n\n                                          Description  \n1                                 Travel to Space ...  \n3          Review stuff; prepare cancellations of ...  \n5                           Review lots of stuff/o...  \n7                      Draft email to world leader...  \n9                                  Review this thing.  \n11  Telephone   Call   to   A.   Bell   return   h...  \n'
"import pandas as pd \n\ndf = pd.DataFrame(['Usa','Australia','Asia','Africa','Europe'],columns = ['continent'])\n\n\n# make a list of word you want to filter \n\nlist_ = ['Asia','Europe','Africa']\n\n\n# now you can use pandas isin functionality to filter the data that you want\n\ndf.loc[df['continent'].isin(list_)]\n\n#op\n    continent\n2   Asia\n3   Africa\n4   Europe\n"
"In [10]: import pandas as pd                                                                                           \n\nIn [11]: import numpy as np                                                                                            \n\nIn [12]: df = pd.DataFrame({ \n    ...:     'Director': ['A', 'B', 'C', 'D'], \n    ...:     'Genre1': ['Action', 'Adventure', 'Horror', 'Animation'], \n    ...:     'Genre2': ['Adventure', 'Mystery', 'Thriller', 'Comedy'] \n    ...: })                                                                                                            \n\nIn [13]: unique_genres = list(df['Genre1'].dropna().unique()) + list(df['Genre2'].dropna().unique())                   \n\nIn [14]: for v in unique_genres: \n    ...:     df[v] = df[['Genre1', 'Genre2']].apply(lambda x: v in list(x), raw=True, axis=1) \n    ...:                                                                                                               \n\nIn [15]: df[['Director'] + unique_genres]                                                                              \nOut[15]: \n  Director  Action  Adventure  Horror  Animation  Adventure  Mystery  Thriller  Comedy\n0        A    True       True   False      False       True    False     False   False\n1        B   False       True   False      False       True     True     False   False\n2        C   False      False    True      False      False    False      True   False\n3        D   False      False   False       True      False    False     False    True\n\nIn [16]:                                               \n"
"dfs = []\nfor k, v in data1.items():\n    df = pd.json_normalize(v)\n    df['rowid'] = k\n    dfs.append(df)\n\ndf = pd.concat(dfs).reset_index(drop='index')\nprint(df)\n\n\n   confident      iab  rowid\n0      False  IAB25-3      0\n1      False   IAB6-6      1\n2       True     IAB6      1\n3       True  IAB16-1      2\n4       True    IAB16      2\n5      False     IAB9      2\n6      False  IAB9-28      2\n"
"new_df = df2.merge(df1, on=[&quot;eventID&quot;, &quot;instanceId&quot;], how=&quot;outer&quot;)\n\n# Dictionary with keys as column names and values as the aggregation/summary method.\nagg_dict = {\n    &quot;duration&quot;: &quot;mean&quot;,\n    &quot;value&quot;: &quot;mean&quot;\n}\ngroup_by_columns = [&quot;eventID&quot;, &quot;instanceId&quot;] # We'll get one row in output for each combination of these columns\nnew_df2 = df2.groupby(group_by_columns).agg(agg_dict).reset_index()\n\nresult = new_df2.merge(df1, on=[&quot;eventID&quot;, &quot;instanceId&quot;], how=&quot;outer&quot;)\n"
"for i in range(len(header)):\n  if (line[i] == ' ?'):\n\nfor cell in line:\n    if cell == ' ?':\n        ...\n\nquestion_mark = (' ?' in line)\n"
"temp_df = audiob_adv['Listening Time'].str.extract(r'(\\d+)[^\\d]+(\\d+)').astype('int32')\naudiob_adv[&quot;Time&quot;] = temp_df.iloc[:,0]*60 + temp_df.iloc[:,1]\n"
"import json\n\nwith open('txtfile1.txt') as input:\n    data = input.read()\n#Select the part where the dictionary start (after &quot;rows&quot;:)\nnesteddict = json.loads(data[(data.find('rows')+6):])\nfinallist = []\nfor x in nesteddict:\n    sublist = []\n    sublist.append(x['c'][0]['v'])\n    sublist.append(x['c'][2]['f'])\n    sublist.append(x['c'][5]['v'])\n    finallist.append(sublist)\n\n[['Albania', '2020-09-01', -0.503875968992248], ['Albania', '2020-09-02', -0.358695652173913], ['Albania', '2020-09-03', -0.302083333333333], ['Albania', '2020-09-04', -0.135922330097087], ['Albania', '2020-09-05', -0.436170212765957]]\n"
'csv = [[\'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\'], [\'2\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\'], [\'3\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'1\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\', \'0\']]\n\ncsvdict = {words[0]:words[1:] for words in csv}\nfor x in sorted(csvdict.keys()): # iterate over the keys \'1\', \'2\', ....\n    products = [index+1 for index,v in enumerate(csvdict[x]) if v == \'1\' ] # create list of purchases where the \'1\'s are indexed\n\n    print "costummer", x, "purchased", \' \'.join([\'"item \'+str(i)+\'"\' for i in B])\n'
"def clean(data):\np = re.compile(r'[^-\\w]') # remove characters\ndata = p.sub(' ', data)\nq = re.compile(r'[\\d_\\.]*') # remove numbers\nreturn q.sub('', data)\nfor i in text_train:\n    for j, t in enumerate(text_train[i]):\n        text_train[i][j] = [w for w in clean(t).split() if len(w)&gt;1]\n"
"mystr = '\\r\\nContact Imported:\\r\\nBusinessPhone : 9547711900 Line1 : 2440 East Commercial Blvd.\\r\\n City : Ft. Lauderdale\\r\\n State : FL\\r\\n PostalCode : 33308\\r\\n\\r\\nArt Womack recommends Steve Paul Dentist on Commercial Blvd area.\\r\\nA_womack@me.com&gt;\\r\\nBond? Crowns? Veneer?\\r\\n\\r\\n\\r\\n'\ndata = mystr.split('\\r\\n')\ndata_filtered = list(filter(lambda x: x, data))\nfor d in data_filtered:\n    print(d.strip())\n\nContact Imported:\nBusinessPhone : 9547711900 Line1 : 2440 East Commercial Blvd.\nCity : Ft. Lauderdale\nState : FL\nPostalCode : 33308\nArt Womack recommends Steve Paul Dentist on Commercial Blvd area.\nA_womack@me.com&gt;\nBond? Crowns? Veneer?\n\ndef convert(x):\n    d = x.split(':')\n    newlist = []\n    if len(d) &gt; 2:\n        # Hack will work only in few cases, including this case\n        vals = d[1].strip().split(' ')\n        newlist.append(f'{d[0]}:{vals[0]}')\n        newlist.append(f'{vals[1]}:{d[2]}')\n        return newlist\n\n    return [x]\n\n\nmystr = '\\r\\nContact Imported:\\r\\nBusinessPhone : 9547711900 Line1 : 2440 East Commercial Blvd.\\r\\n City : Ft. Lauderdale\\r\\n State : FL\\r\\n PostalCode : 33308\\r\\n\\r\\nArt Womack recommends Steve Paul Dentist on Commercial Blvd area.\\r\\nA_womack@me.com&gt;\\r\\nBond? Crowns? Veneer?\\r\\n\\r\\n\\r\\n'\ndata = mystr.split('\\r\\n')\ndata_filtered = list(filter(lambda x: x, data))\ndata_filtered_2 = list((map(lambda x: convert(x), data_filtered)))\n\ndata_combined = []\nfor i in data_filtered_2:\n    data_combined += i\n\nfor d in data_combined:\n    print(d.strip())\n"
'def reader(filename):\n    return open(filename, encoding=\'latin1=1\')\n\ndef remover(filename):\n    with reader(filename) as f:\n        for line in f:\n            ...\n\nremover("somefile.txt")\n'
"eg.\n&gt;&gt;&gt; df = pd.DataFrame({'A': [0, 1, 2, 3, 4],\n...                    'B': [5, 6, 7, 8, 9],\n...                    'C': ['a', 'b', 'c', 'd', 'e']})\n&gt;&gt;&gt; df.replace(0, 5)\n   A  B  C\n0  5  5  a\n1  1  6  b\n2  2  7  c\n3  3  8  d\n4  4  9  e\n\ndf.replace('Toyouta','toyota')\n"
"df = pd.DataFrame({'Col1':list('abcdeafgbfhi')})\nsearch_str = 'b'\nidx_list = list(df[(df['Col1']==search_str)].index.values)\nprint(df[idx_list[0]:idx_list[1]])\n\n  Col1\n1    b\n2    c\n3    d\n4    e\n5    a\n6    f\n7    g\n"
'Campaign_info_1 = list()\nfor Detail in I_Details:\n  Campaign_info = Detail.contents\n  Campaign_info = str(Campaign_info)\n  if Campaign_info==None or Campaign_info=="": \n    pass \n  Campaign_info_1.append(Campaign_info) \n  print(Campaign_info)\n'
"v = df.ID.astype(str)\ndf['Year'], df['ID'] = v.str[:4], v.str[4:]\n\ndf\n\n              DateTime  Junction  Vehicles       ID  Year\n0 2015-11-01  00:00:00         1        15  1101001  2015\n1 2015-11-01  01:00:00         1        13  1101011  2015\n2 2015-11-01  02:00:00         1        10  1101021  2015\n3 2015-11-01  03:00:00         1         7  1101031  2015\n4 2015-11-01  04:00:00         1         9  1101041  2015\n5 2015-11-01  05:00:00         1         6  1101051  2015\n6 2015-11-01  06:00:00         1         9  1101061  2015\n7 2015-11-01  07:00:00         1         8  1101071  2015\n8 2015-11-01  08:00:00         1        11  1101081  2015\n9 2015-11-01  09:00:00         1        12  1101091  2015\n\nv = df.ID.astype(str).str.extract('(?P&lt;Year&gt;\\d{4})(?P&lt;ID&gt;.*)', expand=True)\ndf = pd.concat([df.drop('ID', 1), v], 1)\n\ndf\n\n              DateTime  Junction  Vehicles  Year       ID\n0 2015-11-01  00:00:00         1        15  2015  1101001\n1 2015-11-01  01:00:00         1        13  2015  1101011\n2 2015-11-01  02:00:00         1        10  2015  1101021\n3 2015-11-01  03:00:00         1         7  2015  1101031\n4 2015-11-01  04:00:00         1         9  2015  1101041\n5 2015-11-01  05:00:00         1         6  2015  1101051\n6 2015-11-01  06:00:00         1         9  2015  1101061\n7 2015-11-01  07:00:00         1         8  2015  1101071\n8 2015-11-01  08:00:00         1        11  2015  1101081\n9 2015-11-01  09:00:00         1        12  2015  1101091\n"
"import json\n\njson_string = r'''{&quot;created_at&quot;:&quot;Sun Jul 03 15:23:11 +0000 2016&quot;,&quot;id&quot;:749624538015621120,&quot;id_str&quot;:&quot;749624538015621120&quot;,&quot;text&quot;:&quot;Et hop un petit muldo dor\\u00e9-indigo #enroutepourlaG2&quot;,&quot;source&quot;:&quot;\\u003ca href=\\&quot;http:\\/\\/twitter.com\\&quot; rel=\\&quot;nofollow\\&quot;\\u003eTwitter Web Client\\u003c\\/a\\u003e&quot;,&quot;truncated&quot;:false,&quot;in_reply_to_status_id&quot;:null,&quot;in_reply_to_status_id_str&quot;:null,&quot;in_reply_to_user_id&quot;:null,&quot;in_reply_to_user_id_str&quot;:null,&quot;in_reply_to_screen_name&quot;:null,&quot;user&quot;:{&quot;id&quot;:3050686557,&quot;id_str&quot;:&quot;3050686557&quot;,&quot;name&quot;:&quot;Heresia&quot;,&quot;screen_name&quot;:&quot;Air_Et_Zia&quot;,&quot;location&quot;:null,&quot;url&quot;:null,&quot;description&quot;:&quot;Joueur de Dofus depuis 6 ans. Essentiellement ax\\u00e9 PvP. Actuellement sur #Amayiro !&quot;,&quot;protected&quot;:false,&quot;verified&quot;:false,&quot;followers_count&quot;:296,&quot;friends_count&quot;:30,&quot;listed_count&quot;:0,&quot;favourites_count&quot;:23,&quot;statuses_count&quot;:216,&quot;created_at&quot;:&quot;Sat Feb 21 20:45:02 +0000 2015&quot;,&quot;utc_offset&quot;:null,&quot;time_zone&quot;:null,&quot;geo_enabled&quot;:false,&quot;lang&quot;:&quot;fr&quot;,&quot;contributors_enabled&quot;:false,&quot;is_translator&quot;:false,&quot;profile_background_color&quot;:&quot;000000&quot;,&quot;profile_background_image_url&quot;:&quot;http:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png&quot;,&quot;profile_background_image_url_https&quot;:&quot;https:\\/\\/abs.twimg.com\\/images\\/themes\\/theme1\\/bg.png&quot;,&quot;profile_background_tile&quot;:false,&quot;profile_link_color&quot;:&quot;9266CC&quot;,&quot;profile_sidebar_border_color&quot;:&quot;000000&quot;,&quot;profile_sidebar_fill_color&quot;:&quot;000000&quot;,&quot;profile_text_color&quot;:&quot;000000&quot;,&quot;profile_use_background_image&quot;:false,&quot;profile_image_url&quot;:&quot;http:\\/\\/pbs.twimg.com\\/profile_images\\/569237837581545472\\/e_OJaGOl_normal.png&quot;,&quot;profile_image_url_https&quot;:&quot;https:\\/\\/pbs.twimg.com\\/profile_images\\/569237837581545472\\/e_OJaGOl_normal.png&quot;,&quot;default_profile&quot;:false,&quot;default_profile_image&quot;:false,&quot;following&quot;:null,&quot;follow_request_sent&quot;:null,&quot;notifications&quot;:null},&quot;geo&quot;:null,&quot;coordinates&quot;:null,&quot;place&quot;:null,&quot;contributors&quot;:null,&quot;is_quote_status&quot;:false,&quot;retweet_count&quot;:0,&quot;favorite_count&quot;:0,&quot;entities&quot;:{&quot;hashtags&quot;:[{&quot;text&quot;:&quot;enroutepourlaG2&quot;,&quot;indices&quot;:[34,50]}],&quot;urls&quot;:[],&quot;user_mentions&quot;:[],&quot;symbols&quot;:[]},&quot;favorited&quot;:false,&quot;retweeted&quot;:false,&quot;filter_level&quot;:&quot;low&quot;,&quot;lang&quot;:&quot;fr&quot;,&quot;timestamp_ms&quot;:&quot;1467559391870&quot;}'''\n\ntwit = json.loads(json_string)\nprint (json.dumps(twit[&quot;text&quot;]))#or any string manipulation here\n"
